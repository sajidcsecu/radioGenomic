{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMoNHwsli7ru0O0v2yrjwUh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sajidcsecu/radioGenomic/blob/main/UnetinGPU_(3D).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (1) Import Required Libraries"
      ],
      "metadata": {
        "id": "C9Zo7tkcI1CX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SimpleITK\n",
        "!pip install pydicom===2.4.3\n",
        "!pip install pydicom-seg\n",
        "!pip install numpy==1.23.5\n",
        "!pip install medpy"
      ],
      "metadata": {
        "id": "d9OVdEeKXpMe",
        "outputId": "2a548bad-952e-4c26-bafe-c83f257242e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.11/dist-packages (2.4.1)\n",
            "Requirement already satisfied: pydicom===2.4.3 in /usr/local/lib/python3.11/dist-packages (2.4.3)\n",
            "Requirement already satisfied: pydicom-seg in /usr/local/lib/python3.11/dist-packages (0.4.1)\n",
            "Requirement already satisfied: SimpleITK>1.2.4 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (2.4.1)\n",
            "Requirement already satisfied: jsonschema<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (3.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (1.23.5)\n",
            "Requirement already satisfied: pydicom>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (2.4.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (25.3.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (0.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (75.2.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (1.17.0)\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n",
            "Collecting medpy\n",
            "  Using cached medpy-0.5.2.tar.gz (156 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.11/dist-packages (from medpy) (1.14.1)\n",
            "Collecting numpy>=1.24 (from medpy)\n",
            "  Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m971.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: SimpleITK>=2.1 in /usr/local/lib/python3.11/dist-packages (from medpy) (2.4.1)\n",
            "Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: medpy\n",
            "  Building wheel for medpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for medpy: filename=MedPy-0.5.2-py3-none-any.whl size=224710 sha256=56e15cef93b7a0ecb596adb36b1794f381d0d87f9b0644b2b83f0f89c9feced8\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/33/ed/aaac5a347fb8d41679ca515b8f5c49dfdf49be15bdbb9a905d\n",
            "Successfully built medpy\n",
            "Installing collected packages: numpy, medpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydicom-seg 0.4.1 requires numpy<2.0.0,>=1.18.0, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed medpy-0.5.2 numpy-2.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (2) Import required Libraries"
      ],
      "metadata": {
        "id": "JadHvjQcJ-qU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    jaccard_score,\n",
        "    f1_score,\n",
        "    recall_score,\n",
        "    precision_score,\n",
        "    accuracy_score,\n",
        ")\n",
        "import h5py\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import time\n",
        "import csv\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from typing import List\n",
        "import torch.multiprocessing as mp\n",
        "import h5py\n",
        "from google.colab import drive\n",
        "import torch.amp as amp\n",
        "import cv2\n",
        "from typing import List\n",
        "from medpy.metric.binary import dc  # Dice coefficient for binary masks"
      ],
      "metadata": {
        "id": "pmtDNjxMbfB4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (3) Mount Google Drive"
      ],
      "metadata": {
        "id": "uyzguRDWI9bM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lq6jVaaMXZz5",
        "outputId": "495470bf-4718-4f31-9e82-2fe3bac6b01f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (4) Data Preperation"
      ],
      "metadata": {
        "id": "p4PdRsL8DChf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HDF5SegmentationDataset3D(Dataset):\n",
        "    def __init__(self, hdf5_path, patch_size=(64, 512, 512), transform=None, return_pid=False):\n",
        "        self.hdf5_path = hdf5_path\n",
        "        self.patch_size = patch_size  # Desired output shape: (D, H, W)\n",
        "        self.return_pid = return_pid\n",
        "        self.transform = transform\n",
        "\n",
        "        with h5py.File(self.hdf5_path, 'r') as f:\n",
        "            self.patient_ids = list(f.keys())\n",
        "\n",
        "    def pad_volume(self, volume):\n",
        "        \"\"\"Pads volume to match self.patch_size (D, H, W).\"\"\"\n",
        "        c, d, h, w = volume.shape\n",
        "        pd, ph, pw = self.patch_size\n",
        "        pad_d = max(0, pd - d)\n",
        "        pad_h = max(0, ph - h)\n",
        "        pad_w = max(0, pw - w)\n",
        "        padding = (0, pad_w, 0, pad_h, 0, pad_d)  # (w_left, w_right, h_top, h_bottom, d_front, d_back)\n",
        "        return F.pad(volume, padding)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pid = self.patient_ids[idx]\n",
        "        with h5py.File(self.hdf5_path, 'r') as f:\n",
        "            ct = f[pid]['ct'][:]\n",
        "            mask = f[pid]['mask'][:]\n",
        "\n",
        "        # Normalize CT to [0, 1]\n",
        "        ct = (ct - np.min(ct)) / (np.max(ct) - np.min(ct) + 1e-8)\n",
        "\n",
        "        # Convert to tensors and add channel dim: (1, D, H, W)\n",
        "        ct = torch.tensor(ct, dtype=torch.float32).unsqueeze(0)\n",
        "        mask = torch.tensor(mask, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        # Pad to fixed size\n",
        "        ct = self.pad_volume(ct)\n",
        "        mask = self.pad_volume(mask)\n",
        "\n",
        "        if self.transform:\n",
        "            ct, mask = self.transform(ct, mask)\n",
        "\n",
        "        if self.return_pid:\n",
        "            return ct, mask, pid\n",
        "        return ct, mask\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Define the target directory\n",
        "#     target_dir = \"/content/drive/MyDrive/PhDwork/Segmentation\"\n",
        "#     # Change to the target directory\n",
        "#     os.chdir(target_dir)\n",
        "#     # Verify the change\n",
        "#     print(\"Current Directory:\", os.getcwd())\n",
        "\n",
        "#     # Define device\n",
        "#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "#     print(f\"Using device: {device}\")\n",
        "\n",
        "# #     # Define DATASET_DIR\n",
        "#     DATASET_DIR = \"./datasets\"\n",
        "# #   os.makedirs(DATASET_DIR, exist_ok=True) #make the directory if it does not exist.\n",
        "\n",
        "#     batch_size = 8\n",
        "#     num_workers = 0\n",
        "\n",
        "#     # Dataset paths\n",
        "#     train_path = os.path.join(DATASET_DIR, \"train_dataset.hdf5\")\n",
        "#     valid_path = os.path.join(DATASET_DIR, \"valid_dataset.hdf5\")\n",
        "#     test_path = os.path.join(DATASET_DIR, \"test_dataset.hdf5\")\n",
        "#     train_dataset = HDF5SegmentationDataset3D(train_path)\n",
        "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,pin_memory=True)\n",
        "\n",
        "#     valid_dataset = HDF5SegmentationDataset3D(valid_path)\n",
        "#     valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,pin_memory=True)\n",
        "\n",
        "#     test_dataset = HDF5SegmentationDataset3D(test_path)\n",
        "#     test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,pin_memory=True)\n",
        "\n",
        "#     # Check a batch\n",
        "#     for i, (images, masks) in enumerate(train_loader):\n",
        "#         print(f\"Batch {i}: Images shape = {images.shape}, Masks shape = {masks.shape}\")"
      ],
      "metadata": {
        "id": "ZoGkyWq93X-H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Unet"
      ],
      "metadata": {
        "id": "h2iHKHfcF3yP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.LeakyReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UpSample(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.up(x)\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.conv = DoubleConv(in_channels, out_channels)\n",
        "        self.pool = nn.MaxPool3d(2)\n",
        "        self.dropout = nn.Dropout3d(p=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        p = self.pool(x)\n",
        "        return x, self.dropout(p)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.up = UpSample(in_channels, out_channels)\n",
        "        self.conv = DoubleConv(out_channels * 2, out_channels)\n",
        "        self.dropout = nn.Dropout3d(p=dropout)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        x = self.up(x)\n",
        "        x = torch.cat([x, skip], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class UNet3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.e1 = EncoderBlock(in_channels, 64, dropout=dropout)\n",
        "        self.e2 = EncoderBlock(64, 128, dropout=dropout)\n",
        "        self.e3 = EncoderBlock(128, 256, dropout=dropout)\n",
        "        self.e4 = EncoderBlock(256, 512, dropout=dropout)\n",
        "\n",
        "        self.b = DoubleConv(512, 1024)\n",
        "        self.dropout_bottleneck = nn.Dropout3d(p=dropout)\n",
        "\n",
        "        self.d1 = DecoderBlock(1024, 512, dropout=dropout)\n",
        "        self.d2 = DecoderBlock(512, 256, dropout=dropout)\n",
        "        self.d3 = DecoderBlock(256, 128, dropout=dropout)\n",
        "        self.d4 = DecoderBlock(128, 64, dropout=dropout)\n",
        "\n",
        "        self.outputs = nn.Conv3d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        s1, p1 = self.e1(x)\n",
        "        s2, p2 = self.e2(p1)\n",
        "        s3, p3 = self.e3(p2)\n",
        "        s4, p4 = self.e4(p3)\n",
        "\n",
        "        b = self.b(p4)\n",
        "        b = self.dropout_bottleneck(b)\n",
        "\n",
        "        d1 = self.d1(b, s4)\n",
        "        d2 = self.d2(d1, s3)\n",
        "        d3 = self.d3(d2, s2)\n",
        "        d4 = self.d4(d3, s1)\n",
        "\n",
        "        return self.outputs(d4)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "#     # 3D volume input: (batch_size, channels, depth, height, width)\n",
        "#     input_image = torch.randn((1, 1, 64, 128, 128), dtype=torch.float32).to(device)\n",
        "#     model = UNet(1, 1).to(device)\n",
        "#     out = model(input_image)\n",
        "#     print(out.shape)  # Expected output shape: (1, 1, 64, 128, 128) (depends on input)\n",
        "#     print(device)\n",
        "#     print(torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "PamAo5NoFsRv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Loss Function"
      ],
      "metadata": {
        "id": "IFrRJqgG7wxo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "944_4uJbmPPx"
      },
      "outputs": [],
      "source": [
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, smooth=1e-6, epsilon=1e-8):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, preds, targets):\n",
        "        preds = torch.sigmoid(preds)  # Apply sigmoid for binary segmentation\n",
        "        # Flatten batch-wise\n",
        "        preds = preds.contiguous().view(preds.size(0), -1)\n",
        "        targets = targets.contiguous().view(targets.size(0), -1)\n",
        "\n",
        "        intersection = (preds * targets).sum(dim=1)\n",
        "        dice = (2. * intersection + self.smooth) / (\n",
        "            preds.sum(dim=1) + targets.sum(dim=1) + self.smooth + self.epsilon\n",
        "        )\n",
        "        dice_loss = 1 - dice.mean()\n",
        "        return dice_loss\n",
        "\n",
        "\n",
        "class DiceBCELoss3D(nn.Module):\n",
        "    def __init__(self, smooth=1e-6, epsilon=1e-8):\n",
        "        super(DiceBCELoss3D, self).__init__()  # ‚úÖ Correct class name here\n",
        "        self.smooth = smooth\n",
        "        self.epsilon = epsilon\n",
        "        self.bce = nn.BCEWithLogitsLoss()  # Includes sigmoid internally\n",
        "\n",
        "    def forward(self, preds, targets):\n",
        "        bce_loss = self.bce(preds, targets)\n",
        "\n",
        "        preds = torch.sigmoid(preds)\n",
        "        preds = preds.contiguous().view(preds.size(0), -1)\n",
        "        targets = targets.contiguous().view(targets.size(0), -1)\n",
        "\n",
        "        intersection = (preds * targets).sum(dim=1)\n",
        "        dice = (2. * intersection + self.smooth) / (\n",
        "            preds.sum(dim=1) + targets.sum(dim=1) + self.smooth + self.epsilon\n",
        "        )\n",
        "        dice_loss = 1 - dice.mean()\n",
        "\n",
        "        return bce_loss + dice_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Test"
      ],
      "metadata": {
        "id": "CZZ3Gu-DD88X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UnetTest3D:\n",
        "    def __init__(self, model: torch.nn.Module, test_loader: torch.utils.data.DataLoader,\n",
        "                 result_dir: str, device: torch.device):\n",
        "        self.model = model.to(device)\n",
        "        self.test_loader = test_loader\n",
        "        self.result_dir = result_dir\n",
        "        self.device = device\n",
        "\n",
        "        os.makedirs(result_dir, exist_ok=True)\n",
        "        self.pred_volume_dir = os.path.join(result_dir, \"predictions_hdf5\")\n",
        "        self.visual_dir = os.path.join(result_dir, \"visualizations\")\n",
        "\n",
        "        os.makedirs(self.pred_volume_dir, exist_ok=True)\n",
        "        os.makedirs(self.visual_dir, exist_ok=True)\n",
        "\n",
        "    def calculate_metrics_3d(self, y_true: torch.Tensor, y_pred: torch.Tensor) -> List[float]:\n",
        "        y_true = y_true.detach().cpu().numpy().astype(bool).reshape(-1)\n",
        "        y_pred = (y_pred.detach().cpu().numpy() > 0.5).astype(bool).reshape(-1)\n",
        "\n",
        "        score_jaccard = jaccard_score(y_true, y_pred, zero_division=1)\n",
        "        score_f1 = f1_score(y_true, y_pred, zero_division=1)\n",
        "        score_recall = recall_score(y_true, y_pred, zero_division=1)\n",
        "        score_precision = precision_score(y_true, y_pred, zero_division=1)\n",
        "        score_acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "        return [score_jaccard, score_f1, score_recall, score_precision, score_acc]\n",
        "\n",
        "    def save_visualization(self, volume: np.ndarray, mask: np.ndarray, pred: np.ndarray, sample_id: int) -> None:\n",
        "        mid_slice = volume.shape[0] // 2\n",
        "        img_slice = (volume[mid_slice] * 255).astype(np.uint8)\n",
        "        mask_slice = (mask[mid_slice] > 0.5).astype(np.uint8) * 255\n",
        "        pred_slice = (pred[mid_slice] > 0.5).astype(np.uint8) * 255\n",
        "\n",
        "        h, w = img_slice.shape\n",
        "        line = np.ones((h, 10), dtype=np.uint8) * 128\n",
        "        cat = np.concatenate([img_slice, line, mask_slice, line, pred_slice], axis=1)\n",
        "\n",
        "        file_path = os.path.join(self.visual_dir, f\"sample_{sample_id}.png\")\n",
        "        cv2.imwrite(file_path, cat)\n",
        "        print(f\"‚úÖ Visualization saved: {file_path}\")\n",
        "\n",
        "    def save_prediction_hdf5(self, pred_volume: np.ndarray, sample_id: int) -> None:\n",
        "        file_path = os.path.join(self.pred_volume_dir, f\"sample_{sample_id}.hdf5\")\n",
        "        with h5py.File(file_path, \"w\") as hf:\n",
        "            hf.create_dataset(\"prediction\", data=pred_volume.astype(np.uint8))\n",
        "        print(f\"üìÅ HDF5 prediction saved: {file_path}\")\n",
        "\n",
        "    def test(self):\n",
        "        self.model.eval()\n",
        "        print(\"üöÄ Starting 3D U-Net testing...\")\n",
        "\n",
        "        overall_metrics = np.zeros(5)\n",
        "        dice_scores = []\n",
        "        times = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for pid, (x, y) in enumerate(self.test_loader):\n",
        "                x = x.to(self.device, dtype=torch.float32)  # [B, C, D, H, W]\n",
        "                y = y.to(self.device, dtype=torch.float32)\n",
        "\n",
        "                start = time.time()\n",
        "                y_pred = torch.sigmoid(self.model(x))\n",
        "                elapsed = time.time() - start\n",
        "                times.append(elapsed)\n",
        "\n",
        "                y_pred_bin = (y_pred > 0.5).float()\n",
        "\n",
        "                for b in range(x.size(0)):\n",
        "                    volume = x[b].squeeze().cpu().numpy()\n",
        "                    true_mask = y[b].squeeze().cpu().numpy()\n",
        "                    pred_mask = y_pred_bin[b].squeeze().cpu().numpy()\n",
        "\n",
        "                    metrics = self.calculate_metrics(true_mask, pred_mask)\n",
        "                    dice = dc(pred_mask.astype(bool), true_mask.astype(bool)) if np.any(true_mask) else 1.0\n",
        "\n",
        "                    overall_metrics += np.array(metrics)\n",
        "                    dice_scores.append(dice)\n",
        "\n",
        "                    self.save_prediction_hdf5(pred_mask, sample_id=pid * x.size(0) + b)\n",
        "                    self.save_visualization(volume, true_mask, pred_mask, sample_id=pid * x.size(0) + b)\n",
        "\n",
        "        N = len(self.test_loader.dataset)\n",
        "        avg_metrics = overall_metrics / N\n",
        "        mean_dice = np.mean(dice_scores)\n",
        "        fps = 1 / np.mean(times)\n",
        "\n",
        "        print(\"\\nüìä Average Test Metrics:\")\n",
        "        print(f\"Jaccard:  {avg_metrics[0]:.4f}\")\n",
        "        print(f\"F1 Score: {avg_metrics[1]:.4f}\")\n",
        "        print(f\"Recall:   {avg_metrics[2]:.4f}\")\n",
        "        print(f\"Precision:{avg_metrics[3]:.4f}\")\n",
        "        print(f\"Accuracy: {avg_metrics[4]:.4f}\")\n",
        "        print(f\"Dice Coef:{mean_dice:.4f}\")\n",
        "        print(f\"FPS:      {fps:.2f}\")"
      ],
      "metadata": {
        "id": "T4VsKIzmFGLP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Training"
      ],
      "metadata": {
        "id": "Fa8L5nD2EVP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, min_delta=0.001):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = float('inf')\n",
        "        self.counter = 0\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "\n",
        "        if self.counter >= self.patience:\n",
        "            print(f\"‚õî Early stopping triggered after {self.patience} epochs without improvement!\")\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "\n",
        "class UnetTrain3D:\n",
        "    def __init__(self,\n",
        "                 test_loader: torch.utils.data.DataLoader,\n",
        "                 num_epochs: int,\n",
        "                 lr: float,\n",
        "                 train_loader: torch.utils.data.DataLoader,\n",
        "                 valid_loader: torch.utils.data.DataLoader,\n",
        "                 model_file: str,\n",
        "                 loss_result_path: str,\n",
        "                 device: torch.device,\n",
        "                 result_dir: str,\n",
        "                 visualize: bool = True,\n",
        "                 vis_output_dir: str = None):\n",
        "\n",
        "        self.test_loader = test_loader\n",
        "        self.num_epochs = num_epochs\n",
        "        self.lr = lr\n",
        "        self.train_loader = train_loader\n",
        "        self.valid_loader = valid_loader\n",
        "        self.model_file = model_file\n",
        "        self.loss_result_path = loss_result_path\n",
        "        self.device = device\n",
        "        self.result_dir = result_dir\n",
        "        self.visualize = visualize\n",
        "        self.vis_output_dir = vis_output_dir or os.path.join(result_dir, \"visualizations\")\n",
        "\n",
        "        self.pred_volume_dir = os.path.join(self.result_dir, \"predictions_hdf5\")\n",
        "        os.makedirs(self.result_dir, exist_ok=True)\n",
        "        os.makedirs(self.pred_volume_dir, exist_ok=True)\n",
        "        os.makedirs(self.vis_output_dir, exist_ok=True)\n",
        "\n",
        "    def seeding(self, seed):\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    def epoch_time(self, start_time, end_time):\n",
        "        elapsed_time = end_time - start_time\n",
        "        elapsed_mins = int(elapsed_time / 60)\n",
        "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "        return elapsed_mins, elapsed_secs\n",
        "\n",
        "    def visualize_prediction_3d(self, input_volume, ground_truth, prediction, epoch=None, batch_idx=None):\n",
        "        slice_idx = input_volume.shape[0] // 2\n",
        "        ct_slice = input_volume[slice_idx].cpu().numpy()\n",
        "        gt_slice = ground_truth[slice_idx].cpu().numpy()\n",
        "        pred_slice = prediction[slice_idx].cpu().numpy()\n",
        "\n",
        "        fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "        axs[0].imshow(ct_slice, cmap='gray')\n",
        "        axs[0].set_title(\"CT Slice\")\n",
        "        axs[1].imshow(gt_slice, cmap='gray')\n",
        "        axs[1].set_title(\"Ground Truth\")\n",
        "        axs[2].imshow(pred_slice, cmap='gray')\n",
        "        axs[2].set_title(\"Prediction\")\n",
        "        for ax in axs:\n",
        "            ax.axis(\"off\")\n",
        "\n",
        "        if epoch is not None and batch_idx is not None:\n",
        "            plt.savefig(f\"{self.vis_output_dir}/epoch{epoch}_batch{batch_idx}.png\")\n",
        "        plt.close()\n",
        "\n",
        "    def train(self, model, loader, optimizer, loss_fn, device):\n",
        "        scaler = amp.GradScaler()\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for x, y in loader:\n",
        "            x = x.to(device, dtype=torch.float32)\n",
        "            y = y.to(device, dtype=torch.float32)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with amp.autocast(device_type=self.device.type):\n",
        "                y_pred = model(x)\n",
        "                loss = loss_fn(y_pred, y)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        return epoch_loss / len(loader)\n",
        "\n",
        "    def evaluate(self, model, loader, loss_fn, device, epoch=0):\n",
        "        model.eval()\n",
        "        epoch_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (x, y) in enumerate(loader):\n",
        "                x = x.to(device, dtype=torch.float32)\n",
        "                y = y.to(device, dtype=torch.float32)\n",
        "                y_pred = model(x)\n",
        "                loss = loss_fn(y_pred, y)\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if self.visualize and batch_idx == 0:\n",
        "                    y_pred_sigmoid = torch.sigmoid(y_pred)\n",
        "                    y_pred_bin = (y_pred_sigmoid > 0.5).float()\n",
        "                    self.visualize_prediction_3d(\n",
        "                        input_volume=x[0, 0],\n",
        "                        ground_truth=y[0, 0],\n",
        "                        prediction=y_pred_bin[0, 0],\n",
        "                        epoch=epoch,\n",
        "                        batch_idx=batch_idx\n",
        "                    )\n",
        "\n",
        "        return epoch_loss / len(loader)\n",
        "\n",
        "    def execute(self):\n",
        "        model = UNet3D(in_channels=1, out_channels=1).to(self.device)\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), self.lr, weight_decay=1e-5)\n",
        "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "        loss_fn = DiceBCELoss3D()\n",
        "\n",
        "        early_stopping = EarlyStopping(patience=10, min_delta=0.001)\n",
        "        best_valid_loss = float(\"inf\")\n",
        "        results = {\"train_loss\": [], \"valid_loss\": []}\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            start_time = time.time()\n",
        "\n",
        "            train_loss = self.train(model, self.train_loader, optimizer, loss_fn, self.device)\n",
        "            valid_loss = self.evaluate(model, self.valid_loader, loss_fn, self.device, epoch)\n",
        "\n",
        "            if valid_loss < best_valid_loss:\n",
        "                print(f\"‚úÖ Valid loss improved from {best_valid_loss:.4f} to {valid_loss:.4f}. Saving checkpoint.\")\n",
        "                best_valid_loss = valid_loss\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'loss': best_valid_loss,\n",
        "                }, self.model_file)\n",
        "\n",
        "            end_time = time.time()\n",
        "            mins, secs = self.epoch_time(start_time, end_time)\n",
        "            results[\"train_loss\"].append(train_loss)\n",
        "            results[\"valid_loss\"].append(valid_loss)\n",
        "            print(f\"Epoch {epoch+1}: ‚è±Ô∏è {mins}m {secs}s | Train: {train_loss:.4f} | Val: {valid_loss:.4f}\")\n",
        "\n",
        "            if early_stopping(valid_loss):\n",
        "                print(\"üõë Early stopping triggered.\")\n",
        "                break\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        with open(self.loss_result_path, \"w\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            for key, val in results.items():\n",
        "                writer.writerow([key] + val)\n"
      ],
      "metadata": {
        "id": "281KQS_iEIDX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Define the target directory\n",
        "    target_dir = \"/content/drive/MyDrive/PhDwork/Segmentation\"\n",
        "    # Change to the target directory\n",
        "    os.chdir(target_dir)\n",
        "    # Verify the change\n",
        "    print(\"Current Directory:\", os.getcwd())\n",
        "\n",
        "    batch_size = 1   # Smaller batch size for 3D volumes due to memory\n",
        "    num_epochs = 100\n",
        "    lr = 1e-4\n",
        "\n",
        "    output_dir = os.path.join(\".\", \"Segmentation\", \"Results\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    loss_result_file = os.path.join(output_dir, \"results_losses.csv\")\n",
        "    model_file = os.path.join(output_dir, \"model.pth\")\n",
        "    test_result_path = os.path.join(output_dir, \"results_test\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using Device:\", device)\n",
        "\n",
        "    DATASET_DIR = \"./datasets\"\n",
        "    train_path = os.path.join(DATASET_DIR, \"train_dataset.hdf5\")\n",
        "    valid_path = os.path.join(DATASET_DIR, \"valid_dataset.hdf5\")\n",
        "    test_path = os.path.join(DATASET_DIR, \"test_dataset.hdf5\")\n",
        "\n",
        "    print(\"Loading 3D Training Data...\")\n",
        "    train_dataset = HDF5SegmentationDataset3D(train_path)\n",
        "    print(\"Loading 3D Validation Data...\")\n",
        "    valid_dataset = HDF5SegmentationDataset3D(valid_path)\n",
        "    print(\"Loading 3D Test Data...\")\n",
        "    test_dataset = HDF5SegmentationDataset3D(test_path)\n",
        "\n",
        "    num_workers = 0\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "    print(f\"Train: {len(train_dataset)}, Valid: {len(valid_dataset)}, Test: {len(test_dataset)}\")\n",
        "\n",
        "    # ‚úÖ Training\n",
        "    trainer = UnetTrain3D(\n",
        "        test_loader=test_loader,\n",
        "        num_epochs=num_epochs,\n",
        "        lr=lr,\n",
        "        train_loader=train_loader,\n",
        "        valid_loader=valid_loader,\n",
        "        model_file=model_file,\n",
        "        loss_result_path=loss_result_file,\n",
        "        device=device,\n",
        "        result_dir=output_dir,\n",
        "        visualize=True,\n",
        "        vis_output_dir=os.path.join(output_dir, \"visualizations\")\n",
        "    )\n",
        "\n",
        "    trainer.seeding(42)\n",
        "    trainer.execute()\n",
        "\n",
        "    # ‚úÖ Testing\n",
        "    model =UnetTrain3D(in_channels=1, out_channels=1).to(device)\n",
        "    checkpoint = torch.load(model_file, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    utest = UnetTest3D(\n",
        "    model=model,\n",
        "    test_loader=test_loader,\n",
        "    result_dir=test_result_path,\n",
        "    device=device\n",
        "    )\n",
        "    utest.test()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mp.set_start_method(\"spawn\", force=True)  # Important for multiprocessing\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "CwcTHPshqu0O",
        "outputId": "4a789036-fd61-440b-8873-80b6c00c2045"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Directory: /content/drive/MyDrive/PhDwork/Segmentation\n",
            "Using Device: cuda\n",
            "Loading 3D Training Data...\n",
            "Loading 3D Validation Data...\n",
            "Loading 3D Test Data...\n",
            "Train: 340, Valid: 43, Test: 38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py:720: UserWarning: cuDNN cannot be used for large non-batch-splittable convolutions if the V8 API is not enabled or before cuDNN version 9.3+. Consider upgrading cuDNN and/or enabling the V8 API for better efficiency. (Triggered internally at /pytorch/aten/src/ATen/native/Convolution.cpp:430.)\n",
            "  return F.conv3d(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 108.00 GiB. GPU 0 has a total capacity of 39.56 GiB of which 15.44 GiB is free. Process 6006 has 24.10 GiB memory in use. Of the allocated memory 23.58 GiB is allocated by PyTorch, and 43.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-97ae2a0c12fd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_start_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spawn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Important for multiprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-97ae2a0c12fd>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseeding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# ‚úÖ Testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-576ab3f338fc>\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-576ab3f338fc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, loader, optimizer, loss_fn, device)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-ad6ceaff3d99>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0md2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0md3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0md4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-ad6ceaff3d99>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, skip)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-ad6ceaff3d99>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mUpSample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    718\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             )\n\u001b[0;32m--> 720\u001b[0;31m         return F.conv3d(\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         )\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 108.00 GiB. GPU 0 has a total capacity of 39.56 GiB of which 15.44 GiB is free. Process 6006 has 24.10 GiB memory in use. Of the allocated memory 23.58 GiB is allocated by PyTorch, and 43.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xKImglO4-twL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}