{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sajidcsecu/radioGenomic/blob/main/UnetinGPU_(Fifty_Fifty_TverskyLoss_And_Strong_Augmentation).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr2QqHlZ8GZB"
      },
      "source": [
        "# This is the Code for the Segmentation on Rider Dataset (LUNG1). The Code is worked on the 2D slices over GPU. The balanced sampler and the augmentation is used in the code..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9Zo7tkcI1CX"
      },
      "source": [
        "# (1) Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9OVdEeKXpMe",
        "outputId": "db329fb9-8579-489c-8bd3-7736fbaa0664"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.11/dist-packages (2.4.1)\n",
            "Requirement already satisfied: pydicom===2.4.3 in /usr/local/lib/python3.11/dist-packages (2.4.3)\n",
            "Requirement already satisfied: pydicom-seg in /usr/local/lib/python3.11/dist-packages (0.4.1)\n",
            "Requirement already satisfied: SimpleITK>1.2.4 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (2.4.1)\n",
            "Requirement already satisfied: jsonschema<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (3.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (1.23.5)\n",
            "Requirement already satisfied: pydicom>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (2.4.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (25.3.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (0.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (75.2.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (1.17.0)\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install SimpleITK\n",
        "!pip install pydicom===2.4.3\n",
        "!pip install pydicom-seg\n",
        "!pip install numpy==1.23.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JadHvjQcJ-qU"
      },
      "source": [
        "\n",
        "# (2) Import required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pmtDNjxMbfB4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import csv\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import (\n",
        "    jaccard_score,\n",
        "    f1_score,\n",
        "    recall_score,\n",
        "    precision_score,\n",
        "    accuracy_score,\n",
        ")\n",
        "from tqdm import tqdm\n",
        "\n",
        "import cv2\n",
        "from typing import List\n",
        "import torch.multiprocessing as mp\n",
        "import h5py\n",
        "from google.colab import drive\n",
        "import torch.amp as amp\n",
        "import pickle\n",
        "from torch.utils.data import Sampler\n",
        "import torchvision.transforms.functional as TF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyzguRDWI9bM"
      },
      "source": [
        "# (3) Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq6jVaaMXZz5",
        "outputId": "02245c13-aba8-4d54-f1e6-7589f6f8d8ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4PdRsL8DChf"
      },
      "source": [
        "# (4) Data Preperation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nTo3RxLmjW5r"
      },
      "outputs": [],
      "source": [
        "class StrongJointTransform:\n",
        "    def __init__(self, p_flip=0.5, p_rotate=0.5, p_gamma=0.5, p_noise=0.5):\n",
        "        self.p_flip = p_flip\n",
        "        self.p_rotate = p_rotate\n",
        "        self.p_gamma = p_gamma\n",
        "        self.p_noise = p_noise\n",
        "\n",
        "    def __call__(self, image, mask):\n",
        "        if random.random() < self.p_flip:\n",
        "            if random.random() > 0.5:\n",
        "                image = TF.hflip(image)\n",
        "                mask = TF.hflip(mask)\n",
        "            else:\n",
        "                image = TF.vflip(image)\n",
        "                mask = TF.vflip(mask)\n",
        "\n",
        "        if random.random() < self.p_rotate:\n",
        "            angle = random.choice([90, 180, 270])\n",
        "            image = TF.rotate(image, angle)\n",
        "            mask = TF.rotate(mask, angle)\n",
        "\n",
        "        if random.random() < self.p_gamma:\n",
        "            gamma = random.uniform(0.7, 1.5)\n",
        "            image = TF.adjust_gamma(image, gamma)\n",
        "\n",
        "        if random.random() < self.p_noise:\n",
        "            noise = torch.randn_like(image) * 0.05\n",
        "            image = torch.clamp(image + noise, 0, 1)\n",
        "\n",
        "        return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FvhtapeukPgt"
      },
      "outputs": [],
      "source": [
        "class BalancedTumorSampler(Sampler):\n",
        "    def __init__(self, dataset, tumor_ratio=0.5, shuffle=True, index_cache_path=None):\n",
        "        self.dataset = dataset\n",
        "        self.tumor_ratio = tumor_ratio\n",
        "        self.shuffle = shuffle\n",
        "        self.index_cache_path = index_cache_path\n",
        "\n",
        "        self.tumor_indices = []\n",
        "        self.non_tumor_indices = []\n",
        "\n",
        "        if index_cache_path and os.path.exists(index_cache_path):\n",
        "            print(f\"📂 Loading cached indices from {index_cache_path}\")\n",
        "            with open(index_cache_path, 'rb') as f:\n",
        "                cached = pickle.load(f)\n",
        "                self.tumor_indices = cached['tumor']\n",
        "                self.non_tumor_indices = cached['non_tumor']\n",
        "            print(f\"✅ Loaded: {len(self.tumor_indices)} tumor, {len(self.non_tumor_indices)} non-tumor\")\n",
        "        else:\n",
        "            print(\"🛠️ Computing tumor/non-tumor indices...\")\n",
        "            self._prepare_indices()\n",
        "            if index_cache_path:\n",
        "                print(f\"💾 Saving indices to {index_cache_path}\")\n",
        "                with open(index_cache_path, 'wb') as f:\n",
        "                    pickle.dump({'tumor': self.tumor_indices, 'non_tumor': self.non_tumor_indices}, f)\n",
        "\n",
        "    def _prepare_indices(self):\n",
        "        for idx in range(len(self.dataset)):\n",
        "            _, mask = self.dataset[idx]\n",
        "            if mask.sum() > 0:\n",
        "                self.tumor_indices.append(idx)\n",
        "            else:\n",
        "                self.non_tumor_indices.append(idx)\n",
        "\n",
        "    def __iter__(self):\n",
        "        if self.shuffle:\n",
        "            random.shuffle(self.tumor_indices)\n",
        "            random.shuffle(self.non_tumor_indices)\n",
        "\n",
        "        total_samples = min(len(self.tumor_indices), len(self.non_tumor_indices)) * 2\n",
        "        num_tumor = int(self.tumor_ratio * total_samples)\n",
        "        num_non_tumor = total_samples - num_tumor\n",
        "\n",
        "        selected_tumor = self.tumor_indices[:num_tumor]\n",
        "        selected_non_tumor = self.non_tumor_indices[:num_non_tumor]\n",
        "\n",
        "        combined = selected_tumor + selected_non_tumor\n",
        "        if self.shuffle:\n",
        "            random.shuffle(combined)\n",
        "\n",
        "        return iter(combined)\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.tumor_indices), len(self.non_tumor_indices)) * 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZoGkyWq93X-H"
      },
      "outputs": [],
      "source": [
        "class HDF5SegmentationDataset(Dataset):\n",
        "    def __init__(self, hdf5_path, transform=None):\n",
        "        self.hdf5_path = hdf5_path\n",
        "        self.transform = transform\n",
        "        self.patient_ids = []\n",
        "\n",
        "        # Read only keys for indexing\n",
        "        with h5py.File(self.hdf5_path, 'r') as f:\n",
        "            self.patient_ids = list(f.keys())\n",
        "            self.slice_indices = [(pid, i) for pid in self.patient_ids for i in range(f[pid]['ct'].shape[0])]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.slice_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pid, slice_idx = self.slice_indices[idx]\n",
        "\n",
        "        with h5py.File(self.hdf5_path, 'r') as f:\n",
        "            ct_slice = f[pid]['ct'][slice_idx]\n",
        "            mask_slice = f[pid]['mask'][slice_idx]\n",
        "\n",
        "        # Normalize CT to [0, 1]\n",
        "        ct_slice = (ct_slice - np.min(ct_slice)) / (np.max(ct_slice) - np.min(ct_slice) + 1e-5)\n",
        "\n",
        "        # Convert to torch tensors and add channel dim\n",
        "        ct_tensor = torch.tensor(ct_slice, dtype=torch.float32).unsqueeze(0)\n",
        "        mask_tensor = torch.tensor(mask_slice, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "        if self.transform:\n",
        "            ct_tensor, mask_tensor = self.transform(ct_tensor, mask_tensor)\n",
        "\n",
        "        return ct_tensor, mask_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2iHKHfcF3yP"
      },
      "source": [
        "# 2. Unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PamAo5NoFsRv"
      },
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UpSample(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.up(x)\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.conv = DoubleConv(in_channels, out_channels)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        p = self.pool(x)\n",
        "        return x, self.dropout(p)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.up = UpSample(in_channels, out_channels)\n",
        "        self.conv = DoubleConv(out_channels * 2, out_channels)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        x = self.up(x)\n",
        "        x = torch.cat([x, skip], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.e1 = EncoderBlock(in_channels, 64, dropout=dropout)\n",
        "        self.e2 = EncoderBlock(64, 128, dropout=dropout)\n",
        "        self.e3 = EncoderBlock(128, 256, dropout=dropout)\n",
        "        self.e4 = EncoderBlock(256, 512, dropout=dropout)\n",
        "\n",
        "        self.b = DoubleConv(512, 1024)\n",
        "        self.dropout_bottleneck = nn.Dropout(p=dropout)\n",
        "\n",
        "        self.d1 = DecoderBlock(1024, 512, dropout=dropout)\n",
        "        self.d2 = DecoderBlock(512, 256, dropout=dropout)\n",
        "        self.d3 = DecoderBlock(256, 128, dropout=dropout)\n",
        "        self.d4 = DecoderBlock(128, 64, dropout=dropout)\n",
        "\n",
        "        self.outputs = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        s1, p1 = self.e1(x)\n",
        "        s2, p2 = self.e2(p1)\n",
        "        s3, p3 = self.e3(p2)\n",
        "        s4, p4 = self.e4(p3)\n",
        "\n",
        "        b = self.b(p4)\n",
        "        b = self.dropout_bottleneck(b)\n",
        "\n",
        "        d1 = self.d1(b, s4)\n",
        "        d2 = self.d2(d1, s3)\n",
        "        d3 = self.d3(d2, s2)\n",
        "        d4 = self.d4(d3, s1)\n",
        "\n",
        "        return self.outputs(d4)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # double_conv = DoubleConv(256, 256)\n",
        "#     # print(double_conv)\n",
        "#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "#     input_image = torch.randn((1, 1, 512, 512), dtype=torch.float32)\n",
        "#     model = UNet(1, 1).to(device)\n",
        "#     input_image = input_image.to(device)\n",
        "#     out = model(input_image)\n",
        "#     print(out.shape)\n",
        "#     print(device)\n",
        "#     print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFrRJqgG7wxo"
      },
      "source": [
        "## 2. Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "944_4uJbmPPx"
      },
      "outputs": [],
      "source": [
        "class TverskyLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.5, beta=0.5, smooth=1e-6):\n",
        "        super(TverskyLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, preds, targets):\n",
        "        # Ensure sigmoid is applied\n",
        "        preds = torch.sigmoid(preds)\n",
        "\n",
        "        # Flatten\n",
        "        preds = preds.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        TP = (preds * targets).sum()\n",
        "        FP = ((1 - targets) * preds).sum()\n",
        "        FN = (targets * (1 - preds)).sum()\n",
        "\n",
        "        tversky = (TP + self.smooth) / (TP + self.alpha * FP + self.beta * FN + self.smooth)\n",
        "        return 1 - tversky"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZZ3Gu-DD88X"
      },
      "source": [
        "# 3. Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "T4VsKIzmFGLP"
      },
      "outputs": [],
      "source": [
        "class UnetTest:\n",
        "    def __init__(self, test_result_path: str,metrics_csv, device: torch.device):\n",
        "        self.test_result_path = test_result_path\n",
        "        self.device = device\n",
        "        self.metrics_csv = metrics_csv\n",
        "\n",
        "        os.makedirs(self.test_result_path, exist_ok=True)\n",
        "\n",
        "        # Initialize CSV file with headers\n",
        "        if not os.path.exists(self.metrics_csv):\n",
        "            with open(self.metrics_csv, mode='w', newline='') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([\"SampleID\", \"Jaccard\", \"F1\", \"Recall\", \"Precision\", \"Accuracy\", \"Time\"])\n",
        "\n",
        "        print(f\"Test results will be saved to: {self.test_result_path}\")\n",
        "        print(f\"Using device: {self.device} (CUDA available: {torch.cuda.is_available()})\")\n",
        "\n",
        "    def calculate_metrics(self, y_true: torch.Tensor, y_pred: torch.Tensor) -> List[float]:\n",
        "        # Apply sigmoid and threshold at 0.5\n",
        "        y_pred = (y_pred > 0.5).float()\n",
        "\n",
        "        # Move to CPU and convert to numpy\n",
        "        y_true_np = y_true.detach().cpu().numpy().astype(bool).reshape(-1)\n",
        "        y_pred_np = y_pred.detach().cpu().numpy().astype(bool).reshape(-1)\n",
        "\n",
        "        # If ground truth is completely empty or prediction is empty, set zero_division=0 for clarity\n",
        "        return [\n",
        "            jaccard_score(y_true_np, y_pred_np, zero_division=0),\n",
        "            f1_score(y_true_np, y_pred_np, zero_division=0),\n",
        "            recall_score(y_true_np, y_pred_np, zero_division=0),\n",
        "            precision_score(y_true_np, y_pred_np, zero_division=0),\n",
        "            accuracy_score(y_true_np, y_pred_np)\n",
        "        ]\n",
        "\n",
        "\n",
        "    def save_result(self, image: torch.Tensor, org_mask: torch.Tensor, predicted_mask: torch.Tensor, sample_id: int) -> None:\n",
        "        predicted_mask = (predicted_mask.detach().cpu().numpy().squeeze() > 0.5).astype(np.uint8) * 255\n",
        "        org_mask = (org_mask.detach().cpu().numpy().squeeze() > 0.5).astype(np.uint8) * 255\n",
        "        image = (image.detach().cpu().numpy().squeeze() * 255).astype(np.uint8)\n",
        "\n",
        "        h, w = image.shape\n",
        "        line = np.ones((h, 10), dtype=np.uint8) * 128\n",
        "        cat_images = np.concatenate([image, line, org_mask, line, predicted_mask], axis=1)\n",
        "\n",
        "        file_name = os.path.join(self.test_result_path, f\"sample_{sample_id}.png\")\n",
        "        success = cv2.imwrite(file_name, cat_images)\n",
        "\n",
        "        if success:\n",
        "            print(f\"✅ Saved: {file_name}\")\n",
        "        else:\n",
        "            print(f\"❌ Failed to save image: {file_name}\")\n",
        "\n",
        "    def append_metrics_to_csv(self, sample_id: int, metrics: List[float], elapsed_time: float) -> None:\n",
        "        with open(self.metrics_csv, mode='a', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([sample_id] + [f\"{m:.4f}\" for m in metrics] + [f\"{elapsed_time:.4f}\"])\n",
        "\n",
        "    def test(self, model: torch.nn.Module, test_loader: torch.utils.data.DataLoader) -> None:\n",
        "        model.eval()\n",
        "        metrics_score = np.zeros(5)\n",
        "        time_taken = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for pid, (x, y) in enumerate(test_loader):\n",
        "                # if (pid >=2):\n",
        "                #   break\n",
        "                x = x.to(self.device, dtype=torch.float32)\n",
        "                y = y.to(self.device, dtype=torch.float32)\n",
        "\n",
        "                start_time = time.time()\n",
        "                y_pred = torch.sigmoid(model(x))\n",
        "                elapsed_time = time.time() - start_time\n",
        "                time_taken.append(elapsed_time)\n",
        "\n",
        "                batch_metrics = self.calculate_metrics(y, y_pred)\n",
        "                metrics_score += np.array(batch_metrics)\n",
        "\n",
        "                for idx in range(x.size(0)):\n",
        "                    sample_id = pid * x.size(0) + idx\n",
        "                    self.save_result(x[idx], y[idx], y_pred[idx], sample_id)\n",
        "                    self.append_metrics_to_csv(sample_id, batch_metrics, elapsed_time)\n",
        "\n",
        "        num_batches = len(test_loader)\n",
        "        avg_metrics = metrics_score / num_batches\n",
        "\n",
        "        print(f\"\\n🧪 Total Batches in Test Set: {num_batches}\")\n",
        "        print(f\"📊 Jaccard: {avg_metrics[0]:.4f} | F1: {avg_metrics[1]:.4f} | Recall: {avg_metrics[2]:.4f} | \"\n",
        "              f\"Precision: {avg_metrics[3]:.4f} | Accuracy: {avg_metrics[4]:.4f}\")\n",
        "        print(f\"⚡ FPS: {1 / np.mean(time_taken):.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa8L5nD2EVP_"
      },
      "source": [
        "# 4. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "281KQS_iEIDX"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, verbose=True, min_delta=0, path='checkpoint.pt'):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.min_delta = min_delta\n",
        "        self.path = path\n",
        "\n",
        "    def __call__(self, val_loss, model, epoch=None, optimizer=None):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, epoch, optimizer)\n",
        "        elif score < self.best_score + self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f\"⏳ EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, epoch, optimizer)\n",
        "            self.counter = 0\n",
        "\n",
        "        return self.early_stop\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model, epoch, optimizer):\n",
        "        if self.verbose:\n",
        "            print(f\"✅ Valid loss improved. Saving model...\")\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict() if optimizer else None,\n",
        "            'val_loss': val_loss\n",
        "        }\n",
        "        torch.save(checkpoint, self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "class UnetTrain:\n",
        "    def __init__(self,\n",
        "                 model_file: str,\n",
        "                 loss_result_path: str,\n",
        "                 lr: float,\n",
        "                 num_epochs: int,\n",
        "                 device: torch.device):\n",
        "\n",
        "        self.model_file = model_file\n",
        "        self.loss_result_path = loss_result_path\n",
        "        self.lr = lr\n",
        "        self.num_epochs = num_epochs\n",
        "        self.device = device\n",
        "\n",
        "        # For reproducibility\n",
        "        self.seeding(42)\n",
        "\n",
        "        print(f\"🔧 Training initialized: lr={self.lr}, epochs={self.num_epochs}\")\n",
        "        print(f\"📁 Model will be saved to: {self.model_file}\")\n",
        "        print(f\"📁 Loss log will be saved to: {self.loss_result_path}\")\n",
        "        print(f\"💻 Device in use: {self.device} (CUDA available: {torch.cuda.is_available()})\")\n",
        "\n",
        "    def seeding(self, seed):\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    def epoch_time(self, start_time, end_time):\n",
        "        elapsed_time = end_time - start_time\n",
        "        elapsed_mins = int(elapsed_time / 60)\n",
        "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "        return elapsed_mins, elapsed_secs\n",
        "\n",
        "    def train(self, model, loader, optimizer, loss_fn, device):\n",
        "        scaler = amp.GradScaler()\n",
        "        epoch_loss = 0.0\n",
        "        model.train()\n",
        "\n",
        "        for batch_idx, (x, y) in enumerate(loader):\n",
        "            # if (batch_idx >=10):\n",
        "            #       break\n",
        "            x = x.to(device, dtype=torch.float32)\n",
        "            y = y.to(device, dtype=torch.float32)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with amp.autocast(device_type=self.device.type):\n",
        "                y_pred = model(x)\n",
        "                loss = loss_fn(y_pred, y)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        return epoch_loss / len(loader)\n",
        "\n",
        "    def evaluate(self, model, loader, loss_fn, device):\n",
        "        epoch_loss = 0.0\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (x, y) in enumerate(loader):\n",
        "                # if (batch_idx >=2):\n",
        "                #     break\n",
        "                x = x.to(device, dtype=torch.float32)\n",
        "                y = y.to(device, dtype=torch.float32)\n",
        "                y_pred = model(x)\n",
        "                loss = loss_fn(y_pred, y)\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "        return epoch_loss / len(loader)\n",
        "\n",
        "    def execute(self, train_loader, valid_loader):\n",
        "        model = UNet(in_channels=1, out_channels=1, dropout=0.3).to(self.device)\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=self.lr, weight_decay=1e-5)\n",
        "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "        loss_fn = TverskyLoss(alpha=0.7, beta=0.3)\n",
        "\n",
        "        early_stopping = EarlyStopping(patience=10, min_delta=0.001, path=self.model_file)\n",
        "        results = {\"train_loss\": [], \"valid_loss\": []}\n",
        "\n",
        "        for epoch in tqdm(range(1, self.num_epochs + 1), desc=\"🏋️ Training\"):\n",
        "            start_time = time.time()\n",
        "\n",
        "            train_loss = self.train(model, train_loader, optimizer, loss_fn, self.device)\n",
        "            valid_loss = self.evaluate(model, valid_loader, loss_fn, self.device)\n",
        "            scheduler.step()\n",
        "\n",
        "            end_time = time.time()\n",
        "            epoch_mins, epoch_secs = self.epoch_time(start_time, end_time)\n",
        "\n",
        "            results[\"train_loss\"].append(train_loss)\n",
        "            results[\"valid_loss\"].append(valid_loss)\n",
        "\n",
        "            print(f\"📅 Epoch {epoch:03d} | ⏱️ {epoch_mins}m {epoch_secs}s | 🔥 Train: {train_loss:.4f} | 🎯 Val: {valid_loss:.4f}\")\n",
        "\n",
        "            if early_stopping(valid_loss, model, epoch, optimizer):\n",
        "                print(\"🛑 Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "        # Save train/val loss history to CSV\n",
        "        with open(self.loss_result_path, \"w\", newline=\"\") as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([\"Epoch\"] + list(range(1, len(results[\"train_loss\"]) + 1)))\n",
        "            writer.writerow([\"Train Loss\"] + results[\"train_loss\"])\n",
        "            writer.writerow([\"Valid Loss\"] + results[\"valid_loss\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwcTHPshqu0O",
        "outputId": "3b6d0420-dcd7-4587-c3b8-d6d01c103907"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📁 Current Directory: /content/drive/MyDrive/PhDwork/Segmentation\n",
            "📦 Loading datasets...\n",
            "📂 Loading cached indices from ./datasets/Datasets_OriginalCT_With_Empty_NonEmpty_slices_In_Train/tumor_indices.pkl\n",
            "✅ Loaded: 5858 tumor, 35476 non-tumor\n",
            "✅ Dataset sizes — Train: 41334, Valid: 5141, Test: 4653\n",
            "🔧 Training initialized: lr=1e-05, epochs=100\n",
            "📁 Model will be saved to: ./results/Results_OriginalCT_Fifty_Fifty_TverskyLoss_And_Strong_Augmentation/model.pth\n",
            "📁 Loss log will be saved to: ./results/Results_OriginalCT_Fifty_Fifty_TverskyLoss_And_Strong_Augmentation/train_and_valid_loss_results.csv\n",
            "💻 Device in use: cuda (CUDA available: True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r🏋️ Training:   0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 001 | ⏱️ 34m 17s | 🔥 Train: 0.9908 | 🎯 Val: 0.9970\n",
            "✅ Valid loss improved. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🏋️ Training:   2%|▏         | 2/100 [1:04:50<52:22:58, 1924.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 002 | ⏱️ 30m 25s | 🔥 Train: 0.9884 | 🎯 Val: 0.9967\n",
            "⏳ EarlyStopping counter: 1 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r🏋️ Training:   3%|▎         | 3/100 [1:35:12<50:34:47, 1877.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 003 | ⏱️ 30m 21s | 🔥 Train: 0.9872 | 🎯 Val: 0.9962\n",
            "⏳ EarlyStopping counter: 2 out of 10\n",
            "📅 Epoch 004 | ⏱️ 30m 21s | 🔥 Train: 0.9859 | 🎯 Val: 0.9956\n",
            "✅ Valid loss improved. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🏋️ Training:   5%|▌         | 5/100 [2:36:02<48:42:04, 1845.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 005 | ⏱️ 30m 27s | 🔥 Train: 0.9847 | 🎯 Val: 0.9952\n",
            "⏳ EarlyStopping counter: 1 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r🏋️ Training:   6%|▌         | 6/100 [3:06:23<47:58:13, 1837.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 006 | ⏱️ 30m 20s | 🔥 Train: 0.9838 | 🎯 Val: 0.9952\n",
            "⏳ EarlyStopping counter: 2 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r🏋️ Training:   7%|▋         | 7/100 [3:36:46<47:20:39, 1832.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 007 | ⏱️ 30m 23s | 🔥 Train: 0.9830 | 🎯 Val: 0.9948\n",
            "⏳ EarlyStopping counter: 3 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r🏋️ Training:   8%|▊         | 8/100 [4:07:09<46:45:22, 1829.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 008 | ⏱️ 30m 22s | 🔥 Train: 0.9825 | 🎯 Val: 0.9947\n",
            "⏳ EarlyStopping counter: 4 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r🏋️ Training:   9%|▉         | 9/100 [4:37:31<46:11:26, 1827.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 009 | ⏱️ 30m 22s | 🔥 Train: 0.9821 | 🎯 Val: 0.9947\n",
            "⏳ EarlyStopping counter: 5 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r🏋️ Training:  10%|█         | 10/100 [5:07:55<45:39:18, 1826.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 010 | ⏱️ 30m 23s | 🔥 Train: 0.9820 | 🎯 Val: 0.9946\n",
            "⏳ EarlyStopping counter: 6 out of 10\n",
            "📅 Epoch 011 | ⏱️ 30m 23s | 🔥 Train: 0.9814 | 🎯 Val: 0.9941\n",
            "✅ Valid loss improved. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r🏋️ Training:  11%|█         | 11/100 [5:38:19<45:07:59, 1825.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 012 | ⏱️ 30m 30s | 🔥 Train: 0.9793 | 🎯 Val: 0.9931\n",
            "✅ Valid loss improved. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🏋️ Training:  13%|█▎        | 13/100 [6:39:21<44:11:02, 1828.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 013 | ⏱️ 30m 30s | 🔥 Train: 0.9767 | 🎯 Val: 0.9927\n",
            "⏳ EarlyStopping counter: 1 out of 10\n",
            "📅 Epoch 014 | ⏱️ 30m 23s | 🔥 Train: 0.9737 | 🎯 Val: 0.9919\n",
            "✅ Valid loss improved. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🏋️ Training:  15%|█▌        | 15/100 [7:40:16<43:09:44, 1828.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 015 | ⏱️ 30m 29s | 🔥 Train: 0.9705 | 🎯 Val: 0.9915\n",
            "⏳ EarlyStopping counter: 1 out of 10\n",
            "📅 Epoch 016 | ⏱️ 30m 24s | 🔥 Train: 0.9667 | 🎯 Val: 0.9895\n",
            "✅ Valid loss improved. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🏋️ Training:  17%|█▋        | 17/100 [8:41:14<42:09:50, 1828.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 017 | ⏱️ 30m 32s | 🔥 Train: 0.9625 | 🎯 Val: 0.9887\n",
            "⏳ EarlyStopping counter: 1 out of 10\n",
            "📅 Epoch 018 | ⏱️ 30m 25s | 🔥 Train: 0.9583 | 🎯 Val: 0.9869\n",
            "✅ Valid loss improved. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🏋️ Training:  19%|█▉        | 19/100 [9:42:14<41:10:16, 1829.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 019 | ⏱️ 30m 33s | 🔥 Train: 0.9536 | 🎯 Val: 0.9860\n",
            "⏳ EarlyStopping counter: 1 out of 10\n",
            "📅 Epoch 020 | ⏱️ 30m 26s | 🔥 Train: 0.9492 | 🎯 Val: 0.9848\n",
            "✅ Valid loss improved. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r🏋️ Training:  20%|██        | 20/100 [10:12:41<40:38:36, 1828.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 021 | ⏱️ 30m 33s | 🔥 Train: 0.9447 | 🎯 Val: 0.9835\n",
            "✅ Valid loss improved. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r🏋️ Training:  21%|██        | 21/100 [10:43:16<40:10:22, 1830.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 022 | ⏱️ 30m 33s | 🔥 Train: 0.9405 | 🎯 Val: 0.9821\n",
            "✅ Valid loss improved. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🏋️ Training:  23%|██▎       | 23/100 [11:44:25<39:11:58, 1832.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 023 | ⏱️ 30m 34s | 🔥 Train: 0.9368 | 🎯 Val: 0.9818\n",
            "⏳ EarlyStopping counter: 1 out of 10\n",
            "📅 Epoch 024 | ⏱️ 30m 26s | 🔥 Train: 0.9338 | 🎯 Val: 0.9807\n",
            "✅ Valid loss improved. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🏋️ Training:  25%|██▌       | 25/100 [12:45:27<38:10:17, 1832.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 025 | ⏱️ 30m 34s | 🔥 Train: 0.9308 | 🎯 Val: 0.9805\n",
            "⏳ EarlyStopping counter: 1 out of 10\n",
            "📅 Epoch 026 | ⏱️ 30m 29s | 🔥 Train: 0.9289 | 🎯 Val: 0.9796\n",
            "✅ Valid loss improved. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🏋️ Training:  27%|██▋       | 27/100 [13:46:31<37:09:05, 1832.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 027 | ⏱️ 30m 33s | 🔥 Train: 0.9275 | 🎯 Val: 0.9796\n",
            "⏳ EarlyStopping counter: 1 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r🏋️ Training:  28%|██▊       | 28/100 [14:17:00<36:37:21, 1831.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 028 | ⏱️ 30m 28s | 🔥 Train: 0.9265 | 🎯 Val: 0.9789\n",
            "⏳ EarlyStopping counter: 2 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r🏋️ Training:  29%|██▉       | 29/100 [14:47:28<36:05:50, 1830.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 029 | ⏱️ 30m 28s | 🔥 Train: 0.9262 | 🎯 Val: 0.9790\n",
            "⏳ EarlyStopping counter: 3 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r🏋️ Training:  30%|███       | 30/100 [15:17:58<35:35:14, 1830.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 030 | ⏱️ 30m 29s | 🔥 Train: 0.9259 | 🎯 Val: 0.9792\n",
            "⏳ EarlyStopping counter: 4 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r🏋️ Training:  31%|███       | 31/100 [15:48:28<35:04:47, 1830.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 031 | ⏱️ 30m 30s | 🔥 Train: 0.9247 | 🎯 Val: 0.9786\n",
            "⏳ EarlyStopping counter: 5 out of 10\n",
            "📅 Epoch 032 | ⏱️ 30m 28s | 🔥 Train: 0.9147 | 🎯 Val: 0.9752\n",
            "✅ Valid loss improved. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r🏋️ Training:  32%|███▏      | 32/100 [16:18:58<34:33:59, 1829.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 033 | ⏱️ 30m 38s | 🔥 Train: 0.9037 | 🎯 Val: 0.9702\n",
            "✅ Valid loss improved. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🏋️ Training:  34%|███▍      | 34/100 [17:20:14<33:37:19, 1833.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 034 | ⏱️ 30m 36s | 🔥 Train: 0.8892 | 🎯 Val: 0.9702\n",
            "⏳ EarlyStopping counter: 1 out of 10\n",
            "📅 Epoch 035 | ⏱️ 30m 27s | 🔥 Train: 0.8724 | 🎯 Val: 0.9628\n",
            "✅ Valid loss improved. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r🏋️ Training:  35%|███▌      | 35/100 [17:50:42<33:04:49, 1832.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 036 | ⏱️ 30m 24s | 🔥 Train: 0.8547 | 🎯 Val: 0.9593\n",
            "✅ Valid loss improved. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r🏋️ Training:  36%|███▌      | 36/100 [18:21:07<32:32:05, 1830.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 037 | ⏱️ 30m 22s | 🔥 Train: 0.8352 | 🎯 Val: 0.9540\n",
            "✅ Valid loss improved. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r🏋️ Training:  37%|███▋      | 37/100 [18:51:31<31:59:34, 1828.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 038 | ⏱️ 30m 19s | 🔥 Train: 0.8111 | 🎯 Val: 0.9492\n",
            "✅ Valid loss improved. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r🏋️ Training:  38%|███▊      | 38/100 [19:21:51<31:26:40, 1825.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 039 | ⏱️ 30m 11s | 🔥 Train: 0.7888 | 🎯 Val: 0.9467\n",
            "✅ Valid loss improved. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r🏋️ Training:  39%|███▉      | 39/100 [19:52:04<30:52:14, 1821.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 040 | ⏱️ 30m 18s | 🔥 Train: 0.7671 | 🎯 Val: 0.9404\n",
            "✅ Valid loss improved. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r🏋️ Training:  40%|████      | 40/100 [20:22:23<30:21:00, 1821.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 041 | ⏱️ 30m 12s | 🔥 Train: 0.7392 | 🎯 Val: 0.9380\n",
            "✅ Valid loss improved. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r🏋️ Training:  41%|████      | 41/100 [20:52:36<29:48:32, 1818.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 042 | ⏱️ 30m 17s | 🔥 Train: 0.7138 | 🎯 Val: 0.9328\n",
            "✅ Valid loss improved. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r🏋️ Training:  42%|████▏     | 42/100 [21:22:55<29:18:08, 1818.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 043 | ⏱️ 30m 18s | 🔥 Train: 0.6894 | 🎯 Val: 0.9266\n",
            "✅ Valid loss improved. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r🏋️ Training:  43%|████▎     | 43/100 [21:53:15<28:48:08, 1819.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 044 | ⏱️ 30m 5s | 🔥 Train: 0.6644 | 🎯 Val: 0.9218\n",
            "✅ Valid loss improved. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r🏋️ Training:  44%|████▍     | 44/100 [22:23:22<28:14:22, 1815.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 045 | ⏱️ 30m 4s | 🔥 Train: 0.6418 | 🎯 Val: 0.9174\n",
            "✅ Valid loss improved. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🏋️ Training:  46%|████▌     | 46/100 [23:23:29<27:08:23, 1809.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📅 Epoch 046 | ⏱️ 30m 2s | 🔥 Train: 0.6221 | 🎯 Val: 0.9175\n",
            "⏳ EarlyStopping counter: 1 out of 10\n",
            "📅 Epoch 047 | ⏱️ 29m 56s | 🔥 Train: 0.5964 | 🎯 Val: 0.9152\n",
            "✅ Valid loss improved. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r🏋️ Training:  47%|████▋     | 47/100 [23:53:26<26:34:56, 1805.60s/it]"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # Set working directory\n",
        "    target_dir = \"/content/drive/MyDrive/PhDwork/Segmentation\"\n",
        "    os.chdir(target_dir)\n",
        "    print(\"📁 Current Directory:\", os.getcwd())\n",
        "\n",
        "    # Training configuration\n",
        "    batch_size = 16\n",
        "    num_epochs = 100\n",
        "    lr = 1e-5\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    transformation = \"OriginalCT_With_Empty_NonEmpty_slices_In_Train\"\n",
        "\n",
        "    # Define paths\n",
        "    output_dir = os.path.join(\".\",\"results\",f\"Results_OriginalCT_Fifty_Fifty_TverskyLoss_And_Strong_Augmentation\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    loss_result_file = os.path.join(output_dir, \"train_and_valid_loss_results.csv\")\n",
        "    model_file = os.path.join(output_dir, \"model.pth\")\n",
        "    test_metrics_file = os.path.join(output_dir, \"test_metrics.csv\")\n",
        "    test_result_path = os.path.join(output_dir, \"test_outputs\")\n",
        "    os.makedirs(test_result_path, exist_ok=True)\n",
        "\n",
        "    DATASET_DIR = f\"./datasets/Datasets_{transformation}\"\n",
        "    train_path = os.path.join(DATASET_DIR, \"train_dataset.hdf5\")\n",
        "    valid_path = os.path.join(DATASET_DIR, \"valid_dataset.hdf5\")\n",
        "    test_path = os.path.join(DATASET_DIR, \"test_dataset.hdf5\")\n",
        "\n",
        "    train_transform=StrongJointTransform()\n",
        "\n",
        "    print(\"📦 Loading datasets...\")\n",
        "    train_dataset = HDF5SegmentationDataset(train_path,transform=train_transform)\n",
        "    valid_dataset = HDF5SegmentationDataset(valid_path,transform=None)\n",
        "    test_dataset = HDF5SegmentationDataset(test_path,transform=None)\n",
        "\n",
        "    sampler = BalancedTumorSampler(train_dataset,\n",
        "                               tumor_ratio=0.5,\n",
        "                               shuffle=True,\n",
        "                               index_cache_path=f'{DATASET_DIR}/tumor_indices.pkl')\n",
        "\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler,num_workers=0, pin_memory=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
        "    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "    print(f\"✅ Dataset sizes — Train: {len(train_dataset)}, Valid: {len(valid_dataset)}, Test: {len(test_dataset)}\")\n",
        "\n",
        "    # Initialize and run training\n",
        "    trainer = UnetTrain(\n",
        "        model_file=model_file,\n",
        "        loss_result_path=loss_result_file,\n",
        "        lr=lr,\n",
        "        num_epochs=num_epochs,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    trainer.execute(train_loader, valid_loader)\n",
        "\n",
        "    # Load best model for testing\n",
        "    model = UNet(in_channels=1, out_channels=1).to(device)\n",
        "    checkpoint = torch.load(model_file, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Test and save metrics\n",
        "    tester = UnetTest(test_result_path,test_metrics_file,device)\n",
        "    tester.test(model, test_loader)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mp.set_start_method('spawn')  # Required for some multiprocessing backends\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKImglO4-twL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "import os\n",
        "import csv\n",
        "\n",
        "\n",
        "class LossPlotter:\n",
        "    def __init__(self, csv_path: str):\n",
        "        self.csv_path = Path(csv_path)\n",
        "        self.data = self._load_data()\n",
        "\n",
        "    def _load_data(self):\n",
        "        if not self.csv_path.exists():\n",
        "            raise FileNotFoundError(f\"CSV file not found: {self.csv_path}\")\n",
        "        df = pd.read_csv(self.csv_path, index_col=0)  # Read row labels as index\n",
        "        return df.transpose()  # Make rows into columns\n",
        "\n",
        "    def plot(self, title: str = \"Training and Validation Loss\", save_path= None):\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.plot(self.data.index, self.data['Train Loss'], label='Train Loss', color='blue')\n",
        "        plt.plot(self.data.index, self.data['Valid Loss'], label='Valid Loss', color='orange')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title(title)\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            save_path = Path(save_path)\n",
        "            save_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            plt.savefig(save_path, format='pdf')\n",
        "            print(f\"[INFO] Loss plot saved to {save_path}\")\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "        plt.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    target_dir = \"/content/drive/MyDrive/PhDwork/Segmentation\"\n",
        "    os.chdir(target_dir)\n",
        "    loss_result_file = os.path.join(\".\",\"results\",f\"Results_OriginalCT_With_Balanced_Empty_NonEmpty_slices_In_Train_Augmentation\",\"train_and_valid_loss_results.csv\")\n",
        "    plotter = LossPlotter(loss_result_file)\n",
        "    plotter.plot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyeB21BYGQPu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyM9aPz/82b5NuB6+k00uT5q",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}