{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sajidcsecu/radioGenomic/blob/main/3DUnetinGPU_(Nifti_MONAI6Balanced).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr2QqHlZ8GZB"
      },
      "source": [
        "# This is the Code for the Segmentation on Rider Dataset (LUNG1). The Code is worked on the 3D volume over GPU. This is the improved of Model 5\n",
        "1. the architecture is 3DUNet\n",
        "2. The balanced sampler, preprocessed data (uniform volume spacing and clipping [-1000, 700]) and the\n",
        "3. strong augmentation is used in the code...\n",
        "4. Dice Loss is 1 and Binary classification Entropy is 1\n",
        "5. Number of positive pixels is for all patients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9Zo7tkcI1CX"
      },
      "source": [
        "# (1) Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d9OVdEeKXpMe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "484cd454-68bd-4256-f6a4-545af947536f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.12/dist-packages (2.5.2)\n",
            "Requirement already satisfied: pydicom===2.4.3 in /usr/local/lib/python3.12/dist-packages (2.4.3)\n",
            "Requirement already satisfied: pydicom-seg in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: SimpleITK>1.2.4 in /usr/local/lib/python3.12/dist-packages (from pydicom-seg) (2.5.2)\n",
            "Requirement already satisfied: jsonschema<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from pydicom-seg) (3.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from pydicom-seg) (1.26.4)\n",
            "Requirement already satisfied: pydicom>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from pydicom-seg) (2.4.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (25.4.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (0.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (75.2.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (1.17.0)\n",
            "Collecting numpy==1.23.5\n",
            "  Using cached numpy-1.23.5.tar.gz (10.7 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m√ó\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m√ó\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "Collecting monai\n",
            "  Downloading monai-1.5.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.24 in /usr/local/lib/python3.12/dist-packages (from monai) (1.26.4)\n",
            "Requirement already satisfied: torch>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from monai) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.1->monai) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.1->monai) (3.0.3)\n",
            "Downloading monai-1.5.1-py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: monai\n",
            "Successfully installed monai-1.5.1\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.13.1 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0, 2.7.1, 2.8.0, 2.9.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.13.1\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install SimpleITK\n",
        "!pip install pydicom===2.4.3\n",
        "!pip install pydicom-seg\n",
        "!pip install numpy==1.23.5\n",
        "!pip install monai\n",
        "!pip install torch==1.13.1\n",
        "!pip install nibabel>=5.0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JadHvjQcJ-qU"
      },
      "source": [
        "\n",
        "# (2) Import required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pmtDNjxMbfB4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import csv\n",
        "import time\n",
        "import random\n",
        "import shutil\n",
        "from glob import glob\n",
        "from typing import List\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.cuda.amp as amp\n",
        "from torch.optim import lr_scheduler\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.transforms import AsDiscrete\n",
        "from monai.networks.nets import UNet\n",
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    LoadImaged,\n",
        "    EnsureChannelFirstd,\n",
        "    Spacingd,\n",
        "    Orientationd,\n",
        "    ScaleIntensityRanged,\n",
        "    CropForegroundd,\n",
        "    ResizeWithPadOrCropd,\n",
        "    RandCropByPosNegLabeld,\n",
        "    RandFlipd,\n",
        "    RandAffined,\n",
        "    RandGaussianNoised,\n",
        "    RandScaleIntensityd,\n",
        "    ToTensord,\n",
        "    EnsureTyped,\n",
        "    EnsureChannelFirstD,\n",
        "    SpatialPadd,\n",
        "    Rand3DElasticd,\n",
        "    NormalizeIntensityd,\n",
        "    RandGaussianSmoothd,\n",
        "    RandAdjustContrastd\n",
        "\n",
        ")\n",
        "import json\n",
        "from monai.data import CacheDataset\n",
        "from monai.transforms import Transform\n",
        "from monai.data import CacheDataset\n",
        "import torch.nn.functional as F\n",
        "from monai.data import Dataset, DataLoader, CacheDataset, pad_list_data_collate\n",
        "from monai.networks.layers import Norm\n",
        "import nibabel as nib\n",
        "from sklearn.metrics import jaccard_score, f1_score, recall_score, precision_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import multiprocessing as mp\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from monai.transforms import EnsureTyped\n",
        "from monai.transforms import SaveImaged\n",
        "from monai.utils import set_determinism\n",
        "from monai.metrics import DiceMetric, HausdorffDistanceMetric, SurfaceDistanceMetric\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"Num foregrounds 0, Num backgrounds.*unable to generate class balanced samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyzguRDWI9bM"
      },
      "source": [
        "# (3) Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq6jVaaMXZz5",
        "outputId": "bb335210-e7c0-4ec3-abbc-6470311174bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cGu3zWdW3jje"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFrRJqgG7wxo"
      },
      "source": [
        "## (4). Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "944_4uJbmPPx"
      },
      "outputs": [],
      "source": [
        "class ImprovedDiceFocalLoss(nn.Module):\n",
        "    def __init__(self, smooth=1e-6, dice_weight=0.7, focal_weight=0.3, gamma=2.0, alpha=0.25, pos_weight=None):\n",
        "        super().__init__()\n",
        "        self.smooth = smooth\n",
        "        self.dice_weight = dice_weight\n",
        "        self.focal_weight = focal_weight\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.pos_weight = pos_weight  # Store pos_weight\n",
        "\n",
        "    def forward(self, preds, targets):\n",
        "        preds_sigmoid = torch.sigmoid(preds)\n",
        "\n",
        "        # Dice Loss\n",
        "        intersection = (preds_sigmoid * targets).sum()\n",
        "        dice_loss = 1 - (2. * intersection + self.smooth) / (\n",
        "            preds_sigmoid.sum() + targets.sum() + self.smooth\n",
        "        )\n",
        "\n",
        "        # Focal Loss with pos_weight\n",
        "        if self.pos_weight is not None:\n",
        "            # Apply pos_weight to the focal loss\n",
        "            bce_loss = F.binary_cross_entropy_with_logits(\n",
        "                preds, targets,\n",
        "                reduction='none',\n",
        "                pos_weight=self.pos_weight  # This is the key change!\n",
        "            )\n",
        "        else:\n",
        "            bce_loss = F.binary_cross_entropy_with_logits(preds, targets, reduction='none')\n",
        "\n",
        "        pt = torch.exp(-bce_loss)\n",
        "        focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss\n",
        "\n",
        "        return self.dice_weight * dice_loss + self.focal_weight * focal_loss.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZZ3Gu-DD88X"
      },
      "source": [
        "# (5). Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "T4VsKIzmFGLP"
      },
      "outputs": [],
      "source": [
        "class UnetTest:\n",
        "    def __init__(self, test_result_path: str, metrics_csv: str, device: torch.device):\n",
        "        # Remove super().__init__() since there's no parent class\n",
        "        self.test_result_path = test_result_path\n",
        "        self.metrics_csv = metrics_csv\n",
        "        self.device = device\n",
        "\n",
        "        self.dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
        "        self.hd95_metric = HausdorffDistanceMetric(include_background=False, percentile=95, reduction=\"mean\")\n",
        "        self.asd_metric = SurfaceDistanceMetric(include_background=False, reduction=\"mean\")\n",
        "\n",
        "        # Create test output directory\n",
        "        os.makedirs(self.test_result_path, exist_ok=True)\n",
        "\n",
        "        # Update CSV headers to include new metrics\n",
        "        self._init_enhanced_metrics_csv()\n",
        "\n",
        "    def _init_enhanced_metrics_csv(self):\n",
        "        \"\"\"Initialize CSV with additional metric columns\"\"\"\n",
        "        if not os.path.exists(self.metrics_csv):\n",
        "            with open(self.metrics_csv, 'w', newline='') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([\n",
        "                    \"SampleID\", \"Jaccard\", \"F1\", \"Recall\", \"Precision\", \"Accuracy\",\n",
        "                    \"Dice\", \"HD95\", \"ASD\", \"Time\"\n",
        "                ])\n",
        "\n",
        "    def calculate_basic_metrics(self, y_true: np.ndarray, y_pred: np.ndarray):\n",
        "        \"\"\"Calculate basic segmentation metrics (replacement for missing parent class)\"\"\"\n",
        "        # If you had a parent class with calculate_metrics, implement it here\n",
        "        eps = 1e-8\n",
        "\n",
        "        tp = np.sum(y_true * y_pred)\n",
        "        fp = np.sum((1 - y_true) * y_pred)\n",
        "        fn = np.sum(y_true * (1 - y_pred))\n",
        "        tn = np.sum((1 - y_true) * (1 - y_pred))\n",
        "\n",
        "        jaccard = tp / (tp + fp + fn + eps)\n",
        "        f1 = 2 * tp / (2 * tp + fp + fn + eps)\n",
        "        recall = tp / (tp + fn + eps)\n",
        "        precision = tp / (tp + fp + eps)\n",
        "        accuracy = (tp + tn) / (tp + tn + fp + fn + eps)\n",
        "\n",
        "        return [jaccard, f1, recall, precision, accuracy]\n",
        "\n",
        "    def calculate_comprehensive_metrics(self, y_true: np.ndarray, y_pred: np.ndarray):\n",
        "        \"\"\"Calculate both basic and medical image metrics\"\"\"\n",
        "        # Use the new basic metrics method\n",
        "        basic_metrics = self.calculate_basic_metrics(y_true, y_pred)\n",
        "\n",
        "        # Convert to torch tensors for MONAI metrics\n",
        "        y_true_t = torch.from_numpy(y_true.astype(np.float32)).unsqueeze(0).unsqueeze(0)\n",
        "        y_pred_t = torch.from_numpy(y_pred.astype(np.float32)).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        # Calculate MONAI metrics\n",
        "        dice_value = self.dice_metric(y_pred_t, y_true_t)\n",
        "        hd95_value = self.hd95_metric(y_pred_t, y_true_t)\n",
        "        asd_value = self.asd_metric(y_pred_t, y_true_t)\n",
        "\n",
        "        # Reset metrics for next calculation\n",
        "        self.dice_metric.reset()\n",
        "        self.hd95_metric.reset()\n",
        "        self.asd_metric.reset()\n",
        "\n",
        "        return basic_metrics + [\n",
        "            dice_value.item() if not dice_value.isnan() else 0.0,\n",
        "            hd95_value.item() if not hd95_value.isnan() else 0.0,\n",
        "            asd_value.item() if not asd_value.isnan() else 0.0\n",
        "        ]\n",
        "\n",
        "    def save_result_slices(self, image_np: np.ndarray, pred_np: np.ndarray, label_np: np.ndarray, sample_id: str):\n",
        "        \"\"\"Save sample slices for visualization\"\"\"\n",
        "        try:\n",
        "            # Find slices with predictions for better visualization\n",
        "            pred_slices = np.where(np.any(pred_np, axis=(0, 1)))[0]\n",
        "            label_slices = np.where(np.any(label_np, axis=(0, 1)))[0]\n",
        "\n",
        "            # Combine and get unique slices of interest\n",
        "            slices_of_interest = np.unique(np.concatenate([pred_slices, label_slices]))\n",
        "\n",
        "            # If no positive slices, use center slices\n",
        "            if len(slices_of_interest) == 0:\n",
        "                slices_of_interest = [pred_np.shape[2] // 2 - 1, pred_np.shape[2] // 2, pred_np.shape[2] // 2 + 1]\n",
        "\n",
        "            # Save a few representative slices\n",
        "            for i, slice_idx in enumerate(slices_of_interest[:3]):  # Save max 3 slices\n",
        "                if 0 <= slice_idx < pred_np.shape[2]:\n",
        "                    # Create a simple visualization\n",
        "                    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "                    axes[0].imshow(image_np[:, :, slice_idx], cmap='gray')\n",
        "                    axes[0].set_title(f'Image - Slice {slice_idx}')\n",
        "                    axes[0].axis('off')\n",
        "\n",
        "                    axes[1].imshow(label_np[:, :, slice_idx], cmap='jet', alpha=0.7)\n",
        "                    axes[1].set_title('Ground Truth')\n",
        "                    axes[1].axis('off')\n",
        "\n",
        "                    axes[2].imshow(pred_np[:, :, slice_idx], cmap='jet', alpha=0.7)\n",
        "                    axes[2].set_title('Prediction')\n",
        "                    axes[2].axis('off')\n",
        "\n",
        "                    plt.tight_layout()\n",
        "                    save_path = os.path.join(self.test_result_path, f\"{sample_id}_slice_{slice_idx}.png\")\n",
        "                    plt.savefig(save_path, dpi=100, bbox_inches='tight')\n",
        "                    plt.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Could not save slices for {sample_id}: {e}\")\n",
        "\n",
        "    def append_metrics_to_csv(self, sample_id: str, metrics: list, elapsed_time: float):\n",
        "        \"\"\"Override to handle extended metrics\"\"\"\n",
        "        with open(self.metrics_csv, 'a', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            # Now metrics should have: [jaccard, f1, recall, precision, accuracy, dice, hd95, asd]\n",
        "            writer.writerow([sample_id] + [f\"{m:.4f}\" for m in metrics] + [f\"{elapsed_time:.4f}\"])\n",
        "\n",
        "    def test(self, model: nn.Module, test_loader: DataLoader):\n",
        "        model.eval()\n",
        "        total_metrics = np.zeros(8)  # Now 8 metrics total\n",
        "        total_times = []\n",
        "\n",
        "        # Count samples with actual predictions\n",
        "        samples_with_predictions = 0\n",
        "\n",
        "        roi_size = (96, 96, 96)\n",
        "        sw_batch_size = 1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(test_loader):\n",
        "                image, label = batch[\"vol\"].to(self.device), batch[\"seg\"].to(self.device)\n",
        "                start_time = time.time()\n",
        "\n",
        "                pred = sliding_window_inference(\n",
        "                    inputs=image,\n",
        "                    roi_size=roi_size,\n",
        "                    sw_batch_size=sw_batch_size,\n",
        "                    predictor=model\n",
        "                )\n",
        "                pred = torch.sigmoid(pred) > 0.5\n",
        "\n",
        "                elapsed = time.time() - start_time\n",
        "                total_times.append(elapsed)\n",
        "\n",
        "                # Convert to NumPy\n",
        "                image_np = image[0, 0].cpu().numpy()\n",
        "                label_np = label[0, 0].cpu().numpy()\n",
        "                pred_np = pred[0, 0].cpu().numpy().astype(np.uint8)\n",
        "\n",
        "                # Skip if no predictions AND no ground truth (empty case)\n",
        "                if pred_np.sum() == 0 and label_np.sum() == 0:\n",
        "                    print(f\"üìù Sample {batch_idx}: No predictions and no ground truth - skipping\")\n",
        "                    continue\n",
        "\n",
        "                # Enhanced metrics calculation\n",
        "                metrics = self.calculate_comprehensive_metrics(label_np, pred_np)\n",
        "                total_metrics += np.array(metrics)\n",
        "                samples_with_predictions += 1\n",
        "\n",
        "                sample_id = f\"sample_{batch_idx:03d}\"\n",
        "                self.save_result_slices(image_np, pred_np, label_np, sample_id)\n",
        "                self.append_metrics_to_csv(sample_id, metrics, elapsed)\n",
        "\n",
        "                # Print sample-level results\n",
        "                print(f\"üìä Sample {batch_idx}: Dice={metrics[5]:.4f}, HD95={metrics[6]:.2f}mm, Time={elapsed:.2f}s\")\n",
        "\n",
        "        # Print enhanced summary (only for samples with meaningful data)\n",
        "        if samples_with_predictions > 0:\n",
        "            num_samples = samples_with_predictions\n",
        "            metric_names = [\"Jaccard\", \"F1\", \"Recall\", \"Precision\", \"Accuracy\", \"Dice\", \"HD95\", \"ASD\"]\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"üìä ENHANCED TEST METRICS SUMMARY:\")\n",
        "            print(\"=\"*60)\n",
        "            for i, name in enumerate(metric_names):\n",
        "                print(f\"{name:<12}: {total_metrics[i]/num_samples:.4f}\")\n",
        "            print(f\"üìà Samples evaluated: {num_samples}/{len(test_loader)}\")\n",
        "            print(f\"‚ö° Average FPS: {1 / np.mean(total_times):.2f}\")\n",
        "            print(f\"‚è±Ô∏è  Average time per sample: {np.mean(total_times):.2f}s\")\n",
        "        else:\n",
        "            print(\"‚ùå No samples with meaningful predictions to evaluate!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (6) Early Stopping"
      ],
      "metadata": {
        "id": "3JLRqnw5L7xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, verbose=True, min_delta=0, path='checkpoint.pt',\n",
        "                 start_val_loss_min=None, start_patience_counter=0, pos_weight=None):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.min_delta = min_delta\n",
        "        self.path = path\n",
        "        self.val_loss_min = start_val_loss_min if start_val_loss_min is not None else np.inf\n",
        "        self.counter = start_patience_counter\n",
        "        self.early_stop = False\n",
        "        self.pos_weight = pos_weight  # Store pos_weight\n",
        "\n",
        "    def __call__(self, val_loss, model, epoch=None, optimizer=None, pos_weight=None):\n",
        "        # Update pos_weight if provided\n",
        "        if pos_weight is not None:\n",
        "            self.pos_weight = pos_weight\n",
        "\n",
        "        improved = False\n",
        "        if val_loss < self.val_loss_min - self.min_delta:\n",
        "            self.val_loss_min = val_loss\n",
        "            self.counter = 0\n",
        "            improved = True\n",
        "            if self.verbose:\n",
        "                print(f\"‚úÖ Validation loss improved ({self.val_loss_min:.6f}). Saving model...\")\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f\"‚è≥ EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "\n",
        "        # Always save a full checkpoint with pos_weight\n",
        "        self.save_checkpoint(model, epoch, optimizer)\n",
        "\n",
        "        if self.counter >= self.patience:\n",
        "            self.early_stop = True\n",
        "\n",
        "        return self.early_stop\n",
        "\n",
        "    def save_checkpoint(self, model, epoch=None, optimizer=None):\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict() if optimizer else None,\n",
        "            'val_loss': self.val_loss_min,\n",
        "            'patience_counter': self.counter,\n",
        "            'pos_weight': self.pos_weight  # Save pos_weight here\n",
        "        }\n",
        "        torch.save(checkpoint, self.path)\n",
        "        if self.verbose and self.pos_weight is not None:\n",
        "            print(f\"üíæ Checkpoint saved with pos_weight: {self.pos_weight:.4f}\")"
      ],
      "metadata": {
        "id": "LA4lGL-xL6st"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa8L5nD2EVP_"
      },
      "source": [
        "# (7). Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "281KQS_iEIDX"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ---------- Trainer ----------\n",
        "\n",
        "class UnetTrain:\n",
        "    def __init__(self, model_file, loss_result_path, lr, num_epochs, device):\n",
        "        self.model_file = model_file\n",
        "        self.loss_result_path = loss_result_path\n",
        "        self.lr = lr\n",
        "        self.num_epochs = num_epochs\n",
        "        self.device = device\n",
        "        # Initialize pos_weight_file\n",
        "        self.pos_weight_file = model_file.replace('.pth', '_pos_weight.json')\n",
        "        self.seeding(42)\n",
        "\n",
        "    def seeding(self, seed):\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    def epoch_time(self, start_time, end_time):\n",
        "        elapsed = end_time - start_time\n",
        "        return int(elapsed / 60), int(elapsed % 60)\n",
        "\n",
        "    def train_one_epoch(self, model, loader, optimizer, scheduler, accumulation_steps, loss_fn):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        scaler = torch.amp.GradScaler()\n",
        "        device_type = 'cuda' if self.device.type == 'cuda' else 'cpu'\n",
        "\n",
        "        # Initialize patch monitoring variables\n",
        "        positive_patches = 0\n",
        "        total_patches = 0\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for i, batch in enumerate(loader):\n",
        "            inputs, labels = batch[\"vol\"].to(self.device, non_blocking=True), batch[\"seg\"].to(self.device, non_blocking=True)\n",
        "\n",
        "            # Monitor patch quality\n",
        "            batch_positives = labels.sum().item()\n",
        "            if batch_positives > 0:\n",
        "                positive_patches += 1\n",
        "\n",
        "            total_patches += 1\n",
        "\n",
        "            with torch.amp.autocast(device_type=device_type, enabled=True):\n",
        "                outputs = model(inputs)\n",
        "                loss = loss_fn(outputs, labels) / accumulation_steps\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (i + 1) % accumulation_steps == 0:\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "                if scheduler is not None:\n",
        "                    scheduler.step()\n",
        "\n",
        "            epoch_loss += loss.item() * accumulation_steps\n",
        "\n",
        "            # Progress reporting\n",
        "            if (i + 1) % 50 == 0:\n",
        "                print(f\"  Batch {i+1}/{len(loader)}, Loss: {loss.item() * accumulation_steps:.4f}\")\n",
        "\n",
        "        print(f\"  Patch Quality: {positive_patches}/{total_patches} ({positive_patches/total_patches*100:.1f}%) with positives\")\n",
        "        return epoch_loss / len(loader)\n",
        "\n",
        "    def evaluate(self, model, loader, loss_fn):\n",
        "        model.eval()\n",
        "        epoch_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in loader:\n",
        "                inputs, labels = batch[\"vol\"].to(self.device), batch[\"seg\"].to(self.device)\n",
        "                outputs = model(inputs)\n",
        "                loss = loss_fn(outputs, labels)\n",
        "                epoch_loss += loss.item()\n",
        "        return epoch_loss / len(loader)\n",
        "\n",
        "    def save_pos_weight(self, pos_weight):\n",
        "        \"\"\"Save pos_weight to JSON file\"\"\"\n",
        "        with open(self.pos_weight_file, 'w') as f:\n",
        "            json.dump({'pos_weight': pos_weight, 'timestamp': time.time()}, f)\n",
        "        print(f\"üíæ Saved pos_weight: {pos_weight:.4f}\")\n",
        "\n",
        "    def load_pos_weight(self):\n",
        "        \"\"\"Load pos_weight from JSON file\"\"\"\n",
        "        if os.path.exists(self.pos_weight_file):\n",
        "            try:\n",
        "                with open(self.pos_weight_file, 'r') as f:\n",
        "                    data = json.load(f)\n",
        "                    pos_weight = data['pos_weight']\n",
        "                    print(f\"üìÇ Loaded pos_weight: {pos_weight:.4f}\")\n",
        "                    return pos_weight\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error loading pos_weight: {e}\")\n",
        "        return None\n",
        "\n",
        "    def get_pos_weight(self, train_loader, force_recompute=False):\n",
        "        \"\"\"Get pos_weight - load if exists, otherwise compute and save\"\"\"\n",
        "        if not force_recompute:\n",
        "            pos_weight = self.load_pos_weight()\n",
        "            if pos_weight is not None:\n",
        "                return pos_weight\n",
        "\n",
        "        # Compute new pos_weight\n",
        "        pos_weight = self.compute_pos_weight(train_loader, max_batches=None)\n",
        "        self.save_pos_weight(pos_weight)\n",
        "        return pos_weight\n",
        "\n",
        "    def compute_pos_weight(self, train_loader, max_batches=None):\n",
        "        \"\"\"Compute pos_weight with better diagnostics\"\"\"\n",
        "        pos, neg = 0, 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            seg = batch[\"seg\"].to(self.device)\n",
        "            batch_pos = seg.sum().item()\n",
        "            batch_neg = seg.numel() - batch_pos\n",
        "\n",
        "            pos += batch_pos\n",
        "            neg += batch_neg\n",
        "            total_samples += 1\n",
        "\n",
        "            print(f\"Batch {i}: pos={batch_pos}, neg={batch_neg}, pos_ratio={batch_pos/seg.numel():.6f}\")\n",
        "\n",
        "            if max_batches and (i + 1) >= max_batches:\n",
        "                break\n",
        "\n",
        "        pos_ratio = pos / (pos + neg) if (pos + neg) > 0 else 0\n",
        "        pos_weight = neg / (pos + 1e-8)\n",
        "\n",
        "        print(f\"üìä Final Stats:\")\n",
        "        print(f\"  Total pos voxels: {pos}\")\n",
        "        print(f\"  Total neg voxels: {neg}\")\n",
        "        print(f\"  Positive ratio: {pos_ratio:.6f} ({pos_ratio*100:.4f}%)\")\n",
        "        print(f\"  Computed pos_weight: {pos_weight:.4f}\")\n",
        "        return pos_weight\n",
        "\n",
        "    def execute(self, train_loader, valid_loader):\n",
        "        # Use the smaller, faster model as I suggested\n",
        "        model = UNet(\n",
        "            spatial_dims=3,\n",
        "            in_channels=1,\n",
        "            out_channels=1,\n",
        "            channels=(32, 64, 128, 256, 512),  # Reduced channels\n",
        "            strides=(2, 2, 2, 2),\n",
        "            num_res_units=2,  # Reduced residual units\n",
        "            norm=Norm.INSTANCE,\n",
        "            dropout=0.1,\n",
        "            act='PRELU'  # PReLU for better gradient flow\n",
        "        ).to(self.device)\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=self.lr, weight_decay=1e-4)\n",
        "\n",
        "        scheduler = lr_scheduler.OneCycleLR(\n",
        "            optimizer,\n",
        "            max_lr=self.lr,\n",
        "            epochs=self.num_epochs,\n",
        "            steps_per_epoch=len(train_loader),\n",
        "            pct_start=0.1,\n",
        "            div_factor=10,\n",
        "            final_div_factor=100\n",
        "        )\n",
        "\n",
        "        # üîπ Auto-compute pos_weight before training\n",
        "        pos_weight = self.get_pos_weight(train_loader)\n",
        "        # Cap pos_weight to reasonable value\n",
        "        reasonable_pos_weight = min(pos_weight, 100.0)  # Maximum 100:1 imbalance\n",
        "        print(f\"üìä Using reasonable pos_weight: {reasonable_pos_weight:.2f} (capped from {pos_weight:.2f})\")\n",
        "\n",
        "        # ‚úÖ USE OPTION 1: Modified ImprovedDiceFocalLoss with pos_weight support\n",
        "        loss_fn = ImprovedDiceFocalLoss(\n",
        "            dice_weight=0.8,\n",
        "            focal_weight=0.2,\n",
        "            gamma=3.0,\n",
        "            alpha=0.8,\n",
        "            pos_weight=torch.tensor([reasonable_pos_weight], device=self.device)  # Add pos_weight here\n",
        "        )\n",
        "\n",
        "        accumulation_steps = 4\n",
        "\n",
        "        # ---- Resume training state ----\n",
        "        start_epoch = 1\n",
        "        start_val_loss_min = None\n",
        "        start_patience_counter = 0\n",
        "        history = {\"train_loss\": [], \"valid_loss\": []}\n",
        "\n",
        "        if os.path.exists(self.model_file):\n",
        "            checkpoint = torch.load(self.model_file, map_location=self.device)\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            if checkpoint.get('optimizer_state_dict'):\n",
        "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            start_epoch = checkpoint.get('epoch', 1) + 1\n",
        "            start_val_loss_min = checkpoint.get('val_loss', None)\n",
        "            start_patience_counter = checkpoint.get('patience_counter', 0)\n",
        "\n",
        "            # Load pos_weight from checkpoint if available\n",
        "            loaded_pos_weight = checkpoint.get('pos_weight')\n",
        "            if loaded_pos_weight is not None:\n",
        "                pos_weight = loaded_pos_weight\n",
        "                reasonable_pos_weight = min(pos_weight, 100.0)\n",
        "                # Recreate loss function with loaded pos_weight\n",
        "                loss_fn = ImprovedDiceFocalLoss(\n",
        "                    dice_weight=0.8,\n",
        "                    focal_weight=0.2,\n",
        "                    gamma=3.0,\n",
        "                    alpha=0.8,\n",
        "                    pos_weight=torch.tensor([reasonable_pos_weight], device=self.device)\n",
        "                )\n",
        "                print(f\"üìÇ Loaded pos_weight from checkpoint: {pos_weight:.4f}\")\n",
        "\n",
        "        # History loading code (you'll need to implement this part)\n",
        "        # if os.path.exists(self.loss_result_path):\n",
        "        #     ... your history loading code ...\n",
        "\n",
        "        early_stopping = EarlyStopping(\n",
        "            patience=10,\n",
        "            min_delta=0.0005,\n",
        "            path=self.model_file,\n",
        "            start_val_loss_min=start_val_loss_min,\n",
        "            start_patience_counter=start_patience_counter,\n",
        "            pos_weight=reasonable_pos_weight  # Use the capped value\n",
        "        )\n",
        "\n",
        "        if not os.path.exists(self.loss_result_path):\n",
        "            with open(self.loss_result_path, \"w\", newline=\"\") as f:\n",
        "                csv.writer(f).writerow([\"Epoch\", \"Train Loss\", \"Valid Loss\"])\n",
        "\n",
        "        # ---- Training loop ----\n",
        "        for epoch in range(start_epoch, self.num_epochs + 1):\n",
        "            start_time = time.time()\n",
        "\n",
        "            train_loss = self.train_one_epoch(\n",
        "                model, train_loader, optimizer, scheduler, accumulation_steps, loss_fn\n",
        "            )\n",
        "            valid_loss = self.evaluate(model, valid_loader, loss_fn)\n",
        "\n",
        "            mins, secs = self.epoch_time(start_time, time.time())\n",
        "            print(f\"Epoch {epoch:03d} | Time: {mins}m {secs}s | \"\n",
        "                  f\"Train: {train_loss:.6f} | Val: {valid_loss:.6f}\")\n",
        "\n",
        "            history['train_loss'].append(train_loss)\n",
        "            history['valid_loss'].append(valid_loss)\n",
        "\n",
        "            with open(self.loss_result_path, \"a\", newline=\"\") as f:\n",
        "                csv.writer(f).writerow([epoch, train_loss, valid_loss])\n",
        "\n",
        "            if early_stopping(valid_loss, model, epoch, optimizer, reasonable_pos_weight):\n",
        "                print(\"üõë Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "            torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this class definition with your other custom classes\n",
        "class CenterCropByPositiveRegiond(Transform):\n",
        "    \"\"\"Crop around regions that contain positive labels\"\"\"\n",
        "    def __init__(self, keys, label_key, spatial_size, num_samples=2):\n",
        "        self.keys = keys\n",
        "        self.label_key = label_key\n",
        "        self.spatial_size = spatial_size\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "    def __call__(self, data):\n",
        "        seg = data[self.label_key]\n",
        "\n",
        "        # Find coordinates of positive voxels\n",
        "        positive_coords = torch.nonzero(seg[0] > 0.5)  # [N, 3]\n",
        "\n",
        "        patches = []\n",
        "\n",
        "        if len(positive_coords) > 0:\n",
        "            # Sample different positive regions\n",
        "            for i in range(self.num_samples):\n",
        "                if i < len(positive_coords):\n",
        "                    center_coord = positive_coords[i % len(positive_coords)]\n",
        "                else:\n",
        "                    center_coord = positive_coords[torch.randint(0, len(positive_coords), (1,))[0]]\n",
        "\n",
        "                # Calculate crop bounds\n",
        "                spatial_size = self.spatial_size\n",
        "                roi_start = [\n",
        "                    max(0, center_coord[0].item() - spatial_size[0] // 2),\n",
        "                    max(0, center_coord[1].item() - spatial_size[1] // 2),\n",
        "                    max(0, center_coord[2].item() - spatial_size[2] // 2)\n",
        "                ]\n",
        "                roi_end = [\n",
        "                    min(seg.shape[1], roi_start[0] + spatial_size[0]),\n",
        "                    min(seg.shape[2], roi_start[1] + spatial_size[1]),\n",
        "                    min(seg.shape[3], roi_start[2] + spatial_size[2])\n",
        "                ]\n",
        "\n",
        "                # Apply crop\n",
        "                crop_transform = SpatialCropd(keys=self.keys, roi_start=roi_start, roi_end=roi_end)\n",
        "\n",
        "                try:\n",
        "                    patch = crop_transform(data)\n",
        "                    patches.append(patch)\n",
        "                except Exception as e:\n",
        "                    # Fallback to random crop if center crop fails\n",
        "                    print(f\"‚ö†Ô∏è Center crop failed: {e}, using random fallback\")\n",
        "                    rand_crop = RandCropByPosNegLabeld(\n",
        "                        keys=self.keys, label_key=self.label_key,\n",
        "                        spatial_size=self.spatial_size, num_samples=1\n",
        "                    )\n",
        "                    fallback_patches = rand_crop(data)\n",
        "                    if isinstance(fallback_patches, list):\n",
        "                        patches.extend(fallback_patches)\n",
        "                    else:\n",
        "                        patches.append(fallback_patches)\n",
        "        else:\n",
        "            # Fallback if no positives found\n",
        "            print(\"‚ö†Ô∏è No positive voxels found, using random sampling\")\n",
        "            rand_crop = RandCropByPosNegLabeld(\n",
        "                keys=self.keys, label_key=self.label_key,\n",
        "                spatial_size=self.spatial_size, num_samples=self.num_samples\n",
        "            )\n",
        "            patches = rand_crop(data)\n",
        "\n",
        "        return patches[:self.num_samples]"
      ],
      "metadata": {
        "id": "El6TbMoXGWE9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (8). Pipeline"
      ],
      "metadata": {
        "id": "BZrW1cHg4OlM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CwcTHPshqu0O",
        "outputId": "73a25034-1232-4638-d8d4-e5d8dad370bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Current Directory: /content/drive/MyDrive/PhDwork/Segmentation\n",
            "üì¶ Preparing datasets and transforms...\n",
            "üîÑ Loading training dataset...\n",
            "üìä Filtered 340 -> 340 non-empty samples\n",
            "üìÅ train: Loaded 340 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 34/34 [03:47<00:00,  6.68s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Loading validation dataset...\n",
            "üìÅ valid: Loaded 43 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [01:04<00:00,  8.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Loading test dataset...\n",
            "üìÅ test: Loaded 38 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:54<00:00,  7.78s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Filtered 340 -> 340 non-empty samples\n",
            "üìÅ train: Loaded 340 samples\n",
            "‚úÖ Dataset sizes ‚Äî Train: 340, Valid: 43, Test: 38\n",
            "\n",
            "============================================================\n",
            "üîç RUNNING DATASET QUALITY ANALYSIS...\n",
            "============================================================\n",
            "üìä Analyzing dataset quality...\n",
            "\n",
            "üìä TRAIN (Full Volumes) Dataset Analysis:\n",
            "----------------------------------------\n",
            "  Total volumes: 340\n",
            "  Volumes with positives: 108 (31.8%)\n",
            "  Overall positive ratio: 0.001642 (0.1642%)\n",
            "  Total positive voxels: 1,170,646.0\n",
            "  Total voxels: 713,031,680\n",
            "\n",
            "üìä VALID (Full Volumes) Dataset Analysis:\n",
            "----------------------------------------\n",
            "  Total volumes: 43\n",
            "  Volumes with positives: 16 (37.2%)\n",
            "  Overall positive ratio: 0.002132 (0.2132%)\n",
            "  Total positive voxels: 192,281.0\n",
            "  Total voxels: 90,177,536\n",
            "\n",
            "üìä TEST (Full Volumes) Dataset Analysis:\n",
            "----------------------------------------\n",
            "  Total volumes: 38\n",
            "  Volumes with positives: 14 (36.8%)\n",
            "  Overall positive ratio: 0.001469 (0.1469%)\n",
            "  Total positive voxels: 117,089.0\n",
            "  Total voxels: 79,691,776\n",
            "\n",
            "üîç Analyzing Training Patches (After Sampling):\n",
            "\n",
            "üìä TRAIN PATCHES Patch Analysis (first 20 batches):\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/monai/transforms/transform.py\", line 150, in apply_transform\n    return _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/monai/transforms/transform.py\", line 98, in _apply_transform\n    return transform(data, lazy=lazy) if isinstance(transform, LazyTrait) else transform(data)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/monai/transforms/croppad/dictionary.py\", line 997, in __call__\n    self.randomize(d.get(self.label_key), fg_indices, bg_indices, d.get(self.image_key))\n  File \"/usr/local/lib/python3.12/dist-packages/monai/transforms/croppad/dictionary.py\", line 979, in randomize\n    self.cropper.randomize(label=label, fg_indices=fg_indices, bg_indices=bg_indices, image=image)\n  File \"/usr/local/lib/python3.12/dist-packages/monai/transforms/croppad/array.py\", line 1152, in randomize\n    self.centers = generate_pos_neg_label_crop_centers(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/monai/transforms/utils.py\", line 690, in generate_pos_neg_label_crop_centers\n    centers.append(correct_crop_centers(center, spatial_size, label_spatial_shape, allow_smaller))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/monai/transforms/utils.py\", line 614, in correct_crop_centers\n    raise ValueError(\nValueError: The size of the proposed random crop ROI is larger than the image size, got ROI size (128, 128, 128) and label image size (127, 442, 1344) respectively.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/monai/data/dataset.py\", line 109, in __getitem__\n    return self._transform(index)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/monai/data/dataset.py\", line 914, in _transform\n    return super()._transform(index)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/monai/data/dataset.py\", line 95, in _transform\n    return self.transform(data_i)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/monai/transforms/compose.py\", line 346, in __call__\n    result = execute_compose(\n             ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/monai/transforms/compose.py\", line 116, in execute_compose\n    data = apply_transform(\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/monai/transforms/transform.py\", line 180, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <monai.transforms.croppad.dictionary.RandCropByPosNegLabeld object at 0x7e23576e7290>\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1724966775.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1724966775.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    421\u001b[0m     }\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m     \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnetPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m     \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1724966775.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üîç RUNNING DATASET QUALITY ANALYSIS...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze_dataset_quality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0manalyze_dataset_quality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1724966775.py\u001b[0m in \u001b[0;36manalyze_dataset_quality\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Analyze training patches (after patch sampling)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"üîç Analyzing Training Patches (After Sampling):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_analyze_patch_quality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TRAIN PATCHES\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_analyze_loader_quality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1724966775.py\u001b[0m in \u001b[0;36m_analyze_patch_quality\u001b[0;34m(self, loader, name, max_batches)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mtotal_voxels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1514\u001b[0m                 \u001b[0mworker_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1515\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1516\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1518\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data, worker_idx)\u001b[0m\n\u001b[1;32m   1549\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# be constructed, don't try to instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/monai/transforms/transform.py\", line 150, in apply_transform\n    return _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/monai/transforms/transform.py\", line 98, in _apply_transform\n    return transform(data, lazy=lazy) if isinstance(transform, LazyTrait) else transform(data)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/monai/transforms/croppad/dictionary.py\", line 997, in __call__\n    self.randomize(d.get(self.label_key), fg_indices, bg_indices, d.get(self.image_key))\n  File \"/usr/local/lib/python3.12/dist-packages/monai/transforms/croppad/dictionary.py\", line 979, in randomize\n    self.cropper.randomize(label=label, fg_indices=fg_indices, bg_indices=bg_indices, image=image)\n  File \"/usr/local/lib/python3.12/dist-packages/monai/transforms/croppad/array.py\", line 1152, in randomize\n    self.centers = generate_pos_neg_label_crop_centers(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/monai/transforms/utils.py\", line 690, in generate_pos_neg_label_crop_centers\n    centers.append(correct_crop_centers(center, spatial_size, label_spatial_shape, allow_smaller))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/monai/transforms/utils.py\", line 614, in correct_crop_centers\n    raise ValueError(\nValueError: The size of the proposed random crop ROI is larger than the image size, got ROI size (128, 128, 128) and label image size (127, 442, 1344) respectively.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/monai/data/dataset.py\", line 109, in __getitem__\n    return self._transform(index)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/monai/data/dataset.py\", line 914, in _transform\n    return super()._transform(index)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/monai/data/dataset.py\", line 95, in _transform\n    return self.transform(data_i)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/monai/transforms/compose.py\", line 346, in __call__\n    result = execute_compose(\n             ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/monai/transforms/compose.py\", line 116, in execute_compose\n    data = apply_transform(\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/monai/transforms/transform.py\", line 180, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <monai.transforms.croppad.dictionary.RandCropByPosNegLabeld object at 0x7e23576e7290>\n"
          ]
        }
      ],
      "source": [
        "class UnetPipeline:\n",
        "    def __init__(self, config: Dict):\n",
        "        self.config = config\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        set_determinism(seed=42)\n",
        "\n",
        "        # Setup paths\n",
        "        os.chdir(self.config['target_dir'])\n",
        "        print(f\"üìÅ Current Directory: {os.getcwd()}\")\n",
        "\n",
        "        self.output_dir = os.path.join(\".\", \"results\", self.config['output_folder_name'])\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "        self.loss_result_file = os.path.join(self.output_dir, \"train_and_valid_loss_results.csv\")\n",
        "        self.model_file = os.path.join(self.output_dir, \"model.pth\")\n",
        "        self.test_metrics_file = os.path.join(self.output_dir, \"test_metrics.csv\")\n",
        "        self.test_result_path = os.path.join(self.output_dir, \"test_outputs\")\n",
        "        os.makedirs(self.test_result_path, exist_ok=True)\n",
        "\n",
        "        self.dataset_dir = os.path.join(\"./datasets\", f\"Datasets_{self.config['transformation']}\")\n",
        "\n",
        "        # Prepare loaders\n",
        "        print(\"üì¶ Preparing datasets and transforms...\")\n",
        "        self.train_loader, self.valid_loader, self.test_loader, self.full_train_loader = self.prepare_loaders()\n",
        "\n",
        "        # Dataset quality analysis\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üîç RUNNING DATASET QUALITY ANALYSIS...\")\n",
        "        print(\"=\"*60)\n",
        "        self.analyze_dataset_quality()\n",
        "\n",
        "    def analyze_dataset_quality(self):\n",
        "        \"\"\"Comprehensive dataset quality analysis\"\"\"\n",
        "        print(\"üìä Analyzing dataset quality...\")\n",
        "\n",
        "        # Analyze full volumes (before patch sampling)\n",
        "        self._analyze_loader_quality(self.full_train_loader, \"TRAIN (Full Volumes)\")\n",
        "        self._analyze_loader_quality(self.valid_loader, \"VALID (Full Volumes)\")\n",
        "        self._analyze_loader_quality(self.test_loader, \"TEST (Full Volumes)\")\n",
        "\n",
        "        # Analyze training patches (after patch sampling)\n",
        "        print(\"\\n\" + \"üîç Analyzing Training Patches (After Sampling):\")\n",
        "        self._analyze_patch_quality(self.train_loader, \"TRAIN PATCHES\")\n",
        "\n",
        "    def _analyze_loader_quality(self, loader, name: str):\n",
        "        \"\"\"Analyze a specific data loader\"\"\"\n",
        "        print(f\"\\nüìä {name} Dataset Analysis:\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        total_volumes = 0\n",
        "        volumes_with_positives = 0\n",
        "        total_pos_voxels = 0\n",
        "        total_voxels = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(loader):\n",
        "            seg = batch[\"seg\"]\n",
        "\n",
        "            # Check each volume in batch\n",
        "            for i in range(seg.shape[0]):\n",
        "                volume_seg = seg[i]\n",
        "                total_volumes += 1\n",
        "                volume_positives = volume_seg.sum().item()\n",
        "                volume_voxels = volume_seg.numel()\n",
        "\n",
        "                if volume_positives > 0:\n",
        "                    volumes_with_positives += 1\n",
        "\n",
        "                total_pos_voxels += volume_positives\n",
        "                total_voxels += volume_voxels\n",
        "\n",
        "        overall_pos_ratio = total_pos_voxels / total_voxels if total_voxels > 0 else 0\n",
        "\n",
        "        print(f\"  Total volumes: {total_volumes}\")\n",
        "        print(f\"  Volumes with positives: {volumes_with_positives} ({volumes_with_positives/total_volumes*100:.1f}%)\")\n",
        "        print(f\"  Overall positive ratio: {overall_pos_ratio:.6f} ({overall_pos_ratio*100:.4f}%)\")\n",
        "        print(f\"  Total positive voxels: {total_pos_voxels:,}\")\n",
        "        print(f\"  Total voxels: {total_voxels:,}\")\n",
        "\n",
        "        return overall_pos_ratio\n",
        "\n",
        "    def _analyze_patch_quality(self, loader, name: str, max_batches: int = 20):\n",
        "        \"\"\"Analyze patch distribution in training loader\"\"\"\n",
        "        print(f\"\\nüìä {name} Patch Analysis (first {max_batches} batches):\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        total_patches = 0\n",
        "        patches_with_positives = 0\n",
        "        total_pos_voxels = 0\n",
        "        total_voxels = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(loader):\n",
        "            if batch_idx >= max_batches:\n",
        "                break\n",
        "\n",
        "            seg = batch[\"seg\"]\n",
        "\n",
        "            # Check each patch in batch\n",
        "            for i in range(seg.shape[0]):\n",
        "                patch_seg = seg[i]\n",
        "                total_patches += 1\n",
        "                patch_positives = patch_seg.sum().item()\n",
        "                patch_voxels = patch_seg.numel()\n",
        "\n",
        "                if patch_positives > 0:\n",
        "                    patches_with_positives += 1\n",
        "\n",
        "                total_pos_voxels += patch_positives\n",
        "                total_voxels += patch_voxels\n",
        "\n",
        "                pos_ratio = patch_positives / patch_voxels if patch_voxels > 0 else 0\n",
        "                if patch_positives > 0:  # Only print patches with positives for clarity\n",
        "                    print(f\"  Batch {batch_idx}, Patch {i}: {patch_positives}/{patch_voxels} \"\n",
        "                          f\"({pos_ratio*100:.3f}% positive)\")\n",
        "\n",
        "        overall_pos_ratio = total_pos_voxels / total_voxels if total_voxels > 0 else 0\n",
        "\n",
        "        print(f\"\\nüìà {name} Patch Summary:\")\n",
        "        print(f\"  Total patches analyzed: {total_patches}\")\n",
        "        print(f\"  Patches with positives: {patches_with_positives} ({patches_with_positives/total_patches*100:.1f}%)\")\n",
        "        print(f\"  Overall positive ratio: {overall_pos_ratio:.6f} ({overall_pos_ratio*100:.4f}%)\")\n",
        "\n",
        "        return overall_pos_ratio\n",
        "\n",
        "    def check_data_sanity(self, loader, name: str = \"train\"):\n",
        "        \"\"\"Check if data is loaded correctly\"\"\"\n",
        "        for i, batch in enumerate(loader):\n",
        "            vol = batch[\"vol\"]\n",
        "            seg = batch[\"seg\"]\n",
        "\n",
        "            print(f\"\\nüîç {name} Batch {i}:\")\n",
        "            print(f\"  Volume shape: {vol.shape}\")\n",
        "            print(f\"  Volume range: [{vol.min():.3f}, {vol.max():.3f}]\")\n",
        "            print(f\"  Seg shape: {seg.shape}\")\n",
        "            print(f\"  Seg unique values: {torch.unique(seg)}\")\n",
        "            print(f\"  Seg positives: {seg.sum().item()}/{seg.numel()} ({seg.sum().item()/seg.numel()*100:.3f}%)\")\n",
        "\n",
        "            if i >= 2:  # Check first 3 batches\n",
        "                break\n",
        "\n",
        "    def filter_empty_samples(self, file_list: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"Remove samples that have no positive segmentation labels\"\"\"\n",
        "        filtered_files = []\n",
        "\n",
        "        for file_pair in file_list:\n",
        "            # Load just the segmentation to check if it's empty\n",
        "            seg_loader = Compose([\n",
        "                LoadImaged(keys=[\"seg\"]),\n",
        "                EnsureChannelFirstd(keys=[\"seg\"]),\n",
        "            ])\n",
        "\n",
        "            try:\n",
        "                seg_data = seg_loader(file_pair)[\"seg\"]\n",
        "                if seg_data.sum() > 0:  # Only keep if has positive voxels\n",
        "                    filtered_files.append(file_pair)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error loading {file_pair['seg']}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"üìä Filtered {len(file_list)} -> {len(filtered_files)} non-empty samples\")\n",
        "        return filtered_files\n",
        "\n",
        "    def prepare_loaders(self) -> Tuple[DataLoader, DataLoader, DataLoader, DataLoader]:\n",
        "        \"\"\"Prepare data loaders for training, validation, and testing\"\"\"\n",
        "        pixdim = (1, 1, 1)\n",
        "        a_min, a_max = -1000, 700\n",
        "        patch_size = (128, 128, 128)\n",
        "\n",
        "        def get_files(split: str) -> List[Dict]:\n",
        "            \"\"\"Get file paths for a specific split with validation\"\"\"\n",
        "            ct_dir = os.path.join(self.dataset_dir, split, \"ct\")\n",
        "            seg_dir = os.path.join(self.dataset_dir, split, \"segment\")\n",
        "\n",
        "            # Validate directories exist\n",
        "            if not os.path.exists(ct_dir):\n",
        "                raise FileNotFoundError(f\"CT directory not found: {ct_dir}\")\n",
        "            if not os.path.exists(seg_dir):\n",
        "                raise FileNotFoundError(f\"Segmentation directory not found: {seg_dir}\")\n",
        "\n",
        "            ct_files = sorted(glob(os.path.join(ct_dir, \"*.nii.gz\")))\n",
        "            seg_files = sorted(glob(os.path.join(seg_dir, \"*.nii.gz\")))\n",
        "\n",
        "            if len(ct_files) == 0:\n",
        "                raise RuntimeError(f\"No CT files found in {ct_dir}\")\n",
        "            if len(seg_files) == 0:\n",
        "                raise RuntimeError(f\"No segmentation files found in {seg_dir}\")\n",
        "            if len(ct_files) != len(seg_files):\n",
        "                raise RuntimeError(f\"Mismatch CT/SEG for {split}: {len(ct_files)} vs {len(seg_files)}\")\n",
        "\n",
        "            # Create file pairs\n",
        "            files = []\n",
        "            for ct_file, seg_file in zip(ct_files, seg_files):\n",
        "                # Verify file correspondence\n",
        "                ct_id = os.path.basename(ct_file).replace('.nii.gz', '')\n",
        "                seg_id = os.path.basename(seg_file).replace('.nii.gz', '')\n",
        "                if ct_id != seg_id:\n",
        "                    print(f\"‚ö†Ô∏è Warning: File name mismatch - CT: {ct_id}, SEG: {seg_id}\")\n",
        "                files.append({\"vol\": ct_file, \"seg\": seg_file})\n",
        "\n",
        "            # Filter out empty samples for training\n",
        "            if split == \"train\":\n",
        "                files = self.filter_empty_samples(files)\n",
        "\n",
        "            print(f\"üìÅ {split}: Loaded {len(files)} samples\")\n",
        "            return files\n",
        "\n",
        "        # Common base transforms for ALL datasets\n",
        "        base_transforms = Compose([\n",
        "            LoadImaged(keys=[\"vol\", \"seg\"]),\n",
        "            EnsureChannelFirstd(keys=[\"vol\", \"seg\"]),\n",
        "            Spacingd(keys=[\"vol\", \"seg\"], pixdim=pixdim, mode=(\"bilinear\", \"nearest\")),\n",
        "            Orientationd(keys=[\"vol\", \"seg\"], axcodes=\"RAS\", labels=None),\n",
        "            ScaleIntensityRanged(keys=[\"vol\"], a_min=a_min, a_max=a_max, b_min=0.0, b_max=1.0, clip=True),\n",
        "            NormalizeIntensityd(keys=[\"vol\"], subtrahend=0.5, divisor=0.5),  # Normalize to [-1, 1]\n",
        "            CropForegroundd(keys=[\"vol\", \"seg\"], source_key=\"vol\"),\n",
        "            ResizeWithPadOrCropd(keys=[\"vol\", \"seg\"], spatial_size=patch_size, mode=\"constant\"),\n",
        "            ToTensord(keys=[\"vol\", \"seg\"]),\n",
        "            EnsureTyped(keys=[\"vol\", \"seg\"], track_meta=True),\n",
        "        ])\n",
        "\n",
        "        # Training transforms with aggressive positive sampling\n",
        "        train_transforms = Compose([\n",
        "            LoadImaged(keys=[\"vol\", \"seg\"]),\n",
        "            EnsureChannelFirstd(keys=[\"vol\", \"seg\"]),\n",
        "            Spacingd(keys=[\"vol\", \"seg\"], pixdim=pixdim, mode=(\"bilinear\", \"nearest\")),\n",
        "            Orientationd(keys=[\"vol\", \"seg\"], axcodes=\"RAS\", labels=None),\n",
        "            ScaleIntensityRanged(keys=[\"vol\"], a_min=a_min, a_max=a_max, b_min=0.0, b_max=1.0, clip=True),\n",
        "            NormalizeIntensityd(keys=[\"vol\"], subtrahend=0.5, divisor=0.5),\n",
        "            CropForegroundd(keys=[\"vol\", \"seg\"], source_key=\"vol\", margin=10),  # Add margin for context\n",
        "\n",
        "            # Critical: Force positive patches for training\n",
        "            RandCropByPosNegLabeld(\n",
        "                keys=[\"vol\", \"seg\"],\n",
        "                label_key=\"seg\",\n",
        "                spatial_size=patch_size,\n",
        "                pos=1.0,      # 100% positive samples\n",
        "                neg=0.0,      # 0% negative samples\n",
        "                num_samples=4,\n",
        "                image_key=\"vol\",\n",
        "                image_threshold=0,\n",
        "            ),\n",
        "\n",
        "            # Conservative augmentations\n",
        "            RandFlipd(keys=[\"vol\", \"seg\"], prob=0.5, spatial_axis=[0, 1, 2]),\n",
        "            RandGaussianNoised(keys=[\"vol\"], prob=0.2, mean=0.0, std=0.05),\n",
        "\n",
        "            ToTensord(keys=[\"vol\", \"seg\"]),\n",
        "            EnsureTyped(keys=[\"vol\", \"seg\"]),\n",
        "        ])\n",
        "\n",
        "        # Create datasets with optimized cache rates for 3D data\n",
        "        print(\"üîÑ Loading training dataset...\")\n",
        "        train_ds = CacheDataset(\n",
        "            data=get_files(\"train\"),\n",
        "            transform=train_transforms,\n",
        "            cache_rate=0.1,  # Reduced for 3D data\n",
        "            num_workers=2\n",
        "        )\n",
        "\n",
        "        print(\"üîÑ Loading validation dataset...\")\n",
        "        valid_ds = CacheDataset(\n",
        "            data=get_files(\"valid\"),\n",
        "            transform=base_transforms,\n",
        "            cache_rate=0.2,  # Reduced for 3D data\n",
        "            num_workers=2\n",
        "        )\n",
        "\n",
        "        print(\"üîÑ Loading test dataset...\")\n",
        "        test_ds = CacheDataset(\n",
        "            data=get_files(\"test\"),\n",
        "            transform=base_transforms,\n",
        "            cache_rate=0.2,  # Reduced for 3D data\n",
        "            num_workers=2\n",
        "        )\n",
        "\n",
        "        # Create data loaders with optimized settings\n",
        "        train_loader = DataLoader(\n",
        "            train_ds,\n",
        "            batch_size=self.config['batch_size'],\n",
        "            shuffle=True,\n",
        "            num_workers=min(4, os.cpu_count() // 2),  # Adaptive worker count\n",
        "            pin_memory=torch.cuda.is_available(),\n",
        "            persistent_workers=True,\n",
        "            prefetch_factor=2,\n",
        "            collate_fn=pad_list_data_collate,\n",
        "            drop_last=True  # Avoid partial batches\n",
        "        )\n",
        "\n",
        "        valid_loader = DataLoader(\n",
        "            valid_ds,\n",
        "            batch_size=self.config['batch_size'],\n",
        "            shuffle=False,\n",
        "            num_workers=min(4, os.cpu_count() // 2),\n",
        "            pin_memory=torch.cuda.is_available(),\n",
        "            # No persistent_workers for validation\n",
        "            collate_fn=pad_list_data_collate\n",
        "        )\n",
        "\n",
        "        test_loader = DataLoader(\n",
        "            test_ds,\n",
        "            batch_size=1,\n",
        "            shuffle=False,\n",
        "            num_workers=min(4, os.cpu_count() // 2),\n",
        "            pin_memory=torch.cuda.is_available(),\n",
        "            # No persistent_workers for test\n",
        "            collate_fn=pad_list_data_collate\n",
        "        )\n",
        "\n",
        "        # Full volume train loader for sanity checks (no patching)\n",
        "        full_train_ds = Dataset(get_files(\"train\"), base_transforms)\n",
        "        full_train_loader = DataLoader(\n",
        "            full_train_ds,\n",
        "            batch_size=1,\n",
        "            shuffle=False,\n",
        "            num_workers=0,\n",
        "            collate_fn=pad_list_data_collate\n",
        "        )\n",
        "\n",
        "        print(f\"‚úÖ Dataset sizes ‚Äî Train: {len(train_ds)}, Valid: {len(valid_ds)}, Test: {len(test_ds)}\")\n",
        "\n",
        "        return train_loader, valid_loader, test_loader, full_train_loader\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Execute training with resource monitoring\"\"\"\n",
        "        # Monitor resources\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"üéØ GPU Memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "            print(f\"üéØ GPU Memory cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
        "\n",
        "        # Validate we have data\n",
        "        if len(self.train_loader.dataset) == 0:\n",
        "            raise ValueError(\"No training data available!\")\n",
        "        if len(self.valid_loader.dataset) == 0:\n",
        "            raise ValueError(\"No validation data available!\")\n",
        "\n",
        "        trainer = UnetTrain(\n",
        "            model_file=self.model_file,\n",
        "            loss_result_path=self.loss_result_file,\n",
        "            lr=self.config['learning_rate'],\n",
        "            num_epochs=self.config['num_epochs'],\n",
        "            device=self.device\n",
        "        )\n",
        "        trainer.execute(self.train_loader, self.valid_loader)\n",
        "\n",
        "    def test(self):\n",
        "        \"\"\"Execute testing with checkpoint validation\"\"\"\n",
        "        # Use consistent architecture with training\n",
        "        model = UNet(\n",
        "            spatial_dims=3,\n",
        "            in_channels=1,\n",
        "            out_channels=1,\n",
        "            channels=(32, 64, 128, 256, 512),  # Must match training\n",
        "            strides=(2, 2, 2, 2),\n",
        "            num_res_units=2,\n",
        "            norm=Norm.INSTANCE,\n",
        "            dropout=0.1,\n",
        "            act='PRELU'\n",
        "        ).to(self.device)\n",
        "\n",
        "        if os.path.exists(self.model_file):\n",
        "            print(\"üìÇ Loading checkpoint...\")\n",
        "            checkpoint = torch.load(self.model_file, map_location=self.device)\n",
        "\n",
        "            # Validate checkpoint\n",
        "            if 'model_state_dict' not in checkpoint:\n",
        "                raise ValueError(\"Invalid checkpoint: missing 'model_state_dict'\")\n",
        "\n",
        "            try:\n",
        "                model.load_state_dict(checkpoint['model_state_dict'])\n",
        "                print(\"‚úÖ Checkpoint loaded successfully\")\n",
        "\n",
        "                # Print training info if available\n",
        "                if 'epoch' in checkpoint:\n",
        "                    print(f\"üìÖ Checkpoint from epoch: {checkpoint['epoch']}\")\n",
        "                if 'val_loss' in checkpoint:\n",
        "                    print(f\"üìâ Checkpoint validation loss: {checkpoint['val_loss']:.6f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Checkpoint loading failed: {e}\")\n",
        "                raise\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"Model file not found: {self.model_file}\")\n",
        "\n",
        "        tester = UnetTest(self.test_result_path, self.test_metrics_file, self.device)\n",
        "        tester.test(model, self.test_loader)\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Execute full pipeline with validation\"\"\"\n",
        "        print(\"üöÄ Starting pipeline with:\")\n",
        "        print(f\"   - Train samples: {len(self.train_loader.dataset)}\")\n",
        "        print(f\"   - Valid samples: {len(self.valid_loader.dataset)}\")\n",
        "        print(f\"   - Test samples: {len(self.test_loader.dataset)}\")\n",
        "        print(f\"   - Device: {self.device}\")\n",
        "        print(f\"   - Output directory: {self.output_dir}\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            self.train()\n",
        "            self.test()\n",
        "\n",
        "            total_time = time.time() - start_time\n",
        "            print(f\"‚úÖ Pipeline completed in {total_time/60:.2f} minutes\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Pipeline failed: {e}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "# ---------- Run ----------\n",
        "def main():\n",
        "    import multiprocessing as mp\n",
        "    mp.set_start_method('spawn', force=True)\n",
        "\n",
        "    config = {\n",
        "        'target_dir': \"/content/drive/MyDrive/PhDwork/Segmentation\",\n",
        "        'output_folder_name': \"Results_Nifti_MONAI6_Original\",\n",
        "        'transformation': \"OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\",\n",
        "        'batch_size': 1,   # Effective patches per step = num_samples * batch_size\n",
        "        'num_epochs': 200,\n",
        "        'learning_rate': 1e-4,\n",
        "    }\n",
        "\n",
        "    pipeline = UnetPipeline(config)\n",
        "    pipeline.run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#(8) Mask Generation"
      ],
      "metadata": {
        "id": "hSP35kUBOaDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from monai.networks.nets import UNet\n",
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    Resized,\n",
        "    CopyItemsd,\n",
        "    Invertd,\n",
        "    LoadImaged,\n",
        "    EnsureChannelFirstd,\n",
        "    Spacingd,\n",
        "    Orientationd,\n",
        "    ScaleIntensityRanged,\n",
        "    CropForegroundd,\n",
        "    EnsureTyped,\n",
        "    SaveImaged,\n",
        "    ToTensord,\n",
        ")\n",
        "from monai.data import Dataset, DataLoader, decollate_batch\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.utils import set_determinism\n",
        "from monai.networks.layers import Norm\n",
        "# from monai.transforms.utils import SaveTransform\n",
        "\n",
        "\n",
        "\n",
        "class UNetInferencePipeline:\n",
        "    def __init__(self, model_path, input_ct_dir, input_seg_dir, output_dir, device=\"cuda:0\"):\n",
        "        self.device = device if torch.cuda.is_available() else \"cpu\"\n",
        "        self.input_ct_dir = input_ct_dir\n",
        "        self.input_seg_dir = input_seg_dir\n",
        "        self.output_dir = output_dir\n",
        "        self.ct_out_dir = os.path.join(output_dir, \"ct\")\n",
        "        self.seg_out_dir = os.path.join(output_dir, \"segment\")\n",
        "        os.makedirs(self.ct_out_dir, exist_ok=True)\n",
        "        os.makedirs(self.seg_out_dir, exist_ok=True)\n",
        "        self.model_path = model_path\n",
        "        self.model = self._load_model()\n",
        "        set_determinism(seed=42)\n",
        "        self.forward_transforms = self._get_forward_transforms()\n",
        "        self.inverse_transforms = None\n",
        "        self.dataloader = self._prepare_dataloader()\n",
        "\n",
        "    def _load_model(self):\n",
        "        if not os.path.exists(self.model_path):\n",
        "            raise FileNotFoundError(f\"Model file not found at: {self.model_path}\")\n",
        "\n",
        "        model = UNet(\n",
        "            spatial_dims=3,\n",
        "            in_channels=1,\n",
        "            out_channels=1,\n",
        "            channels=(16, 32, 64, 128, 256),\n",
        "            strides=(2, 2, 2, 2),\n",
        "            num_res_units=2,\n",
        "            norm=Norm.BATCH\n",
        "        ).to(self.device)\n",
        "\n",
        "        state_dict = torch.load(self.model_path, map_location=self.device)\n",
        "        model.load_state_dict(state_dict.get('model_state_dict', state_dict))\n",
        "\n",
        "        print(f\"‚úÖ Model loaded successfully from {self.model_path}\")\n",
        "        return model\n",
        "\n",
        "\n",
        "\n",
        "    def _get_forward_transforms(self):\n",
        "        return Compose([\n",
        "            LoadImaged(keys=[\"vol\"]),\n",
        "            EnsureChannelFirstd(keys=[\"vol\"]),\n",
        "            CopyItemsd(keys=[\"vol\"], names=[\"vol_meta_dict\"]),\n",
        "            Spacingd(keys=[\"vol\"], pixdim=(1.0, 1.0, 1.0), mode=\"bilinear\"),\n",
        "            Orientationd(keys=[\"vol\"], axcodes=\"RAS\"),\n",
        "            ScaleIntensityRanged(keys=[\"vol\"], a_min=-1000, a_max=700, b_min=0.0, b_max=1.0, clip=True),\n",
        "            CropForegroundd(keys=[\"vol\"], source_key=\"vol\"),\n",
        "            Resized(keys=[\"vol\"], spatial_size=(96, 96, 96)),\n",
        "            EnsureTyped(keys=[\"vol\"]),\n",
        "        ])\n",
        "\n",
        "    def _get_inverse_transforms(self):\n",
        "        return Compose([\n",
        "            Invertd(\n",
        "                keys=[\"seg\"],\n",
        "                transform=self.forward_transforms,\n",
        "                orig_keys=[\"vol\"],\n",
        "                meta_keys=[\"vol_meta_dict\"],\n",
        "                nearest_interp=True,\n",
        "                to_tensor=False,\n",
        "            ),\n",
        "            EnsureTyped(keys=[\"seg\"])\n",
        "        ])\n",
        "\n",
        "    def _prepare_dataloader(self):\n",
        "        data = []\n",
        "        for f in os.listdir(self.input_ct_dir):\n",
        "            if f.endswith(('.nii', '.nii.gz')):\n",
        "                ct_path = os.path.join(self.input_ct_dir, f)\n",
        "                data.append({\"vol\": ct_path})\n",
        "        print(f\"üîç Found {len(data)} NIfTI files for inference.\")\n",
        "        return DataLoader(Dataset(data=data, transform=self.forward_transforms), batch_size=1, num_workers=0)\n",
        "\n",
        "    def infer(self):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(self.dataloader):\n",
        "                batch = decollate_batch(batch)[0]\n",
        "                vol_meta = batch[\"vol_meta_dict\"]\n",
        "                ct = batch[\"vol\"]\n",
        "\n",
        "                if ct.dim() == 4:\n",
        "                    ct = ct.unsqueeze(0)\n",
        "                ct = ct.to(self.device)\n",
        "\n",
        "                filename = os.path.basename(vol_meta.meta[\"filename_or_obj\"])\n",
        "                orig_vol = nib.load(vol_meta.meta[\"filename_or_obj\"]).get_fdata()\n",
        "                print(f\"üîç Inference on [{i+1}] {filename} | shape = {ct.shape}\")\n",
        "                print(f\"üîç Original volume shape = {orig_vol.shape}\")\n",
        "                pred = self.model(ct)\n",
        "                pred = (torch.sigmoid(pred) > 0.5).float()\n",
        "\n",
        "                print(f\"‚úÖ Predicted mask shape: {pred.shape}\")\n",
        "\n",
        "                batch[\"seg\"] = pred.cpu().squeeze(0)\n",
        "                print(f\"‚úÖ Batch shape: {batch['seg'].shape}\")\n",
        "\n",
        "                if self.inverse_transforms is None:\n",
        "                    self.inverse_transforms = self._get_inverse_transforms()\n",
        "\n",
        "                inverted = self.inverse_transforms(batch)\n",
        "                inv_seg = inverted[\"seg\"].squeeze(0).numpy()\n",
        "                inv_seg = (inv_seg > 0.5).astype(np.uint8)\n",
        "                print(f\"‚úÖ Inverted mask shape: {inv_seg.shape}\")\n",
        "\n",
        "                self._save_nifti(inv_seg, vol_meta, self.seg_out_dir, filename, is_segmentation=True)\n",
        "\n",
        "\n",
        "    def _save_nifti(self, array, meta_tensor, out_dir, filename, is_segmentation=False):\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "        affine = meta_tensor.meta.get(\"original_affine\", meta_tensor.meta.get(\"affine\", np.eye(4)))\n",
        "        dtype = np.uint8 if is_segmentation else np.float32\n",
        "        nib_img = nib.Nifti1Image(array.astype(dtype), affine)\n",
        "        nib.save(nib_img, os.path.join(out_dir, filename))\n",
        "        print(f\"‚úÖ Saved: {os.path.join(out_dir, filename)}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ROOT_DIR = \"/content/drive/MyDrive/PhDwork/Segmentation\"\n",
        "    MODEL_PATH = os.path.join(ROOT_DIR, \"results\", \"Results_MONAI_Augmented\", \"model.pth\")\n",
        "    INPUT_CT_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"Lung3\", \"ct\")\n",
        "    INPUT_SEG_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"Lung3\", \"segment\")\n",
        "    OUTPUT_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"Lung3_Predicted\")\n",
        "\n",
        "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "    os.chdir(ROOT_DIR)\n",
        "\n",
        "    try:\n",
        "        pipeline = UNetInferencePipeline(MODEL_PATH, INPUT_CT_FOLDER, INPUT_SEG_FOLDER, OUTPUT_FOLDER)\n",
        "        pipeline.infer()\n",
        "        print(\"üéâ Inference completed successfully for all patients!\")\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "z6a0G1DXTIV8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81a85b9f-c61a-4fd8-b379-3bd00e32c605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model loaded successfully from /content/drive/MyDrive/PhDwork/Segmentation/results/Results_MONAI_Augmented/model.pth\n",
            "üîç Found 89 NIfTI files for inference.\n",
            "üîç Inference on [1] LUNG3-01.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (59, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (59, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-01.nii.gz\n",
            "üîç Inference on [2] LUNG3-02.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (57, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (57, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-02.nii.gz\n",
            "üîç Inference on [3] LUNG3-03.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (61, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (61, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-03.nii.gz\n",
            "üîç Inference on [4] LUNG3-04.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (61, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (61, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-04.nii.gz\n",
            "üîç Inference on [5] LUNG3-05.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (229, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (229, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-05.nii.gz\n",
            "üîç Inference on [6] LUNG3-06.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (61, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (61, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-06.nii.gz\n",
            "üîç Inference on [7] LUNG3-07.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (86, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (86, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-07.nii.gz\n",
            "üîç Inference on [8] LUNG3-08.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (158, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (158, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-08.nii.gz\n",
            "üîç Inference on [9] LUNG3-09.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (140, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (140, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-09.nii.gz\n",
            "üîç Inference on [10] LUNG3-10.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (95, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (95, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-10.nii.gz\n",
            "üîç Inference on [11] LUNG3-11.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (252, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (252, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-11.nii.gz\n",
            "üîç Inference on [12] LUNG3-12.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (206, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (206, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-12.nii.gz\n",
            "üîç Inference on [13] LUNG3-13.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (71, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (71, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-13.nii.gz\n",
            "üîç Inference on [14] LUNG3-14.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (325, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (325, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-14.nii.gz\n",
            "üîç Inference on [15] LUNG3-15.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (234, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (234, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-15.nii.gz\n",
            "üîç Inference on [16] LUNG3-16.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (192, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (192, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-16.nii.gz\n",
            "üîç Inference on [17] LUNG3-17.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (226, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (226, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-17.nii.gz\n",
            "üîç Inference on [18] LUNG3-18.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (178, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (178, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-18.nii.gz\n",
            "üîç Inference on [19] LUNG3-19.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (178, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (178, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-19.nii.gz\n",
            "üîç Inference on [20] LUNG3-20.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (176, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (176, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-20.nii.gz\n",
            "üîç Inference on [21] LUNG3-21.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (74, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (74, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-21.nii.gz\n",
            "üîç Inference on [22] LUNG3-22.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (236, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (236, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-22.nii.gz\n",
            "üîç Inference on [23] LUNG3-23.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (52, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (52, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-23.nii.gz\n",
            "üîç Inference on [24] LUNG3-24.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-24.nii.gz\n",
            "üîç Inference on [25] LUNG3-25.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (202, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (202, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-25.nii.gz\n",
            "üîç Inference on [26] LUNG3-26.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (83, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (83, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-26.nii.gz\n",
            "üîç Inference on [27] LUNG3-27.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (149, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (149, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-27.nii.gz\n",
            "üîç Inference on [28] LUNG3-28.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (86, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (86, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-28.nii.gz\n",
            "üîç Inference on [29] LUNG3-29.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (173, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (173, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-29.nii.gz\n",
            "üîç Inference on [30] LUNG3-30.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (72, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (72, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-30.nii.gz\n",
            "üîç Inference on [31] LUNG3-31.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (242, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (242, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-31.nii.gz\n",
            "üîç Inference on [32] LUNG3-32.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (58, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (58, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-32.nii.gz\n",
            "üîç Inference on [33] LUNG3-33.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (276, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (276, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-33.nii.gz\n",
            "üîç Inference on [34] LUNG3-34.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (255, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (255, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-34.nii.gz\n",
            "üîç Inference on [35] LUNG3-35.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (253, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (253, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-35.nii.gz\n",
            "üîç Inference on [36] LUNG3-36.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (356, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (356, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-36.nii.gz\n",
            "üîç Inference on [37] LUNG3-37.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (97, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (97, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-37.nii.gz\n",
            "üîç Inference on [38] LUNG3-38.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (223, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (223, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-38.nii.gz\n",
            "üîç Inference on [39] LUNG3-39.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (82, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (82, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-39.nii.gz\n",
            "üîç Inference on [40] LUNG3-40.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (239, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (239, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-40.nii.gz\n",
            "üîç Inference on [41] LUNG3-41.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (110, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (110, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-41.nii.gz\n",
            "üîç Inference on [42] LUNG3-42.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (255, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (255, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-42.nii.gz\n",
            "üîç Inference on [43] LUNG3-43.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (68, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (68, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-43.nii.gz\n",
            "üîç Inference on [44] LUNG3-44.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (227, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (227, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-44.nii.gz\n",
            "üîç Inference on [45] LUNG3-45.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (178, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (178, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-45.nii.gz\n",
            "üîç Inference on [46] LUNG3-46.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (184, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (184, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-46.nii.gz\n",
            "üîç Inference on [47] LUNG3-47.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (275, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (275, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-47.nii.gz\n",
            "üîç Inference on [48] LUNG3-48.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (157, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (157, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-48.nii.gz\n",
            "üîç Inference on [49] LUNG3-49.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (89, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (89, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-49.nii.gz\n",
            "üîç Inference on [50] LUNG3-50.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (255, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (255, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-50.nii.gz\n",
            "üîç Inference on [51] LUNG3-51.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (76, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (76, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-51.nii.gz\n",
            "üîç Inference on [52] LUNG3-52.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (50, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (50, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-52.nii.gz\n",
            "üîç Inference on [53] LUNG3-53.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (255, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (255, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-53.nii.gz\n",
            "üîç Inference on [54] LUNG3-54.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (175, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (175, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-54.nii.gz\n",
            "üîç Inference on [55] LUNG3-55.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (72, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (72, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-55.nii.gz\n",
            "üîç Inference on [56] LUNG3-56.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (178, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (178, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-56.nii.gz\n",
            "üîç Inference on [57] LUNG3-57.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (178, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (178, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-57.nii.gz\n",
            "üîç Inference on [58] LUNG3-58.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (64, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (64, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-58.nii.gz\n",
            "üîç Inference on [59] LUNG3-59.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (62, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (62, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-59.nii.gz\n",
            "üîç Inference on [60] LUNG3-60.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (92, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (92, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-60.nii.gz\n",
            "üîç Inference on [61] LUNG3-61.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (203, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (203, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-61.nii.gz\n",
            "üîç Inference on [62] LUNG3-62.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (66, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (66, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-62.nii.gz\n",
            "üîç Inference on [63] LUNG3-63.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (57, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (57, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-63.nii.gz\n",
            "üîç Inference on [64] LUNG3-64.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (172, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (172, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-64.nii.gz\n",
            "üîç Inference on [65] LUNG3-65.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (175, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (175, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-65.nii.gz\n",
            "üîç Inference on [66] LUNG3-66.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (69, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (69, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-66.nii.gz\n",
            "üîç Inference on [67] LUNG3-67.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (74, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (74, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-67.nii.gz\n",
            "üîç Inference on [68] LUNG3-68.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (60, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (60, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-68.nii.gz\n",
            "üîç Inference on [69] LUNG3-69.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (158, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (158, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-69.nii.gz\n",
            "üîç Inference on [70] LUNG3-70.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (258, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (258, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-70.nii.gz\n",
            "üîç Inference on [71] LUNG3-71.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (287, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (287, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-71.nii.gz\n",
            "üîç Inference on [72] LUNG3-72.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (84, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (84, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-72.nii.gz\n",
            "üîç Inference on [73] LUNG3-73.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (218, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (218, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-73.nii.gz\n",
            "üîç Inference on [74] LUNG3-74.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (67, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (67, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-74.nii.gz\n",
            "üîç Inference on [75] LUNG3-75.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (88, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (88, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-75.nii.gz\n",
            "üîç Inference on [76] LUNG3-76.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (74, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (74, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-76.nii.gz\n",
            "üîç Inference on [77] LUNG3-77.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (99, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (99, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-77.nii.gz\n",
            "üîç Inference on [78] LUNG3-78.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (255, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (255, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-78.nii.gz\n",
            "üîç Inference on [79] LUNG3-79.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (240, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (240, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-79.nii.gz\n",
            "üîç Inference on [80] LUNG3-80.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (59, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (59, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-80.nii.gz\n",
            "üîç Inference on [81] LUNG3-81.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (307, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (307, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-81.nii.gz\n",
            "üîç Inference on [82] LUNG3-82.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (78, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (78, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-82.nii.gz\n",
            "üîç Inference on [83] LUNG3-83.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (178, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (178, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-83.nii.gz\n",
            "üîç Inference on [84] LUNG3-84.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (66, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (66, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-84.nii.gz\n",
            "üîç Inference on [85] LUNG3-85.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (234, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (234, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-85.nii.gz\n",
            "üîç Inference on [86] LUNG3-86.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (78, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (78, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-86.nii.gz\n",
            "üîç Inference on [87] LUNG3-87.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (79, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (79, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-87.nii.gz\n",
            "üîç Inference on [88] LUNG3-88.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (154, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (154, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-88.nii.gz\n",
            "üîç Inference on [89] LUNG3-89.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (158, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (158, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-89.nii.gz\n",
            "üéâ Inference completed successfully for all patients!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    ROOT_DIR = \"/content/drive/MyDrive/PhDwork/Segmentation\"\n",
        "    MODEL_PATH = os.path.join(ROOT_DIR, \"results\", \"Results_MONAI_Augmented\", \"model.pth\")\n",
        "    INPUT_CT_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"test\", \"ct\")\n",
        "    INPUT_SEG_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"test\", \"segment\")\n",
        "    OUTPUT_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"test_Predicted\")\n",
        "\n",
        "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "    os.chdir(ROOT_DIR)\n",
        "\n",
        "    try:\n",
        "        pipeline = UNetInferencePipeline(MODEL_PATH, INPUT_CT_FOLDER, INPUT_SEG_FOLDER, OUTPUT_FOLDER)\n",
        "        pipeline.infer()\n",
        "        print(\"üéâ Inference completed successfully for all patients!\")\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "id": "Ckpn4rjZJ9fn",
        "outputId": "b54762e0-6093-45f5-86b6-48c278f39d56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model loaded successfully from /content/drive/MyDrive/PhDwork/Segmentation/results/Results_MONAI_Augmented/model.pth\n",
            "üîç Found 38 NIfTI files for inference.\n",
            "üîç Inference on [1] LUNG1-001.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-001.nii.gz\n",
            "üîç Inference on [2] LUNG1-025.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (106, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (106, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-025.nii.gz\n",
            "üîç Inference on [3] LUNG1-027.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (108, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (108, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-027.nii.gz\n",
            "üîç Inference on [4] LUNG1-034.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (95, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (95, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-034.nii.gz\n",
            "üîç Inference on [5] LUNG1-039.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (95, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (95, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-039.nii.gz\n",
            "üîç Inference on [6] LUNG1-066.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (92, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (92, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-066.nii.gz\n",
            "üîç Inference on [7] LUNG1-078.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (136, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (136, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-078.nii.gz\n",
            "üîç Inference on [8] LUNG1-088.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (123, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (123, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-088.nii.gz\n",
            "üîç Inference on [9] LUNG1-107.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (116, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (116, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-107.nii.gz\n",
            "üîç Inference on [10] LUNG1-132.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (114, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (114, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-132.nii.gz\n",
            "üîç Inference on [11] LUNG1-133.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (184, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (184, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-133.nii.gz\n",
            "üîç Inference on [12] LUNG1-143.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-143.nii.gz\n",
            "üîç Inference on [13] LUNG1-149.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (118, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (118, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-149.nii.gz\n",
            "üîç Inference on [14] LUNG1-151.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (118, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (118, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-151.nii.gz\n",
            "üîç Inference on [15] LUNG1-158.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (115, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (115, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-158.nii.gz\n",
            "üîç Inference on [16] LUNG1-168.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-168.nii.gz\n",
            "üîç Inference on [17] LUNG1-175.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (112, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (112, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-175.nii.gz\n",
            "üîç Inference on [18] LUNG1-176.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (106, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (106, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-176.nii.gz\n",
            "üîç Inference on [19] LUNG1-201.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-201.nii.gz\n",
            "üîç Inference on [20] LUNG1-224.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (93, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (93, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-224.nii.gz\n",
            "üîç Inference on [21] LUNG1-225.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-225.nii.gz\n",
            "üîç Inference on [22] LUNG1-235.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (129, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (129, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-235.nii.gz\n",
            "üîç Inference on [23] LUNG1-239.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-239.nii.gz\n",
            "üîç Inference on [24] LUNG1-246.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (115, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (115, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-246.nii.gz\n",
            "üîç Inference on [25] LUNG1-263.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-263.nii.gz\n",
            "üîç Inference on [26] LUNG1-266.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (94, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (94, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-266.nii.gz\n",
            "üîç Inference on [27] LUNG1-281.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (101, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (101, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-281.nii.gz\n",
            "üîç Inference on [28] LUNG1-286.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (136, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (136, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-286.nii.gz\n",
            "üîç Inference on [29] LUNG1-312.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-312.nii.gz\n",
            "üîç Inference on [30] LUNG1-338.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-338.nii.gz\n",
            "üîç Inference on [31] LUNG1-352.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-352.nii.gz\n",
            "üîç Inference on [32] LUNG1-353.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-353.nii.gz\n",
            "üîç Inference on [33] LUNG1-365.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-365.nii.gz\n",
            "üîç Inference on [34] LUNG1-374.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (130, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (130, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-374.nii.gz\n",
            "üîç Inference on [35] LUNG1-383.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-383.nii.gz\n",
            "üîç Inference on [36] LUNG1-405.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-405.nii.gz\n",
            "üîç Inference on [37] LUNG1-408.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (107, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (107, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-408.nii.gz\n",
            "üîç Inference on [38] LUNG1-410.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-410.nii.gz\n",
            "üéâ Inference completed successfully for all patients!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    ROOT_DIR = \"/content/drive/MyDrive/PhDwork/Segmentation\"\n",
        "    MODEL_PATH = os.path.join(ROOT_DIR, \"results\", \"Results_MONAI_Augmented\", \"model.pth\")\n",
        "    INPUT_CT_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"valid\", \"ct\")\n",
        "    INPUT_SEG_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"valid\", \"segment\")\n",
        "    OUTPUT_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"valid_Predicted\")\n",
        "\n",
        "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "    os.chdir(ROOT_DIR)\n",
        "\n",
        "    try:\n",
        "        pipeline = UNetInferencePipeline(MODEL_PATH, INPUT_CT_FOLDER, INPUT_SEG_FOLDER, OUTPUT_FOLDER)\n",
        "        pipeline.infer()\n",
        "        print(\"üéâ Inference completed successfully for all patients!\")\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcIi6GZ347x8",
        "outputId": "5b5fc90e-eeaf-4b07-d6a5-bf3197b8e4a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model loaded successfully from /content/drive/MyDrive/PhDwork/Segmentation/results/Results_MONAI_Augmented/model.pth\n",
            "üîç Found 43 NIfTI files for inference.\n",
            "üîç Inference on [1] LUNG1-010.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (91, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (91, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-010.nii.gz\n",
            "üîç Inference on [2] LUNG1-031.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (153, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (153, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-031.nii.gz\n",
            "üîç Inference on [3] LUNG1-040.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (95, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (95, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-040.nii.gz\n",
            "üîç Inference on [4] LUNG1-056.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-056.nii.gz\n",
            "üîç Inference on [5] LUNG1-057.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (101, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (101, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-057.nii.gz\n",
            "üîç Inference on [6] LUNG1-071.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (135, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (135, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-071.nii.gz\n",
            "üîç Inference on [7] LUNG1-073.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (176, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (176, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-073.nii.gz\n",
            "üîç Inference on [8] LUNG1-074.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (115, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (115, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-074.nii.gz\n",
            "üîç Inference on [9] LUNG1-076.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (92, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (92, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-076.nii.gz\n",
            "üîç Inference on [10] LUNG1-077.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (117, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (117, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-077.nii.gz\n",
            "üîç Inference on [11] LUNG1-080.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (99, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (99, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-080.nii.gz\n",
            "üîç Inference on [12] LUNG1-091.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (135, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (135, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-091.nii.gz\n",
            "üîç Inference on [13] LUNG1-095.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (106, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (106, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-095.nii.gz\n",
            "üîç Inference on [14] LUNG1-117.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (90, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (90, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-117.nii.gz\n",
            "üîç Inference on [15] LUNG1-134.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (108, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (108, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-134.nii.gz\n",
            "üîç Inference on [16] LUNG1-139.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (107, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (107, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-139.nii.gz\n",
            "üîç Inference on [17] LUNG1-147.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (99, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (99, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-147.nii.gz\n",
            "üîç Inference on [18] LUNG1-170.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (110, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (110, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-170.nii.gz\n",
            "üîç Inference on [19] LUNG1-177.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (94, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (94, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-177.nii.gz\n",
            "üîç Inference on [20] LUNG1-186.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-186.nii.gz\n",
            "üîç Inference on [21] LUNG1-194.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (127, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (127, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-194.nii.gz\n",
            "üîç Inference on [22] LUNG1-196.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (94, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (94, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-196.nii.gz\n",
            "üîç Inference on [23] LUNG1-198.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (131, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (131, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-198.nii.gz\n",
            "üîç Inference on [24] LUNG1-210.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (131, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (131, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-210.nii.gz\n",
            "üîç Inference on [25] LUNG1-220.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (94, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (94, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-220.nii.gz\n",
            "üîç Inference on [26] LUNG1-230.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (93, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (93, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-230.nii.gz\n",
            "üîç Inference on [27] LUNG1-233.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-233.nii.gz\n",
            "üîç Inference on [28] LUNG1-241.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (136, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (136, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-241.nii.gz\n",
            "üîç Inference on [29] LUNG1-249.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (93, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (93, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-249.nii.gz\n",
            "üîç Inference on [30] LUNG1-264.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (130, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (130, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-264.nii.gz\n",
            "üîç Inference on [31] LUNG1-273.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (136, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (136, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-273.nii.gz\n",
            "üîç Inference on [32] LUNG1-299.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (93, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (93, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-299.nii.gz\n",
            "üîç Inference on [33] LUNG1-329.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-329.nii.gz\n",
            "üîç Inference on [34] LUNG1-337.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-337.nii.gz\n",
            "üîç Inference on [35] LUNG1-340.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (92, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (92, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-340.nii.gz\n",
            "üîç Inference on [36] LUNG1-356.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-356.nii.gz\n",
            "üîç Inference on [37] LUNG1-371.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (173, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (173, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-371.nii.gz\n",
            "üîç Inference on [38] LUNG1-372.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-372.nii.gz\n",
            "üîç Inference on [39] LUNG1-412.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-412.nii.gz\n",
            "üîç Inference on [40] LUNG1-415.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (122, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (122, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-415.nii.gz\n",
            "üîç Inference on [41] LUNG1-418.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (133, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (133, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-418.nii.gz\n",
            "üîç Inference on [42] LUNG1-419.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-419.nii.gz\n",
            "üîç Inference on [43] LUNG1-421.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-421.nii.gz\n",
            "üéâ Inference completed successfully for all patients!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKImglO4-twL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "import os\n",
        "import csv\n",
        "\n",
        "\n",
        "class LossPlotter:\n",
        "    def __init__(self, csv_path: str):\n",
        "        self.csv_path = Path(csv_path)\n",
        "        self.data = self._load_data()\n",
        "\n",
        "    def _load_data(self):\n",
        "        if not self.csv_path.exists():\n",
        "            raise FileNotFoundError(f\"CSV file not found: {self.csv_path}\")\n",
        "        df = pd.read_csv(self.csv_path, index_col=0)  # Read row labels as index\n",
        "        return df  # Make rows into columns\n",
        "\n",
        "    def plot(self, title: str = \"Training and Validation Loss\", save_path= None):\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.plot(self.data.index, self.data['Train Loss'], label='Train Loss', color='blue')\n",
        "        plt.plot(self.data.index, self.data['Valid Loss'], label='Valid Loss', color='orange')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title(title)\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            save_path = Path(save_path)\n",
        "            save_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            plt.savefig(save_path, format='pdf')\n",
        "            print(f\"[INFO] Loss plot saved to {save_path}\")\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "        plt.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    target_dir = \"/content/drive/MyDrive/PhDwork/Segmentation\"\n",
        "    os.chdir(target_dir)\n",
        "    loss_result_file = os.path.join(\".\",\"results\",f\"Results_PreProcessedCT_Fifty_Fifty_DiceLoss_And_Strong_Augmentation\",\"train_and_valid_loss_results.csv\")\n",
        "    plotter = LossPlotter(loss_result_file)\n",
        "    plotter.plot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyeB21BYGQPu"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "os.chdir(\"/content/drive/MyDrive/PhDwork/Segmentation\")\n",
        "print(f\"üìÅ Current Directory: {os.getcwd()}\")\n",
        "with h5py.File('./datasets/Datasets_PreprocessedCT_clipping_uniformSpacing_With_Empty_NonEmpty_slices_In_Train/train_dataset.hdf5', 'r') as f:\n",
        "    print(list(f.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ud1cFDGmKQBK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNMiMvtcfcYaK9u1C74X8uA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}