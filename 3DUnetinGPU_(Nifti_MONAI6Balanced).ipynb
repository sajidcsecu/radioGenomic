{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sajidcsecu/radioGenomic/blob/main/3DUnetinGPU_(Nifti_MONAI6Balanced).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr2QqHlZ8GZB"
      },
      "source": [
        "# This is the Code for the Segmentation on Rider Dataset (LUNG1). The Code is worked on the 3D volume over GPU. This is the improved of Model 5\n",
        "1. the architecture is 3DUNet\n",
        "2. The balanced sampler, preprocessed data (uniform volume spacing and clipping [-1000, 700]) and the\n",
        "3. strong augmentation is used in the code...\n",
        "4. Dice Loss is 1 and Binary classification Entropy is 1\n",
        "5. Number of positive pixels is for all patients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9Zo7tkcI1CX"
      },
      "source": [
        "# (1) Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d9OVdEeKXpMe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee437858-f98f-4544-f7e6-f9f26b19b1de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.12/dist-packages (2.5.2)\n",
            "Requirement already satisfied: pydicom===2.4.3 in /usr/local/lib/python3.12/dist-packages (2.4.3)\n",
            "Requirement already satisfied: pydicom-seg in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: SimpleITK>1.2.4 in /usr/local/lib/python3.12/dist-packages (from pydicom-seg) (2.5.2)\n",
            "Requirement already satisfied: jsonschema<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from pydicom-seg) (3.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from pydicom-seg) (1.26.4)\n",
            "Requirement already satisfied: pydicom>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from pydicom-seg) (2.4.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (25.3.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (0.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (75.2.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (1.17.0)\n",
            "Collecting numpy==1.23.5\n",
            "  Using cached numpy-1.23.5.tar.gz (10.7 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m√ó\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m√ó\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "Collecting monai\n",
            "  Downloading monai-1.5.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.24 in /usr/local/lib/python3.12/dist-packages (from monai) (1.26.4)\n",
            "Requirement already satisfied: torch>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from monai) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.1->monai) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.1->monai) (3.0.3)\n",
            "Downloading monai-1.5.1-py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: monai\n",
            "Successfully installed monai-1.5.1\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.13.1 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0, 2.7.1, 2.8.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.13.1\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install SimpleITK\n",
        "!pip install pydicom===2.4.3\n",
        "!pip install pydicom-seg\n",
        "!pip install numpy==1.23.5\n",
        "!pip install monai\n",
        "!pip install torch==1.13.1\n",
        "!pip install nibabel>=5.0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JadHvjQcJ-qU"
      },
      "source": [
        "\n",
        "# (2) Import required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pmtDNjxMbfB4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import csv\n",
        "import time\n",
        "import random\n",
        "import shutil\n",
        "from glob import glob\n",
        "from typing import List\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.cuda.amp as amp\n",
        "from torch.optim import lr_scheduler\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.transforms import AsDiscrete\n",
        "from monai.networks.nets import UNet\n",
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    LoadImaged,\n",
        "    EnsureChannelFirstd,\n",
        "    Spacingd,\n",
        "    Orientationd,\n",
        "    ScaleIntensityRanged,\n",
        "    CropForegroundd,\n",
        "    ResizeWithPadOrCropd,\n",
        "    RandCropByPosNegLabeld,\n",
        "    RandFlipd,\n",
        "    RandAffined,\n",
        "    RandGaussianNoised,\n",
        "    RandScaleIntensityd,\n",
        "    ToTensord,\n",
        "    EnsureTyped,\n",
        "    EnsureChannelFirstD,\n",
        "    SpatialPadd,\n",
        "    Rand3DElasticd,\n",
        "    NormalizeIntensityd,\n",
        "    RandGaussianSmoothd,\n",
        "    RandAdjustContrastd\n",
        "\n",
        ")\n",
        "import json\n",
        "from monai.transforms import Transform\n",
        "import torch.nn.functional as F\n",
        "from monai.data import Dataset, DataLoader, CacheDataset, pad_list_data_collate\n",
        "from monai.networks.layers import Norm\n",
        "import nibabel as nib\n",
        "from sklearn.metrics import jaccard_score, f1_score, recall_score, precision_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import multiprocessing as mp\n",
        "from monai.transforms import EnsureTyped\n",
        "from monai.transforms import SaveImaged\n",
        "from monai.utils import set_determinism\n",
        "from monai.metrics import DiceMetric, HausdorffDistanceMetric, SurfaceDistanceMetric\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"Num foregrounds 0, Num backgrounds.*unable to generate class balanced samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyzguRDWI9bM"
      },
      "source": [
        "# (3) Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq6jVaaMXZz5",
        "outputId": "a3593375-3e09-4065-c09b-97ef9b01809b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cGu3zWdW3jje"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFrRJqgG7wxo"
      },
      "source": [
        "## (4). Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "944_4uJbmPPx"
      },
      "outputs": [],
      "source": [
        "# ---------- Combined Dice + BCE loss for 3D segmentation ----------\n",
        "class DiceBCELoss3D(nn.Module):\n",
        "    def __init__(self, smooth=1e-6, dice_w=1.0, bce_w=1.0, focal_alpha=0.8, focal_gamma=2.0):\n",
        "        super().__init__()\n",
        "        self.smooth = smooth\n",
        "        self.dice_w = dice_w\n",
        "        self.bce_w = bce_w\n",
        "        self.focal_alpha = focal_alpha\n",
        "        self.focal_gamma = focal_gamma\n",
        "\n",
        "    def set_pos_weight(self, pos_weight=None, device=\"cpu\"):\n",
        "        self.pos_weight = torch.tensor([pos_weight], dtype=torch.float, device=device) if pos_weight else None\n",
        "        return self\n",
        "\n",
        "    def forward(self, preds, targets):\n",
        "        preds_sigmoid = torch.sigmoid(preds)\n",
        "\n",
        "        # Improved Dice with squared terms for better gradient flow\n",
        "        inter = (preds_sigmoid * targets).sum()\n",
        "        dice_loss = 1 - (2. * inter + self.smooth) / (\n",
        "            preds_sigmoid.pow(2).sum() + targets.pow(2).sum() + self.smooth\n",
        "        )\n",
        "\n",
        "        # Focal BCE instead of regular BCE\n",
        "        bce_loss = F.binary_cross_entropy_with_logits(preds, targets, reduction='none')\n",
        "\n",
        "        # Focal weighting\n",
        "        pt = torch.exp(-bce_loss)\n",
        "        focal_weight = (self.focal_alpha * targets + (1 - self.focal_alpha) * (1 - targets)) * (1 - pt) ** self.focal_gamma\n",
        "\n",
        "        if self.pos_weight is not None:\n",
        "            weight_mask = torch.where(targets > 0.5, self.pos_weight, torch.tensor(1.0).to(targets.device))\n",
        "            focal_weight = focal_weight * weight_mask\n",
        "\n",
        "        bce_loss = (focal_weight * bce_loss).mean()\n",
        "\n",
        "        return self.dice_w * dice_loss + self.bce_w * bce_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZZ3Gu-DD88X"
      },
      "source": [
        "# (5). Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "T4VsKIzmFGLP"
      },
      "outputs": [],
      "source": [
        "class UnetTest:\n",
        "    def __init__(self, test_result_path: str, metrics_csv: str, device: torch.device):\n",
        "        super().__init__(test_result_path, metrics_csv, device)\n",
        "        self.dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
        "        self.hd95_metric = HausdorffDistanceMetric(include_background=False, percentile=95, reduction=\"mean\")\n",
        "        self.asd_metric = SurfaceDistanceMetric(include_background=False, reduction=\"mean\")\n",
        "\n",
        "        # Update CSV headers to include new metrics\n",
        "        self._init_enhanced_metrics_csv()\n",
        "\n",
        "    def _init_enhanced_metrics_csv(self):\n",
        "        \"\"\"Initialize CSV with additional metric columns\"\"\"\n",
        "        if not os.path.exists(self.metrics_csv):\n",
        "            with open(self.metrics_csv, 'w', newline='') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([\n",
        "                    \"SampleID\", \"Jaccard\", \"F1\", \"Recall\", \"Precision\", \"Accuracy\",\n",
        "                    \"Dice\", \"HD95\", \"ASD\", \"Time\"\n",
        "                ])\n",
        "\n",
        "    def calculate_comprehensive_metrics(self, y_true: np.ndarray, y_pred: np.ndarray):\n",
        "        \"\"\"Calculate both basic and medical image metrics\"\"\"\n",
        "        # Basic metrics from parent class\n",
        "        basic_metrics = super().calculate_metrics(y_true, y_pred)\n",
        "\n",
        "        # Convert to torch tensors for MONAI metrics\n",
        "        y_true_t = torch.from_numpy(y_true.astype(np.float32)).unsqueeze(0).unsqueeze(0)\n",
        "        y_pred_t = torch.from_numpy(y_pred.astype(np.float32)).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        # Calculate MONAI metrics\n",
        "        dice_value = self.dice_metric(y_pred_t, y_true_t)\n",
        "        hd95_value = self.hd95_metric(y_pred_t, y_true_t)\n",
        "        asd_value = self.asd_metric(y_pred_t, y_true_t)\n",
        "\n",
        "        # Reset metrics for next calculation\n",
        "        self.dice_metric.reset()\n",
        "        self.hd95_metric.reset()\n",
        "        self.asd_metric.reset()\n",
        "\n",
        "        return basic_metrics + [\n",
        "            dice_value.item() if not dice_value.isnan() else 0.0,\n",
        "            hd95_value.item() if not hd95_value.isnan() else 0.0,\n",
        "            asd_value.item() if not asd_value.isnan() else 0.0\n",
        "        ]\n",
        "\n",
        "    def append_metrics_to_csv(self, sample_id: str, metrics: list, elapsed_time: float):\n",
        "        \"\"\"Override to handle extended metrics\"\"\"\n",
        "        with open(self.metrics_csv, 'a', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            # Now metrics should have: [jaccard, f1, recall, precision, accuracy, dice, hd95, asd]\n",
        "            writer.writerow([sample_id] + [f\"{m:.4f}\" for m in metrics] + [f\"{elapsed_time:.4f}\"])\n",
        "\n",
        "    def test(self, model: nn.Module, test_loader: DataLoader):\n",
        "        model.eval()\n",
        "        total_metrics = np.zeros(8)  # Now 8 metrics total\n",
        "        total_times = []\n",
        "\n",
        "        roi_size = (96, 96, 96)\n",
        "        sw_batch_size = 1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(test_loader):\n",
        "                image, label = batch[\"vol\"].to(self.device), batch[\"seg\"].to(self.device)\n",
        "                start_time = time.time()\n",
        "\n",
        "                pred = sliding_window_inference(\n",
        "                    inputs=image,\n",
        "                    roi_size=roi_size,\n",
        "                    sw_batch_size=sw_batch_size,\n",
        "                    predictor=model\n",
        "                )\n",
        "                pred = torch.sigmoid(pred) > 0.5\n",
        "\n",
        "                elapsed = time.time() - start_time\n",
        "                total_times.append(elapsed)\n",
        "\n",
        "                # Convert to NumPy\n",
        "                image_np = image[0, 0].cpu().numpy()\n",
        "                label_np = label[0, 0].cpu().numpy()\n",
        "                pred_np = pred[0, 0].cpu().numpy().astype(np.uint8)\n",
        "\n",
        "                # Enhanced metrics calculation\n",
        "                metrics = self.calculate_comprehensive_metrics(label_np, pred_np)\n",
        "                total_metrics += np.array(metrics)\n",
        "\n",
        "                sample_id = f\"sample_{batch_idx:03d}\"\n",
        "                self.save_result_slices(image_np, pred_np, label_np, sample_id)\n",
        "                self.append_metrics_to_csv(sample_id, metrics, elapsed)\n",
        "\n",
        "        # Print enhanced summary\n",
        "        num_samples = len(test_loader)\n",
        "        metric_names = [\"Jaccard\", \"F1\", \"Recall\", \"Precision\", \"Accuracy\", \"Dice\", \"HD95\", \"ASD\"]\n",
        "        print(\"\\nüìä Enhanced Test Metrics:\")\n",
        "        for i, name in enumerate(metric_names):\n",
        "            print(f\"{name:<10}: {total_metrics[i]/num_samples:.4f}\")\n",
        "        print(f\"‚ö° FPS:    {1 / np.mean(total_times):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (6) Early Stopping"
      ],
      "metadata": {
        "id": "3JLRqnw5L7xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, verbose=True, min_delta=0, path='checkpoint.pt',\n",
        "                 start_val_loss_min=None, start_patience_counter=0, pos_weight=None):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.min_delta = min_delta\n",
        "        self.path = path\n",
        "        self.val_loss_min = start_val_loss_min if start_val_loss_min is not None else np.inf\n",
        "        self.counter = start_patience_counter\n",
        "        self.early_stop = False\n",
        "        self.pos_weight = pos_weight  # Store pos_weight\n",
        "\n",
        "    def __call__(self, val_loss, model, epoch=None, optimizer=None, pos_weight=None):\n",
        "        # Update pos_weight if provided\n",
        "        if pos_weight is not None:\n",
        "            self.pos_weight = pos_weight\n",
        "\n",
        "        improved = False\n",
        "        if val_loss < self.val_loss_min - self.min_delta:\n",
        "            self.val_loss_min = val_loss\n",
        "            self.counter = 0\n",
        "            improved = True\n",
        "            if self.verbose:\n",
        "                print(f\"‚úÖ Validation loss improved ({self.val_loss_min:.6f}). Saving model...\")\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f\"‚è≥ EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "\n",
        "        # Always save a full checkpoint with pos_weight\n",
        "        self.save_checkpoint(model, epoch, optimizer)\n",
        "\n",
        "        if self.counter >= self.patience:\n",
        "            self.early_stop = True\n",
        "\n",
        "        return self.early_stop\n",
        "\n",
        "    def save_checkpoint(self, model, epoch=None, optimizer=None):\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict() if optimizer else None,\n",
        "            'val_loss': self.val_loss_min,\n",
        "            'patience_counter': self.counter,\n",
        "            'pos_weight': self.pos_weight  # Save pos_weight here\n",
        "        }\n",
        "        torch.save(checkpoint, self.path)\n",
        "        if self.verbose and self.pos_weight is not None:\n",
        "            print(f\"üíæ Checkpoint saved with pos_weight: {self.pos_weight:.4f}\")"
      ],
      "metadata": {
        "id": "LA4lGL-xL6st"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa8L5nD2EVP_"
      },
      "source": [
        "# (7). Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "281KQS_iEIDX"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ---------- Trainer ----------\n",
        "\n",
        "class UnetTrain:\n",
        "    def __init__(self, model_file, loss_result_path, lr, num_epochs, device):\n",
        "        self.model_file = model_file\n",
        "        self.loss_result_path = loss_result_path\n",
        "        self.lr = lr\n",
        "        self.num_epochs = num_epochs\n",
        "        self.device = device\n",
        "        # Initialize pos_weight_file\n",
        "        self.pos_weight_file = model_file.replace('.pth', '_pos_weight.json')\n",
        "        self.seeding(42)\n",
        "\n",
        "    def seeding(self, seed):\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    def epoch_time(self, start_time, end_time):\n",
        "        elapsed = end_time - start_time\n",
        "        return int(elapsed / 60), int(elapsed % 60)\n",
        "\n",
        "    def train_one_epoch(self, model, loader, optimizer, scheduler, accumulation_steps, loss_fn):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        scaler = torch.amp.GradScaler()\n",
        "        device_type = 'cuda' if self.device.type == 'cuda' else 'cpu'\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for i, x in enumerate(loader):\n",
        "            inputs, labels = x[\"vol\"].to(self.device), x[\"seg\"].to(self.device)\n",
        "\n",
        "            with torch.amp.autocast(device_type=device_type):\n",
        "                outputs = model(inputs)\n",
        "                loss = loss_fn(outputs, labels) / accumulation_steps\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(loader):\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "                scheduler.step()  # Step per accumulation\n",
        "\n",
        "            epoch_loss += loss.item() * accumulation_steps\n",
        "\n",
        "        return epoch_loss / len(loader)\n",
        "\n",
        "    def evaluate(self, model, loader, loss_fn):\n",
        "        model.eval()\n",
        "        epoch_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in loader:\n",
        "                inputs, labels = batch[\"vol\"].to(self.device), batch[\"seg\"].to(self.device)\n",
        "                outputs = model(inputs)\n",
        "                loss = loss_fn(outputs, labels)\n",
        "                epoch_loss += loss.item()\n",
        "        return epoch_loss / len(loader)\n",
        "\n",
        "    def save_pos_weight(self, pos_weight):\n",
        "        \"\"\"Save pos_weight to JSON file\"\"\"\n",
        "        with open(self.pos_weight_file, 'w') as f:\n",
        "            json.dump({'pos_weight': pos_weight, 'timestamp': time.time()}, f)\n",
        "        print(f\"üíæ Saved pos_weight: {pos_weight:.4f}\")\n",
        "\n",
        "    def load_pos_weight(self):\n",
        "        \"\"\"Load pos_weight from JSON file\"\"\"\n",
        "        if os.path.exists(self.pos_weight_file):\n",
        "            try:\n",
        "                with open(self.pos_weight_file, 'r') as f:\n",
        "                    data = json.load(f)\n",
        "                    pos_weight = data['pos_weight']\n",
        "                    print(f\"üìÇ Loaded pos_weight: {pos_weight:.4f}\")\n",
        "                    return pos_weight\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error loading pos_weight: {e}\")\n",
        "        return None\n",
        "\n",
        "    def get_pos_weight(self, train_loader, force_recompute=False):\n",
        "        \"\"\"Get pos_weight - load if exists, otherwise compute and save\"\"\"\n",
        "        if not force_recompute:\n",
        "            pos_weight = self.load_pos_weight()\n",
        "            if pos_weight is not None:\n",
        "                return pos_weight\n",
        "\n",
        "        # Compute new pos_weight\n",
        "        pos_weight = self.compute_pos_weight(train_loader, max_batches=None)\n",
        "        self.save_pos_weight(pos_weight)\n",
        "        return pos_weight\n",
        "\n",
        "    def compute_pos_weight(self, train_loader, max_batches=None):\n",
        "        \"\"\"Compute pos_weight with better diagnostics\"\"\"\n",
        "        pos, neg = 0, 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            seg = batch[\"seg\"].to(self.device)\n",
        "            batch_pos = seg.sum().item()\n",
        "            batch_neg = seg.numel() - batch_pos\n",
        "\n",
        "            pos += batch_pos\n",
        "            neg += batch_neg\n",
        "            total_samples += 1\n",
        "\n",
        "            print(f\"Batch {i}: pos={batch_pos}, neg={batch_neg}, pos_ratio={batch_pos/seg.numel():.6f}\")\n",
        "\n",
        "            if max_batches and (i + 1) >= max_batches:\n",
        "                break\n",
        "\n",
        "        pos_ratio = pos / (pos + neg) if (pos + neg) > 0 else 0\n",
        "        pos_weight = neg / (pos + 1e-8)\n",
        "\n",
        "        print(f\"üìä Final Stats:\")\n",
        "        print(f\"  Total pos voxels: {pos}\")\n",
        "        print(f\"  Total neg voxels: {neg}\")\n",
        "        print(f\"  Positive ratio: {pos_ratio:.6f} ({pos_ratio*100:.4f}%)\")\n",
        "        print(f\"  Computed pos_weight: {pos_weight:.4f}\")\n",
        "        return pos_weight\n",
        "\n",
        "    def execute(self, train_loader, valid_loader):\n",
        "        model = UNet(\n",
        "            spatial_dims=3,\n",
        "            in_channels=1,\n",
        "            out_channels=1,\n",
        "            channels=(32, 64, 128, 256, 512),\n",
        "            strides=(2, 2, 2, 2),\n",
        "            num_res_units=4,\n",
        "            norm=Norm.INSTANCE,\n",
        "            dropout=0.1,\n",
        "            act='LEAKYRELU'\n",
        "        ).to(self.device)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=self.lr, weight_decay=1e-4)\n",
        "\n",
        "        scheduler = lr_scheduler.OneCycleLR(\n",
        "            optimizer,\n",
        "            max_lr=self.lr,\n",
        "            epochs=self.num_epochs,\n",
        "            steps_per_epoch=len(train_loader),\n",
        "            pct_start=0.1,\n",
        "            div_factor=10,\n",
        "            final_div_factor=100\n",
        "        )\n",
        "\n",
        "        # üîπ Auto-compute pos_weight before training\n",
        "        pos_weight = self.get_pos_weight(train_loader)\n",
        "        # Cap pos_weight to reasonable value\n",
        "        reasonable_pos_weight = min(pos_weight, 100.0)  # Maximum 50:1 imbalance\n",
        "        print(f\"üìä Using reasonable pos_weight: {reasonable_pos_weight:.2f} (capped from {pos_weight:.2f})\")\n",
        "        # FIX: Use either DiceBCELoss3D OR ImprovedDiceBCELoss, but be consistent\n",
        "        # If you have the improved version, use this:\n",
        "\n",
        "        loss_fn = DiceBCELoss3D(\n",
        "                  dice_w=0.6,        # Emphasize Dice for segmentation\n",
        "                  bce_w=0.4,         # Reduce BCE influence\n",
        "                  focal_alpha=0.9,   # Focus on positive class\n",
        "                  focal_gamma=3.0\n",
        "              ).set_pos_weight(reasonable_pos_weight, device=self.device)\n",
        "\n",
        "        # If you don't have ImprovedDiceBCELoss, use your original:\n",
        "        # loss_fn = DiceBCELoss3D().set_pos_weight(pos_weight, device=self.device)\n",
        "\n",
        "        accumulation_steps = 4\n",
        "\n",
        "        # ---- Resume training state ----\n",
        "        start_epoch = 1\n",
        "        start_val_loss_min = None\n",
        "        start_patience_counter = 0\n",
        "        history = {\"train_loss\": [], \"valid_loss\": []}\n",
        "\n",
        "        if os.path.exists(self.model_file):\n",
        "            checkpoint = torch.load(self.model_file, map_location=self.device)\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            if checkpoint.get('optimizer_state_dict'):\n",
        "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            start_epoch = checkpoint.get('epoch', 1) + 1\n",
        "            start_val_loss_min = checkpoint.get('val_loss', None)\n",
        "            start_patience_counter = checkpoint.get('patience_counter', 0)\n",
        "\n",
        "            # Load pos_weight from checkpoint if available\n",
        "            loaded_pos_weight = checkpoint.get('pos_weight')\n",
        "            if loaded_pos_weight is not None:\n",
        "                pos_weight = loaded_pos_weight\n",
        "                loss_fn = DiceBCELoss3D(\n",
        "                    dice_w=0.8,\n",
        "                    bce_w=0.2,\n",
        "                    focal_alpha=0.7,\n",
        "                    focal_gamma=2.0\n",
        "                ).set_pos_weight(pos_weight, device=self.device)\n",
        "                print(f\"üìÇ Loaded pos_weight from checkpoint: {pos_weight:.4f}\")\n",
        "\n",
        "        # History loading code (you'll need to implement this part)\n",
        "        # if os.path.exists(self.loss_result_path):\n",
        "        #     ... your history loading code ...\n",
        "\n",
        "        early_stopping = EarlyStopping(\n",
        "            patience=10,\n",
        "            min_delta=0.0005,\n",
        "            path=self.model_file,\n",
        "            start_val_loss_min=start_val_loss_min,\n",
        "            start_patience_counter=start_patience_counter,\n",
        "            pos_weight=pos_weight\n",
        "        )\n",
        "\n",
        "        if not os.path.exists(self.loss_result_path):\n",
        "            with open(self.loss_result_path, \"w\", newline=\"\") as f:\n",
        "                csv.writer(f).writerow([\"Epoch\", \"Train Loss\", \"Valid Loss\"])\n",
        "\n",
        "        # ---- Training loop ----\n",
        "        for epoch in range(start_epoch, self.num_epochs + 1):\n",
        "            start_time = time.time()\n",
        "\n",
        "            train_loss = self.train_one_epoch(\n",
        "                model, train_loader, optimizer, scheduler, accumulation_steps, loss_fn\n",
        "            )\n",
        "            valid_loss = self.evaluate(model, valid_loader, loss_fn)\n",
        "\n",
        "            mins, secs = self.epoch_time(start_time, time.time())\n",
        "            print(f\"Epoch {epoch:03d} | Time: {mins}m {secs}s | \"\n",
        "                  f\"Train: {train_loss:.6f} | Val: {valid_loss:.6f}\")\n",
        "\n",
        "            history['train_loss'].append(train_loss)\n",
        "            history['valid_loss'].append(valid_loss)\n",
        "\n",
        "            with open(self.loss_result_path, \"a\", newline=\"\") as f:\n",
        "                csv.writer(f).writerow([epoch, train_loss, valid_loss])\n",
        "\n",
        "            if early_stopping(valid_loss, model, epoch, optimizer, pos_weight):\n",
        "                print(\"üõë Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "            torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FilteredRandCropByPosNegLabeld(Transform):\n",
        "    \"\"\"Custom transform that ensures patches contain some positive samples\"\"\"\n",
        "    def __init__(self, keys, label_key, spatial_size, num_samples=4, min_pos_ratio=0.001):\n",
        "        self.keys = keys\n",
        "        self.label_key = label_key\n",
        "        self.spatial_size = spatial_size\n",
        "        self.num_samples = num_samples\n",
        "        self.min_pos_ratio = min_pos_ratio\n",
        "        self.cropper = RandCropByPosNegLabeld(\n",
        "            keys=keys, label_key=label_key, spatial_size=spatial_size,\n",
        "            pos=1, neg=1, num_samples=num_samples * 2,  # Generate extra samples for filtering\n",
        "            image_threshold=0\n",
        "        )\n",
        "\n",
        "    def __call__(self, data):\n",
        "        # Generate candidate patches\n",
        "        candidates = self.cropper(data)\n",
        "\n",
        "        # Filter patches to ensure minimum positive ratio\n",
        "        filtered_patches = []\n",
        "        for patch in candidates:\n",
        "            seg = patch[self.label_key]\n",
        "            pos_ratio = seg.sum().item() / seg.numel()\n",
        "            if pos_ratio >= self.min_pos_ratio or len(filtered_patches) < self.num_samples // 2:\n",
        "                filtered_patches.append(patch)\n",
        "            if len(filtered_patches) >= self.num_samples:\n",
        "                break\n",
        "\n",
        "        # If we don't have enough patches, add some from candidates\n",
        "        while len(filtered_patches) < self.num_samples and len(candidates) > 0:\n",
        "            filtered_patches.append(candidates.pop(0))\n",
        "\n",
        "        return filtered_patches[:self.num_samples]"
      ],
      "metadata": {
        "id": "El6TbMoXGWE9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (8). Pipeline"
      ],
      "metadata": {
        "id": "BZrW1cHg4OlM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwcTHPshqu0O",
        "outputId": "692c25e9-792b-4cf9-81b0-b4d085ca46ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Current Directory: /content/drive/MyDrive/PhDwork/Segmentation\n",
            "üì¶ Preparing datasets and transforms...\n",
            "üìä Filtered 340 -> 340 non-empty samples\n",
            "üìä Filtered 340 -> 340 non-empty samples\n",
            "‚úÖ Dataset sizes ‚Äî Train: 340, Valid: 43, Test: 38\n",
            "\n",
            "============================================================\n",
            "üîç RUNNING DATASET QUALITY ANALYSIS...\n",
            "============================================================\n",
            "üìä Analyzing dataset quality...\n",
            "\n",
            "üìä TRAIN (Full Volumes) Dataset Analysis:\n",
            "----------------------------------------\n",
            "  Total volumes: 340\n",
            "  Volumes with positives: 108 (31.8%)\n",
            "  Overall positive ratio: 0.001642 (0.1642%)\n",
            "  Total positive voxels: 1,170,646.0\n",
            "  Total voxels: 713,031,680\n",
            "\n",
            "üìä VALID (Full Volumes) Dataset Analysis:\n",
            "----------------------------------------\n",
            "  Total volumes: 43\n",
            "  Volumes with positives: 16 (37.2%)\n",
            "  Overall positive ratio: 0.002132 (0.2132%)\n",
            "  Total positive voxels: 192,281.0\n",
            "  Total voxels: 90,177,536\n",
            "\n",
            "üìä TEST (Full Volumes) Dataset Analysis:\n",
            "----------------------------------------\n",
            "  Total volumes: 38\n",
            "  Volumes with positives: 14 (36.8%)\n",
            "  Overall positive ratio: 0.001469 (0.1469%)\n",
            "  Total positive voxels: 117,089.0\n",
            "  Total voxels: 79,691,776\n",
            "\n",
            "üîç Analyzing Training Patches (After Sampling):\n",
            "\n",
            "üìä TRAIN PATCHES Patch Analysis (first 20 batches):\n",
            "--------------------------------------------------\n",
            "  Batch 0, Patch 0: 0.0/2097152 (0.000% positive)\n",
            "  Batch 0, Patch 1: 0.0/2097152 (0.000% positive)\n",
            "  Batch 0, Patch 2: 0.0/2097152 (0.000% positive)\n",
            "  Batch 0, Patch 3: 0.0/2097152 (0.000% positive)\n",
            "  Batch 1, Patch 0: 0.0/2097152 (0.000% positive)\n",
            "  Batch 1, Patch 1: 0.0/2097152 (0.000% positive)\n",
            "  Batch 1, Patch 2: 0.0/2097152 (0.000% positive)\n",
            "  Batch 1, Patch 3: 0.0/2097152 (0.000% positive)\n",
            "  Batch 2, Patch 0: 0.0/2097152 (0.000% positive)\n",
            "  Batch 2, Patch 1: 0.0/2097152 (0.000% positive)\n",
            "  Batch 2, Patch 2: 0.0/2097152 (0.000% positive)\n",
            "  Batch 2, Patch 3: 0.0/2097152 (0.000% positive)\n",
            "  Batch 3, Patch 0: 0.0/2097152 (0.000% positive)\n",
            "  Batch 3, Patch 1: 0.0/2097152 (0.000% positive)\n",
            "  Batch 3, Patch 2: 0.0/2097152 (0.000% positive)\n",
            "  Batch 3, Patch 3: 0.0/2097152 (0.000% positive)\n",
            "  Batch 4, Patch 0: 0.0/2097152 (0.000% positive)\n",
            "  Batch 4, Patch 1: 0.0/2097152 (0.000% positive)\n",
            "  Batch 4, Patch 2: 0.0/2097152 (0.000% positive)\n",
            "  Batch 4, Patch 3: 0.0/2097152 (0.000% positive)\n",
            "  Batch 5, Patch 0: 3162.0/2097152 (0.151% positive)\n",
            "  Batch 5, Patch 1: 3162.0/2097152 (0.151% positive)\n",
            "  Batch 5, Patch 2: 3162.0/2097152 (0.151% positive)\n",
            "  Batch 5, Patch 3: 3162.0/2097152 (0.151% positive)\n",
            "  Batch 6, Patch 0: 0.0/2097152 (0.000% positive)\n",
            "  Batch 6, Patch 1: 0.0/2097152 (0.000% positive)\n",
            "  Batch 6, Patch 2: 0.0/2097152 (0.000% positive)\n",
            "  Batch 6, Patch 3: 0.0/2097152 (0.000% positive)\n",
            "  Batch 7, Patch 0: 0.0/2097152 (0.000% positive)\n",
            "  Batch 7, Patch 1: 0.0/2097152 (0.000% positive)\n",
            "  Batch 7, Patch 2: 0.0/2097152 (0.000% positive)\n",
            "  Batch 7, Patch 3: 0.0/2097152 (0.000% positive)\n",
            "  Batch 8, Patch 0: 0.0/2097152 (0.000% positive)\n",
            "  Batch 8, Patch 1: 0.0/2097152 (0.000% positive)\n",
            "  Batch 8, Patch 2: 0.0/2097152 (0.000% positive)\n",
            "  Batch 8, Patch 3: 0.0/2097152 (0.000% positive)\n",
            "  Batch 9, Patch 0: 864.0/2097152 (0.041% positive)\n",
            "  Batch 9, Patch 1: 864.0/2097152 (0.041% positive)\n",
            "  Batch 9, Patch 2: 864.0/2097152 (0.041% positive)\n",
            "  Batch 9, Patch 3: 864.0/2097152 (0.041% positive)\n",
            "  Batch 10, Patch 0: 0.0/2097152 (0.000% positive)\n",
            "  Batch 10, Patch 1: 0.0/2097152 (0.000% positive)\n",
            "  Batch 10, Patch 2: 0.0/2097152 (0.000% positive)\n",
            "  Batch 10, Patch 3: 0.0/2097152 (0.000% positive)\n",
            "  Batch 11, Patch 0: 378.0/2097152 (0.018% positive)\n",
            "  Batch 11, Patch 1: 378.0/2097152 (0.018% positive)\n",
            "  Batch 11, Patch 2: 378.0/2097152 (0.018% positive)\n",
            "  Batch 11, Patch 3: 378.0/2097152 (0.018% positive)\n",
            "  Batch 12, Patch 0: 1305.0/2097152 (0.062% positive)\n",
            "  Batch 12, Patch 1: 1305.0/2097152 (0.062% positive)\n",
            "  Batch 12, Patch 2: 1305.0/2097152 (0.062% positive)\n",
            "  Batch 12, Patch 3: 1305.0/2097152 (0.062% positive)\n",
            "  Batch 13, Patch 0: 0.0/2097152 (0.000% positive)\n",
            "  Batch 13, Patch 1: 0.0/2097152 (0.000% positive)\n",
            "  Batch 13, Patch 2: 0.0/2097152 (0.000% positive)\n",
            "  Batch 13, Patch 3: 0.0/2097152 (0.000% positive)\n",
            "  Batch 14, Patch 0: 0.0/2097152 (0.000% positive)\n",
            "  Batch 14, Patch 1: 0.0/2097152 (0.000% positive)\n",
            "  Batch 14, Patch 2: 0.0/2097152 (0.000% positive)\n",
            "  Batch 14, Patch 3: 0.0/2097152 (0.000% positive)\n",
            "  Batch 15, Patch 0: 38736.0/2097152 (1.847% positive)\n",
            "  Batch 15, Patch 1: 38736.0/2097152 (1.847% positive)\n",
            "  Batch 15, Patch 2: 38736.0/2097152 (1.847% positive)\n",
            "  Batch 15, Patch 3: 38736.0/2097152 (1.847% positive)\n",
            "  Batch 16, Patch 0: 0.0/2097152 (0.000% positive)\n",
            "  Batch 16, Patch 1: 0.0/2097152 (0.000% positive)\n",
            "  Batch 16, Patch 2: 0.0/2097152 (0.000% positive)\n",
            "  Batch 16, Patch 3: 0.0/2097152 (0.000% positive)\n",
            "  Batch 17, Patch 0: 0.0/2097152 (0.000% positive)\n",
            "  Batch 17, Patch 1: 0.0/2097152 (0.000% positive)\n",
            "  Batch 17, Patch 2: 0.0/2097152 (0.000% positive)\n",
            "  Batch 17, Patch 3: 0.0/2097152 (0.000% positive)\n",
            "  Batch 18, Patch 0: 0.0/2097152 (0.000% positive)\n",
            "  Batch 18, Patch 1: 0.0/2097152 (0.000% positive)\n",
            "  Batch 18, Patch 2: 0.0/2097152 (0.000% positive)\n",
            "  Batch 18, Patch 3: 0.0/2097152 (0.000% positive)\n",
            "  Batch 19, Patch 0: 0.0/2097152 (0.000% positive)\n",
            "  Batch 19, Patch 1: 0.0/2097152 (0.000% positive)\n",
            "  Batch 19, Patch 2: 0.0/2097152 (0.000% positive)\n",
            "  Batch 19, Patch 3: 0.0/2097152 (0.000% positive)\n",
            "\n",
            "üìà TRAIN PATCHES Patch Summary:\n",
            "  Total patches analyzed: 80\n",
            "  Patches with positives: 20 (25.0%)\n",
            "  Overall positive ratio: 0.001060 (0.1060%)\n",
            "Batch 0: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 1: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 2: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 3: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 4: pos=27980.0, neg=8360628.0, pos_ratio=0.003335\n",
            "Batch 5: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 6: pos=378808.0, neg=8009800.0, pos_ratio=0.045157\n",
            "Batch 7: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 8: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 9: pos=274696.0, neg=8113912.0, pos_ratio=0.032746\n",
            "Batch 10: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 11: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 12: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 13: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 14: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 15: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 16: pos=5220.0, neg=8383388.0, pos_ratio=0.000622\n",
            "Batch 17: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 18: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 19: pos=34500.0, neg=8354108.0, pos_ratio=0.004113\n",
            "Batch 20: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 21: pos=3376.0, neg=8385232.0, pos_ratio=0.000402\n",
            "Batch 22: pos=8580.0, neg=8380028.0, pos_ratio=0.001023\n",
            "Batch 23: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 24: pos=17532.0, neg=8371076.0, pos_ratio=0.002090\n",
            "Batch 25: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 26: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 27: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 28: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 29: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 30: pos=1512.0, neg=8387096.0, pos_ratio=0.000180\n",
            "Batch 31: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 32: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 33: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 34: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 35: pos=45824.0, neg=8342784.0, pos_ratio=0.005463\n",
            "Batch 36: pos=220572.0, neg=8168036.0, pos_ratio=0.026294\n",
            "Batch 37: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 38: pos=24532.0, neg=8364076.0, pos_ratio=0.002924\n",
            "Batch 39: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 40: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 41: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 42: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 43: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 44: pos=112.0, neg=8388496.0, pos_ratio=0.000013\n",
            "Batch 45: pos=245612.0, neg=8142996.0, pos_ratio=0.029279\n",
            "Batch 46: pos=204.0, neg=8388404.0, pos_ratio=0.000024\n",
            "Batch 47: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 48: pos=81960.0, neg=8306648.0, pos_ratio=0.009770\n",
            "Batch 49: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 50: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 51: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 52: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 53: pos=65412.0, neg=8323196.0, pos_ratio=0.007798\n",
            "Batch 54: pos=278832.0, neg=8109776.0, pos_ratio=0.033239\n",
            "Batch 55: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 56: pos=4128.0, neg=8384480.0, pos_ratio=0.000492\n",
            "Batch 57: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 58: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 59: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 60: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 61: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 62: pos=828.0, neg=8387780.0, pos_ratio=0.000099\n",
            "Batch 63: pos=8564.0, neg=8380044.0, pos_ratio=0.001021\n",
            "Batch 64: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 65: pos=1908.0, neg=8386700.0, pos_ratio=0.000227\n",
            "Batch 66: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 67: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 68: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 69: pos=6592.0, neg=8382016.0, pos_ratio=0.000786\n",
            "Batch 70: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 71: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 72: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 73: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 74: pos=384.0, neg=8388224.0, pos_ratio=0.000046\n",
            "Batch 75: pos=3456.0, neg=8385152.0, pos_ratio=0.000412\n",
            "Batch 76: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 77: pos=31980.0, neg=8356628.0, pos_ratio=0.003812\n",
            "Batch 78: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 79: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 80: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 81: pos=60888.0, neg=8327720.0, pos_ratio=0.007258\n",
            "Batch 82: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 83: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 84: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 85: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 86: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 87: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 88: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 89: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 90: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 91: pos=68316.0, neg=8320292.0, pos_ratio=0.008144\n",
            "Batch 92: pos=3364.0, neg=8385244.0, pos_ratio=0.000401\n",
            "Batch 93: pos=3348.0, neg=8385260.0, pos_ratio=0.000399\n",
            "Batch 94: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 95: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 96: pos=7404.0, neg=8381204.0, pos_ratio=0.000883\n",
            "Batch 97: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 98: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 99: pos=696.0, neg=8387912.0, pos_ratio=0.000083\n",
            "Batch 100: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 101: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 102: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 103: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 104: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 105: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 106: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 107: pos=65424.0, neg=8323184.0, pos_ratio=0.007799\n",
            "Batch 108: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 109: pos=123592.0, neg=8265016.0, pos_ratio=0.014733\n",
            "Batch 110: pos=1848.0, neg=8386760.0, pos_ratio=0.000220\n",
            "Batch 111: pos=4068.0, neg=8384540.0, pos_ratio=0.000485\n",
            "Batch 112: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 113: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 114: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 115: pos=504.0, neg=8388104.0, pos_ratio=0.000060\n",
            "Batch 116: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 117: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 118: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 119: pos=144796.0, neg=8243812.0, pos_ratio=0.017261\n",
            "Batch 120: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 121: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 122: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 123: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 124: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 125: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 126: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 127: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 128: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 129: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 130: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 131: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 132: pos=1412.0, neg=8387196.0, pos_ratio=0.000168\n",
            "Batch 133: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 134: pos=600.0, neg=8388008.0, pos_ratio=0.000072\n",
            "Batch 135: pos=5244.0, neg=8383364.0, pos_ratio=0.000625\n",
            "Batch 136: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 137: pos=10192.0, neg=8378416.0, pos_ratio=0.001215\n",
            "Batch 138: pos=156888.0, neg=8231720.0, pos_ratio=0.018703\n",
            "Batch 139: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 140: pos=31536.0, neg=8357072.0, pos_ratio=0.003759\n",
            "Batch 141: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 142: pos=91164.0, neg=8297444.0, pos_ratio=0.010868\n",
            "Batch 143: pos=7080.0, neg=8381528.0, pos_ratio=0.000844\n",
            "Batch 144: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 145: pos=140.0, neg=8388468.0, pos_ratio=0.000017\n",
            "Batch 146: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 147: pos=52628.0, neg=8335980.0, pos_ratio=0.006274\n",
            "Batch 148: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 149: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 150: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 151: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 152: pos=154944.0, neg=8233664.0, pos_ratio=0.018471\n",
            "Batch 153: pos=1628.0, neg=8386980.0, pos_ratio=0.000194\n",
            "Batch 154: pos=49080.0, neg=8339528.0, pos_ratio=0.005851\n",
            "Batch 155: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 156: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 157: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 158: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 159: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 160: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 161: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 162: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 163: pos=3236.0, neg=8385372.0, pos_ratio=0.000386\n",
            "Batch 164: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 165: pos=6064.0, neg=8382544.0, pos_ratio=0.000723\n",
            "Batch 166: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 167: pos=264.0, neg=8388344.0, pos_ratio=0.000031\n",
            "Batch 168: pos=768.0, neg=8387840.0, pos_ratio=0.000092\n",
            "Batch 169: pos=15160.0, neg=8373448.0, pos_ratio=0.001807\n",
            "Batch 170: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 171: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 172: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 173: pos=52620.0, neg=8335988.0, pos_ratio=0.006273\n",
            "Batch 174: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 175: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 176: pos=33696.0, neg=8354912.0, pos_ratio=0.004017\n",
            "Batch 177: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 178: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 179: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 180: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 181: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 182: pos=25980.0, neg=8362628.0, pos_ratio=0.003097\n",
            "Batch 183: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 184: pos=29476.0, neg=8359132.0, pos_ratio=0.003514\n",
            "Batch 185: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 186: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 187: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 188: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 189: pos=492.0, neg=8388116.0, pos_ratio=0.000059\n",
            "Batch 190: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 191: pos=376.0, neg=8388232.0, pos_ratio=0.000045\n",
            "Batch 192: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 193: pos=9400.0, neg=8379208.0, pos_ratio=0.001121\n",
            "Batch 194: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 195: pos=98064.0, neg=8290544.0, pos_ratio=0.011690\n",
            "Batch 196: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 197: pos=2404.0, neg=8386204.0, pos_ratio=0.000287\n",
            "Batch 198: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 199: pos=94376.0, neg=8294232.0, pos_ratio=0.011250\n",
            "Batch 200: pos=479148.0, neg=7909460.0, pos_ratio=0.057119\n",
            "Batch 201: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 202: pos=195608.0, neg=8193000.0, pos_ratio=0.023318\n",
            "Batch 203: pos=77272.0, neg=8311336.0, pos_ratio=0.009212\n",
            "Batch 204: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 205: pos=38016.0, neg=8350592.0, pos_ratio=0.004532\n",
            "Batch 206: pos=372.0, neg=8388236.0, pos_ratio=0.000044\n",
            "Batch 207: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 208: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 209: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 210: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 211: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 212: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 213: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 214: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 215: pos=1068.0, neg=8387540.0, pos_ratio=0.000127\n",
            "Batch 216: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 217: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 218: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 219: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 220: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 221: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 222: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 223: pos=104.0, neg=8388504.0, pos_ratio=0.000012\n",
            "Batch 224: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 225: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 226: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 227: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 228: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 229: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 230: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 231: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 232: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 233: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 234: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 235: pos=17788.0, neg=8370820.0, pos_ratio=0.002120\n",
            "Batch 236: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 237: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 238: pos=8056.0, neg=8380552.0, pos_ratio=0.000960\n",
            "Batch 239: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 240: pos=3936.0, neg=8384672.0, pos_ratio=0.000469\n",
            "Batch 241: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 242: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 243: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 244: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 245: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 246: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 247: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 248: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 249: pos=484.0, neg=8388124.0, pos_ratio=0.000058\n",
            "Batch 250: pos=42928.0, neg=8345680.0, pos_ratio=0.005117\n",
            "Batch 251: pos=158640.0, neg=8229968.0, pos_ratio=0.018911\n",
            "Batch 252: pos=2460.0, neg=8386148.0, pos_ratio=0.000293\n",
            "Batch 253: pos=148.0, neg=8388460.0, pos_ratio=0.000018\n",
            "Batch 254: pos=2400.0, neg=8386208.0, pos_ratio=0.000286\n",
            "Batch 255: pos=25776.0, neg=8362832.0, pos_ratio=0.003073\n",
            "Batch 256: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 257: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 258: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 259: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 260: pos=4356.0, neg=8384252.0, pos_ratio=0.000519\n",
            "Batch 261: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 262: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 263: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 264: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 265: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 266: pos=24132.0, neg=8364476.0, pos_ratio=0.002877\n",
            "Batch 267: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 268: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 269: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 270: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 271: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 272: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 273: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 274: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 275: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 276: pos=1728.0, neg=8386880.0, pos_ratio=0.000206\n",
            "Batch 277: pos=192.0, neg=8388416.0, pos_ratio=0.000023\n",
            "Batch 278: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 279: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 280: pos=38032.0, neg=8350576.0, pos_ratio=0.004534\n",
            "Batch 281: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 282: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 283: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 284: pos=76.0, neg=8388532.0, pos_ratio=0.000009\n",
            "Batch 285: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 286: pos=160.0, neg=8388448.0, pos_ratio=0.000019\n",
            "Batch 287: pos=4588.0, neg=8384020.0, pos_ratio=0.000547\n",
            "Batch 288: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 289: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 290: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 291: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 292: pos=15676.0, neg=8372932.0, pos_ratio=0.001869\n",
            "Batch 293: pos=12508.0, neg=8376100.0, pos_ratio=0.001491\n",
            "Batch 294: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 295: pos=1112.0, neg=8387496.0, pos_ratio=0.000133\n",
            "Batch 296: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 297: pos=7716.0, neg=8380892.0, pos_ratio=0.000920\n",
            "Batch 298: pos=24.0, neg=8388584.0, pos_ratio=0.000003\n",
            "Batch 299: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 300: pos=12648.0, neg=8375960.0, pos_ratio=0.001508\n",
            "Batch 301: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 302: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 303: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 304: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 305: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 306: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 307: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 308: pos=14680.0, neg=8373928.0, pos_ratio=0.001750\n",
            "Batch 309: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 310: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 311: pos=176.0, neg=8388432.0, pos_ratio=0.000021\n",
            "Batch 312: pos=1044.0, neg=8387564.0, pos_ratio=0.000124\n",
            "Batch 313: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 314: pos=248.0, neg=8388360.0, pos_ratio=0.000030\n",
            "Batch 315: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 316: pos=77736.0, neg=8310872.0, pos_ratio=0.009267\n",
            "Batch 317: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 318: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 319: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 320: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 321: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 322: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 323: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 324: pos=13744.0, neg=8374864.0, pos_ratio=0.001638\n",
            "Batch 325: pos=612.0, neg=8387996.0, pos_ratio=0.000073\n",
            "Batch 326: pos=10548.0, neg=8378060.0, pos_ratio=0.001257\n",
            "Batch 327: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 328: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 329: pos=204.0, neg=8388404.0, pos_ratio=0.000024\n",
            "Batch 330: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 331: pos=7560.0, neg=8381048.0, pos_ratio=0.000901\n",
            "Batch 332: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 333: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 334: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 335: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 336: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 337: pos=186616.0, neg=8201992.0, pos_ratio=0.022246\n",
            "Batch 338: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "Batch 339: pos=0.0, neg=8388608.0, pos_ratio=0.000000\n",
            "üìä Final Stats:\n",
            "  Total pos voxels: 4682584.0\n",
            "  Total neg voxels: 2847444136.0\n",
            "  Positive ratio: 0.001642 (0.1642%)\n",
            "  Computed pos_weight: 608.0925\n",
            "üíæ Saved pos_weight: 608.0925\n",
            "üìä Using reasonable pos_weight: 100.00 (capped from 608.09)\n",
            "Epoch 001 | Time: 66m 57s | Train: 0.612057 | Val: 0.606740\n",
            "‚úÖ Validation loss improved (0.606740). Saving model...\n",
            "üíæ Checkpoint saved with pos_weight: 608.0925\n",
            "Epoch 002 | Time: 67m 46s | Train: 0.606978 | Val: 0.605226\n",
            "‚úÖ Validation loss improved (0.605226). Saving model...\n",
            "üíæ Checkpoint saved with pos_weight: 608.0925\n"
          ]
        }
      ],
      "source": [
        "class UnetPipeline:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        set_determinism(seed=42)\n",
        "\n",
        "        # paths\n",
        "        os.chdir(self.config['target_dir'])\n",
        "        print(f\"üìÅ Current Directory: {os.getcwd()}\")\n",
        "        self.output_dir = os.path.join(\".\", \"results\", self.config['output_folder_name'])\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "        self.loss_result_file = os.path.join(self.output_dir, \"train_and_valid_loss_results.csv\")\n",
        "        self.model_file = os.path.join(self.output_dir, \"model.pth\")\n",
        "        self.test_metrics_file = os.path.join(self.output_dir, \"test_metrics.csv\")\n",
        "        self.test_result_path = os.path.join(self.output_dir, \"test_outputs\")\n",
        "        os.makedirs(self.test_result_path, exist_ok=True)\n",
        "\n",
        "        self.dataset_dir = os.path.join(\"./datasets\", f\"Datasets_{self.config['transformation']}\")\n",
        "\n",
        "        # prepare loaders\n",
        "        print(\"üì¶ Preparing datasets and transforms...\")\n",
        "        self.train_loader, self.valid_loader, self.test_loader, self.full_train_loader = self.prepare_loaders()\n",
        "        # üî• ADD DATASET QUALITY ANALYSIS HERE:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üîç RUNNING DATASET QUALITY ANALYSIS...\")\n",
        "        print(\"=\"*60)\n",
        "        self.analyze_dataset_quality()\n",
        "        # self.check_data_sanity(self.train_loader, \"train\")\n",
        "        # self.check_data_sanity(self.valid_loader, \"valid\")\n",
        "        # self.check_data_sanity(self.test_loader, \"test\")\n",
        "\n",
        "        # # run sanity checks on full-volume loader (not patch loader)\n",
        "        # print(\"üîé Running dataset sanity checks on full volumes (not patches)...\")\n",
        "        # self.print_dataset_stats(self.full_train_loader, name=\"train_full\", n=30)\n",
        "        # self.print_dataset_stats(self.valid_loader, name=\"valid\", n=30)\n",
        "        # self.print_dataset_stats(self.test_loader, name=\"test\", n=30)\n",
        "\n",
        "\n",
        "    def analyze_dataset_quality(self):\n",
        "        \"\"\"Comprehensive dataset quality analysis\"\"\"\n",
        "        print(\"üìä Analyzing dataset quality...\")\n",
        "\n",
        "        # Analyze full volumes (before patch sampling)\n",
        "        self._analyze_loader_quality(self.full_train_loader, \"TRAIN (Full Volumes)\")\n",
        "        self._analyze_loader_quality(self.valid_loader, \"VALID (Full Volumes)\")\n",
        "        self._analyze_loader_quality(self.test_loader, \"TEST (Full Volumes)\")\n",
        "\n",
        "        # Analyze training patches (after patch sampling)\n",
        "        print(\"\\n\" + \"üîç Analyzing Training Patches (After Sampling):\")\n",
        "        self._analyze_patch_quality(self.train_loader, \"TRAIN PATCHES\")\n",
        "\n",
        "    def _analyze_loader_quality(self, loader, name):\n",
        "        \"\"\"Analyze a specific data loader\"\"\"\n",
        "        print(f\"\\nüìä {name} Dataset Analysis:\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        total_volumes = 0\n",
        "        volumes_with_positives = 0\n",
        "        total_pos_voxels = 0\n",
        "        total_voxels = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(loader):\n",
        "            seg = batch[\"seg\"]\n",
        "\n",
        "            # Check each volume in batch\n",
        "            for i in range(seg.shape[0]):\n",
        "                volume_seg = seg[i]\n",
        "                total_volumes += 1\n",
        "                volume_positives = volume_seg.sum().item()\n",
        "                volume_voxels = volume_seg.numel()\n",
        "\n",
        "                if volume_positives > 0:\n",
        "                    volumes_with_positives += 1\n",
        "\n",
        "                total_pos_voxels += volume_positives\n",
        "                total_voxels += volume_voxels\n",
        "\n",
        "        overall_pos_ratio = total_pos_voxels / total_voxels if total_voxels > 0 else 0\n",
        "\n",
        "        print(f\"  Total volumes: {total_volumes}\")\n",
        "        print(f\"  Volumes with positives: {volumes_with_positives} ({volumes_with_positives/total_volumes*100:.1f}%)\")\n",
        "        print(f\"  Overall positive ratio: {overall_pos_ratio:.6f} ({overall_pos_ratio*100:.4f}%)\")\n",
        "        print(f\"  Total positive voxels: {total_pos_voxels:,}\")\n",
        "        print(f\"  Total voxels: {total_voxels:,}\")\n",
        "\n",
        "        return overall_pos_ratio\n",
        "\n",
        "    def _analyze_patch_quality(self, loader, name, max_batches=20):\n",
        "        \"\"\"Analyze patch distribution in training loader\"\"\"\n",
        "        print(f\"\\nüìä {name} Patch Analysis (first {max_batches} batches):\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        total_patches = 0\n",
        "        patches_with_positives = 0\n",
        "        total_pos_voxels = 0\n",
        "        total_voxels = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(loader):\n",
        "            if batch_idx >= max_batches:\n",
        "                break\n",
        "\n",
        "            seg = batch[\"seg\"]\n",
        "\n",
        "            # Check each patch in batch\n",
        "            for i in range(seg.shape[0]):\n",
        "                patch_seg = seg[i]\n",
        "                total_patches += 1\n",
        "                patch_positives = patch_seg.sum().item()\n",
        "                patch_voxels = patch_seg.numel()\n",
        "\n",
        "                if patch_positives > 0:\n",
        "                    patches_with_positives += 1\n",
        "\n",
        "                total_pos_voxels += patch_positives\n",
        "                total_voxels += patch_voxels\n",
        "\n",
        "                pos_ratio = patch_positives / patch_voxels if patch_voxels > 0 else 0\n",
        "                print(f\"  Batch {batch_idx}, Patch {i}: {patch_positives}/{patch_voxels} \"\n",
        "                      f\"({pos_ratio*100:.3f}% positive)\")\n",
        "\n",
        "        overall_pos_ratio = total_pos_voxels / total_voxels if total_voxels > 0 else 0\n",
        "\n",
        "        print(f\"\\nüìà {name} Patch Summary:\")\n",
        "        print(f\"  Total patches analyzed: {total_patches}\")\n",
        "        print(f\"  Patches with positives: {patches_with_positives} ({patches_with_positives/total_patches*100:.1f}%)\")\n",
        "        print(f\"  Overall positive ratio: {overall_pos_ratio:.6f} ({overall_pos_ratio*100:.4f}%)\")\n",
        "\n",
        "        return overall_pos_ratio\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def check_data_sanity(self, loader, name=\"train\"):\n",
        "        \"\"\"Check if data is loaded correctly\"\"\"\n",
        "        for i, batch in enumerate(loader):\n",
        "            vol = batch[\"vol\"]\n",
        "            seg = batch[\"seg\"]\n",
        "\n",
        "            print(f\"\\nüîç {name} Batch {i}:\")\n",
        "            print(f\"  Volume shape: {vol.shape}\")\n",
        "            print(f\"  Volume range: [{vol.min():.3f}, {vol.max():.3f}]\")\n",
        "            print(f\"  Seg shape: {seg.shape}\")\n",
        "            print(f\"  Seg unique values: {torch.unique(seg)}\")\n",
        "            print(f\"  Seg positives: {seg.sum().item()}/{seg.numel()} ({seg.sum().item()/seg.numel()*100:.3f}%)\")\n",
        "\n",
        "            if i >= 2:  # Check first 3 batches\n",
        "                break\n",
        "\n",
        "    def print_dataset_stats(self, loader, name=\"train\", n=20):\n",
        "        tot_pos = 0.0\n",
        "        tot_vox = 0\n",
        "        samples = 0\n",
        "        unique_vals = set()\n",
        "        for i, batch in enumerate(loader):\n",
        "            seg = batch[\"seg\"][0, 0]  # [D,H,W]\n",
        "            seg_cpu = seg.cpu()\n",
        "            tot_pos += float(seg_cpu.sum().item())\n",
        "            tot_vox += seg_cpu.numel()\n",
        "            samples += 1\n",
        "            vals = torch.unique(seg_cpu)\n",
        "            for v in vals.tolist():\n",
        "                unique_vals.add(v)\n",
        "            if i + 1 >= n:\n",
        "                break\n",
        "        frac = tot_pos / (tot_vox + 1e-12)\n",
        "        print(f\"  [{name}] checked {samples} samples. total pos voxels: {tot_pos:.1f}, total voxels: {tot_vox}, pos fraction: {frac:.6g}\")\n",
        "        print(f\"  [{name}] example mask unique values (approx): {sorted(list(unique_vals))[:10]}\")\n",
        "        if frac == 0:\n",
        "            print(f\"  ‚ö†Ô∏è [{name}] No positives found in the inspected samples.\")\n",
        "        if any(v not in (0.0, 1.0) for v in unique_vals):\n",
        "            print(f\"  ‚ö†Ô∏è [{name}] Found non-binary mask values (not only 0/1). Use nearest interpolation for segs.\")\n",
        "\n",
        "        return frac\n",
        "\n",
        "    def filter_empty_samples(self, file_list):\n",
        "        \"\"\"Remove samples that have no positive segmentation labels\"\"\"\n",
        "        filtered_files = []\n",
        "\n",
        "        for file_pair in file_list:\n",
        "            # Load just the segmentation to check if it's empty\n",
        "            seg_loader = Compose([\n",
        "                LoadImaged(keys=[\"seg\"]),\n",
        "                EnsureChannelFirstd(keys=[\"seg\"]),\n",
        "            ])\n",
        "\n",
        "            try:\n",
        "                seg_data = seg_loader(file_pair)[\"seg\"]\n",
        "                if seg_data.sum() > 0:  # Only keep if has positive voxels\n",
        "                    filtered_files.append(file_pair)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        print(f\"üìä Filtered {len(file_list)} -> {len(filtered_files)} non-empty samples\")\n",
        "        return filtered_files\n",
        "\n",
        "    def prepare_loaders(self):\n",
        "        pixdim = (1, 1, 1)\n",
        "        a_min, a_max = -1000, 700\n",
        "        patch_size = (128, 128, 128)\n",
        "\n",
        "\n",
        "        # Use in prepare_loaders:\n",
        "        def get_files(split):\n",
        "            ct = sorted(glob(os.path.join(self.dataset_dir, split, \"ct\", \"*.nii.gz\")))\n",
        "            seg = sorted(glob(os.path.join(self.dataset_dir, split, \"segment\", \"*.nii.gz\")))\n",
        "            if len(ct) != len(seg):\n",
        "                raise RuntimeError(f\"Mismatch CT/SEG for {split}: {len(ct)} vs {len(seg)}\")\n",
        "\n",
        "            files = [{\"vol\": c, \"seg\": s} for c, s in zip(ct, seg)]\n",
        "\n",
        "            # Filter out empty samples for training\n",
        "            if split == \"train\":\n",
        "                files = self.filter_empty_samples(files)\n",
        "\n",
        "            return files\n",
        "\n",
        "\n",
        "\n",
        "       # COMMON base transforms for ALL datasets\n",
        "        base_transforms = Compose([\n",
        "            LoadImaged(keys=[\"vol\", \"seg\"]),\n",
        "            EnsureChannelFirstd(keys=[\"vol\", \"seg\"]),\n",
        "            Spacingd(keys=[\"vol\", \"seg\"], pixdim=pixdim, mode=(\"bilinear\", \"nearest\")),\n",
        "            Orientationd(keys=[\"vol\", \"seg\"], axcodes=\"RAS\", labels=None),\n",
        "            ScaleIntensityRanged(keys=[\"vol\"], a_min=a_min, a_max=a_max, b_min=0.0, b_max=1.0, clip=True),\n",
        "            # ADD THIS for consistent normalization:\n",
        "            NormalizeIntensityd(keys=[\"vol\"], subtrahend=0.5, divisor=0.5),  # Normalize to [-1, 1]\n",
        "            CropForegroundd(keys=[\"vol\", \"seg\"], source_key=\"vol\"),\n",
        "            ResizeWithPadOrCropd(keys=[\"vol\", \"seg\"], spatial_size=patch_size, mode=\"constant\"),\n",
        "            ToTensord(keys=[\"vol\", \"seg\"]),\n",
        "            EnsureTyped(keys=[\"vol\",\"seg\"], track_meta=True),\n",
        "        ])\n",
        "\n",
        "        # Train transforms - ADD AUGMENTATIONS CAREFULLY\n",
        "        train_transforms = Compose([\n",
        "            LoadImaged(keys=[\"vol\", \"seg\"]),\n",
        "            EnsureChannelFirstd(keys=[\"vol\", \"seg\"]),\n",
        "            Spacingd(keys=[\"vol\", \"seg\"], pixdim=pixdim, mode=(\"bilinear\", \"nearest\")),\n",
        "            Orientationd(keys=[\"vol\", \"seg\"], axcodes=\"RAS\", labels=None),\n",
        "            ScaleIntensityRanged(keys=[\"vol\"], a_min=a_min, a_max=a_max, b_min=0.0, b_max=1.0, clip=True),\n",
        "            NormalizeIntensityd(keys=[\"vol\"], subtrahend=0.5, divisor=0.5),  # Consistent normalization\n",
        "            CropForegroundd(keys=[\"vol\", \"seg\"], source_key=\"vol\"),\n",
        "            ResizeWithPadOrCropd(keys=[\"vol\", \"seg\"], spatial_size=patch_size, mode=\"constant\"),\n",
        "\n",
        "            # ONLY sample from volumes that have positive labels\n",
        "            FilteredRandCropByPosNegLabeld(\n",
        "                keys=[\"vol\", \"seg\"],\n",
        "                label_key=\"seg\",\n",
        "                spatial_size=patch_size,\n",
        "                num_samples=4,\n",
        "                min_pos_ratio=0.001  # At least 0.1% positive voxels\n",
        "            ),\n",
        "\n",
        "            # Conservative augmentations\n",
        "            RandFlipd(keys=[\"vol\", \"seg\"], prob=0.3, spatial_axis=0),\n",
        "            RandFlipd(keys=[\"vol\", \"seg\"], prob=0.3, spatial_axis=1),\n",
        "            RandGaussianNoised(keys=[\"vol\"], prob=0.2, mean=0.0, std=0.05),\n",
        "\n",
        "            ToTensord(keys=[\"vol\", \"seg\"]),\n",
        "            EnsureTyped(keys=[\"vol\", \"seg\"]),\n",
        "        ])\n",
        "\n",
        "\n",
        "        # Create datasets and loaders\n",
        "        train_ds = Dataset(get_files(\"train\"), train_transforms)\n",
        "        valid_ds = Dataset(get_files(\"valid\"), base_transforms)\n",
        "        test_ds  = Dataset(get_files(\"test\"), base_transforms)\n",
        "\n",
        "        # Optionally you can use CacheDataset for speed (uncomment & tune)\n",
        "        # from monai.data import CacheDataset\n",
        "        # train_ds = CacheDataset(data=get_files(\"train\"), transform=train_transforms, cache_rate=0.5, num_workers=4)\n",
        "\n",
        "        train_loader = DataLoader(train_ds, batch_size=self.config['batch_size'], shuffle=True, num_workers=0, collate_fn=pad_list_data_collate)\n",
        "        valid_loader = DataLoader(valid_ds, batch_size=self.config['batch_size'], shuffle=False, num_workers=0, collate_fn=pad_list_data_collate)\n",
        "        test_loader  = DataLoader(test_ds,  batch_size=1, shuffle=False, num_workers=0, collate_fn=pad_list_data_collate)\n",
        "\n",
        "        # full volume train loader for sanity checks (no patching)\n",
        "        full_train_ds = Dataset(get_files(\"train\"), base_transforms)\n",
        "        full_train_loader = DataLoader(full_train_ds, batch_size=1, shuffle=False, num_workers=0, collate_fn=pad_list_data_collate)\n",
        "\n",
        "        print(f\"‚úÖ Dataset sizes ‚Äî Train: {len(train_ds)}, Valid: {len(valid_ds)}, Test: {len(test_ds)}\")\n",
        "\n",
        "        return train_loader, valid_loader, test_loader, full_train_loader\n",
        "\n",
        "    def train(self):\n",
        "        trainer = UnetTrain(\n",
        "            model_file=self.model_file,\n",
        "            loss_result_path=self.loss_result_file,\n",
        "            lr=self.config['learning_rate'],\n",
        "            num_epochs=self.config['num_epochs'],\n",
        "            device=self.device\n",
        "        )\n",
        "        trainer.execute(self.train_loader, self.valid_loader)\n",
        "\n",
        "    def test(self):\n",
        "        # Create model and load checkpoint for testing\n",
        "        model = UNet(\n",
        "              spatial_dims=3,\n",
        "              in_channels=1,\n",
        "              out_channels=1,\n",
        "              channels=(32, 64, 128, 256, 512),  # Increased base channels\n",
        "              strides=(2, 2, 2, 2),\n",
        "              num_res_units=4,  # Increased residual units\n",
        "              norm=Norm.INSTANCE,  # Instance norm often works better than batch norm for medical images\n",
        "              dropout=0.1,  # Add dropout for regularization\n",
        "              act='LEAKYRELU'  # LeakyReLU for better gradient flow\n",
        "          ).to(self.device)\n",
        "\n",
        "\n",
        "\n",
        "        if os.path.exists(self.model_file):\n",
        "            checkpoint = torch.load(self.model_file, map_location=self.device)\n",
        "            if checkpoint.get('model_state_dict') is not None:\n",
        "                model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            else:\n",
        "                model.load_state_dict(checkpoint)\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"Model file not found: {self.model_file}\")\n",
        "\n",
        "        # tester = None  # your UnetTest implementation can be used here\n",
        "        tester = UnetTest(self.test_result_path, self.test_metrics_file, self.device)\n",
        "        tester.test(model, self.test_loader)\n",
        "\n",
        "    def run(self):\n",
        "        self.train()\n",
        "        self.test()\n",
        "\n",
        "\n",
        "# ---------- Run ----------\n",
        "def main():\n",
        "    import multiprocessing as mp\n",
        "    mp.set_start_method('spawn', force=True)\n",
        "\n",
        "    config = {\n",
        "        'target_dir': \"/content/drive/MyDrive/PhDwork/Segmentation\",\n",
        "        'output_folder_name': \"Results_Nifti_MONAI6_Original\",\n",
        "        'transformation': \"OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\",\n",
        "        'batch_size': 1,   # effective patches per step = num_samples * batch_size (we set num_samples=1)\n",
        "        'num_epochs': 200,\n",
        "        'learning_rate': 1e-4,\n",
        "    }\n",
        "\n",
        "    pipeline = UnetPipeline(config)\n",
        "    pipeline.run()   # uncomment to actually train/test\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#(8) Mask Generation"
      ],
      "metadata": {
        "id": "hSP35kUBOaDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from monai.networks.nets import UNet\n",
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    Resized,\n",
        "    CopyItemsd,\n",
        "    Invertd,\n",
        "    LoadImaged,\n",
        "    EnsureChannelFirstd,\n",
        "    Spacingd,\n",
        "    Orientationd,\n",
        "    ScaleIntensityRanged,\n",
        "    CropForegroundd,\n",
        "    EnsureTyped,\n",
        "    SaveImaged,\n",
        "    ToTensord,\n",
        ")\n",
        "from monai.data import Dataset, DataLoader, decollate_batch\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.utils import set_determinism\n",
        "from monai.networks.layers import Norm\n",
        "# from monai.transforms.utils import SaveTransform\n",
        "\n",
        "\n",
        "\n",
        "class UNetInferencePipeline:\n",
        "    def __init__(self, model_path, input_ct_dir, input_seg_dir, output_dir, device=\"cuda:0\"):\n",
        "        self.device = device if torch.cuda.is_available() else \"cpu\"\n",
        "        self.input_ct_dir = input_ct_dir\n",
        "        self.input_seg_dir = input_seg_dir\n",
        "        self.output_dir = output_dir\n",
        "        self.ct_out_dir = os.path.join(output_dir, \"ct\")\n",
        "        self.seg_out_dir = os.path.join(output_dir, \"segment\")\n",
        "        os.makedirs(self.ct_out_dir, exist_ok=True)\n",
        "        os.makedirs(self.seg_out_dir, exist_ok=True)\n",
        "        self.model_path = model_path\n",
        "        self.model = self._load_model()\n",
        "        set_determinism(seed=42)\n",
        "        self.forward_transforms = self._get_forward_transforms()\n",
        "        self.inverse_transforms = None\n",
        "        self.dataloader = self._prepare_dataloader()\n",
        "\n",
        "    def _load_model(self):\n",
        "        if not os.path.exists(self.model_path):\n",
        "            raise FileNotFoundError(f\"Model file not found at: {self.model_path}\")\n",
        "\n",
        "        model = UNet(\n",
        "            spatial_dims=3,\n",
        "            in_channels=1,\n",
        "            out_channels=1,\n",
        "            channels=(16, 32, 64, 128, 256),\n",
        "            strides=(2, 2, 2, 2),\n",
        "            num_res_units=2,\n",
        "            norm=Norm.BATCH\n",
        "        ).to(self.device)\n",
        "\n",
        "        state_dict = torch.load(self.model_path, map_location=self.device)\n",
        "        model.load_state_dict(state_dict.get('model_state_dict', state_dict))\n",
        "\n",
        "        print(f\"‚úÖ Model loaded successfully from {self.model_path}\")\n",
        "        return model\n",
        "\n",
        "\n",
        "\n",
        "    def _get_forward_transforms(self):\n",
        "        return Compose([\n",
        "            LoadImaged(keys=[\"vol\"]),\n",
        "            EnsureChannelFirstd(keys=[\"vol\"]),\n",
        "            CopyItemsd(keys=[\"vol\"], names=[\"vol_meta_dict\"]),\n",
        "            Spacingd(keys=[\"vol\"], pixdim=(1.0, 1.0, 1.0), mode=\"bilinear\"),\n",
        "            Orientationd(keys=[\"vol\"], axcodes=\"RAS\"),\n",
        "            ScaleIntensityRanged(keys=[\"vol\"], a_min=-1000, a_max=700, b_min=0.0, b_max=1.0, clip=True),\n",
        "            CropForegroundd(keys=[\"vol\"], source_key=\"vol\"),\n",
        "            Resized(keys=[\"vol\"], spatial_size=(96, 96, 96)),\n",
        "            EnsureTyped(keys=[\"vol\"]),\n",
        "        ])\n",
        "\n",
        "    def _get_inverse_transforms(self):\n",
        "        return Compose([\n",
        "            Invertd(\n",
        "                keys=[\"seg\"],\n",
        "                transform=self.forward_transforms,\n",
        "                orig_keys=[\"vol\"],\n",
        "                meta_keys=[\"vol_meta_dict\"],\n",
        "                nearest_interp=True,\n",
        "                to_tensor=False,\n",
        "            ),\n",
        "            EnsureTyped(keys=[\"seg\"])\n",
        "        ])\n",
        "\n",
        "    def _prepare_dataloader(self):\n",
        "        data = []\n",
        "        for f in os.listdir(self.input_ct_dir):\n",
        "            if f.endswith(('.nii', '.nii.gz')):\n",
        "                ct_path = os.path.join(self.input_ct_dir, f)\n",
        "                data.append({\"vol\": ct_path})\n",
        "        print(f\"üîç Found {len(data)} NIfTI files for inference.\")\n",
        "        return DataLoader(Dataset(data=data, transform=self.forward_transforms), batch_size=1, num_workers=0)\n",
        "\n",
        "    def infer(self):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(self.dataloader):\n",
        "                batch = decollate_batch(batch)[0]\n",
        "                vol_meta = batch[\"vol_meta_dict\"]\n",
        "                ct = batch[\"vol\"]\n",
        "\n",
        "                if ct.dim() == 4:\n",
        "                    ct = ct.unsqueeze(0)\n",
        "                ct = ct.to(self.device)\n",
        "\n",
        "                filename = os.path.basename(vol_meta.meta[\"filename_or_obj\"])\n",
        "                orig_vol = nib.load(vol_meta.meta[\"filename_or_obj\"]).get_fdata()\n",
        "                print(f\"üîç Inference on [{i+1}] {filename} | shape = {ct.shape}\")\n",
        "                print(f\"üîç Original volume shape = {orig_vol.shape}\")\n",
        "                pred = self.model(ct)\n",
        "                pred = (torch.sigmoid(pred) > 0.5).float()\n",
        "\n",
        "                print(f\"‚úÖ Predicted mask shape: {pred.shape}\")\n",
        "\n",
        "                batch[\"seg\"] = pred.cpu().squeeze(0)\n",
        "                print(f\"‚úÖ Batch shape: {batch['seg'].shape}\")\n",
        "\n",
        "                if self.inverse_transforms is None:\n",
        "                    self.inverse_transforms = self._get_inverse_transforms()\n",
        "\n",
        "                inverted = self.inverse_transforms(batch)\n",
        "                inv_seg = inverted[\"seg\"].squeeze(0).numpy()\n",
        "                inv_seg = (inv_seg > 0.5).astype(np.uint8)\n",
        "                print(f\"‚úÖ Inverted mask shape: {inv_seg.shape}\")\n",
        "\n",
        "                self._save_nifti(inv_seg, vol_meta, self.seg_out_dir, filename, is_segmentation=True)\n",
        "\n",
        "\n",
        "    def _save_nifti(self, array, meta_tensor, out_dir, filename, is_segmentation=False):\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "        affine = meta_tensor.meta.get(\"original_affine\", meta_tensor.meta.get(\"affine\", np.eye(4)))\n",
        "        dtype = np.uint8 if is_segmentation else np.float32\n",
        "        nib_img = nib.Nifti1Image(array.astype(dtype), affine)\n",
        "        nib.save(nib_img, os.path.join(out_dir, filename))\n",
        "        print(f\"‚úÖ Saved: {os.path.join(out_dir, filename)}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ROOT_DIR = \"/content/drive/MyDrive/PhDwork/Segmentation\"\n",
        "    MODEL_PATH = os.path.join(ROOT_DIR, \"results\", \"Results_MONAI_Augmented\", \"model.pth\")\n",
        "    INPUT_CT_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"Lung3\", \"ct\")\n",
        "    INPUT_SEG_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"Lung3\", \"segment\")\n",
        "    OUTPUT_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"Lung3_Predicted\")\n",
        "\n",
        "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "    os.chdir(ROOT_DIR)\n",
        "\n",
        "    try:\n",
        "        pipeline = UNetInferencePipeline(MODEL_PATH, INPUT_CT_FOLDER, INPUT_SEG_FOLDER, OUTPUT_FOLDER)\n",
        "        pipeline.infer()\n",
        "        print(\"üéâ Inference completed successfully for all patients!\")\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "z6a0G1DXTIV8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81a85b9f-c61a-4fd8-b379-3bd00e32c605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model loaded successfully from /content/drive/MyDrive/PhDwork/Segmentation/results/Results_MONAI_Augmented/model.pth\n",
            "üîç Found 89 NIfTI files for inference.\n",
            "üîç Inference on [1] LUNG3-01.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (59, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (59, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-01.nii.gz\n",
            "üîç Inference on [2] LUNG3-02.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (57, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (57, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-02.nii.gz\n",
            "üîç Inference on [3] LUNG3-03.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (61, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (61, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-03.nii.gz\n",
            "üîç Inference on [4] LUNG3-04.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (61, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (61, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-04.nii.gz\n",
            "üîç Inference on [5] LUNG3-05.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (229, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (229, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-05.nii.gz\n",
            "üîç Inference on [6] LUNG3-06.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (61, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (61, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-06.nii.gz\n",
            "üîç Inference on [7] LUNG3-07.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (86, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (86, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-07.nii.gz\n",
            "üîç Inference on [8] LUNG3-08.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (158, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (158, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-08.nii.gz\n",
            "üîç Inference on [9] LUNG3-09.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (140, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (140, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-09.nii.gz\n",
            "üîç Inference on [10] LUNG3-10.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (95, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (95, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-10.nii.gz\n",
            "üîç Inference on [11] LUNG3-11.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (252, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (252, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-11.nii.gz\n",
            "üîç Inference on [12] LUNG3-12.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (206, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (206, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-12.nii.gz\n",
            "üîç Inference on [13] LUNG3-13.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (71, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (71, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-13.nii.gz\n",
            "üîç Inference on [14] LUNG3-14.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (325, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (325, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-14.nii.gz\n",
            "üîç Inference on [15] LUNG3-15.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (234, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (234, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-15.nii.gz\n",
            "üîç Inference on [16] LUNG3-16.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (192, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (192, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-16.nii.gz\n",
            "üîç Inference on [17] LUNG3-17.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (226, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (226, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-17.nii.gz\n",
            "üîç Inference on [18] LUNG3-18.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (178, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (178, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-18.nii.gz\n",
            "üîç Inference on [19] LUNG3-19.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (178, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (178, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-19.nii.gz\n",
            "üîç Inference on [20] LUNG3-20.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (176, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (176, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-20.nii.gz\n",
            "üîç Inference on [21] LUNG3-21.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (74, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (74, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-21.nii.gz\n",
            "üîç Inference on [22] LUNG3-22.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (236, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (236, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-22.nii.gz\n",
            "üîç Inference on [23] LUNG3-23.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (52, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (52, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-23.nii.gz\n",
            "üîç Inference on [24] LUNG3-24.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-24.nii.gz\n",
            "üîç Inference on [25] LUNG3-25.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (202, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (202, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-25.nii.gz\n",
            "üîç Inference on [26] LUNG3-26.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (83, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (83, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-26.nii.gz\n",
            "üîç Inference on [27] LUNG3-27.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (149, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (149, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-27.nii.gz\n",
            "üîç Inference on [28] LUNG3-28.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (86, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (86, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-28.nii.gz\n",
            "üîç Inference on [29] LUNG3-29.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (173, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (173, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-29.nii.gz\n",
            "üîç Inference on [30] LUNG3-30.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (72, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (72, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-30.nii.gz\n",
            "üîç Inference on [31] LUNG3-31.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (242, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (242, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-31.nii.gz\n",
            "üîç Inference on [32] LUNG3-32.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (58, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (58, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-32.nii.gz\n",
            "üîç Inference on [33] LUNG3-33.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (276, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (276, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-33.nii.gz\n",
            "üîç Inference on [34] LUNG3-34.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (255, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (255, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-34.nii.gz\n",
            "üîç Inference on [35] LUNG3-35.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (253, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (253, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-35.nii.gz\n",
            "üîç Inference on [36] LUNG3-36.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (356, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (356, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-36.nii.gz\n",
            "üîç Inference on [37] LUNG3-37.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (97, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (97, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-37.nii.gz\n",
            "üîç Inference on [38] LUNG3-38.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (223, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (223, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-38.nii.gz\n",
            "üîç Inference on [39] LUNG3-39.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (82, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (82, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-39.nii.gz\n",
            "üîç Inference on [40] LUNG3-40.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (239, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (239, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-40.nii.gz\n",
            "üîç Inference on [41] LUNG3-41.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (110, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (110, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-41.nii.gz\n",
            "üîç Inference on [42] LUNG3-42.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (255, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (255, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-42.nii.gz\n",
            "üîç Inference on [43] LUNG3-43.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (68, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (68, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-43.nii.gz\n",
            "üîç Inference on [44] LUNG3-44.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (227, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (227, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-44.nii.gz\n",
            "üîç Inference on [45] LUNG3-45.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (178, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (178, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-45.nii.gz\n",
            "üîç Inference on [46] LUNG3-46.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (184, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (184, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-46.nii.gz\n",
            "üîç Inference on [47] LUNG3-47.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (275, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (275, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-47.nii.gz\n",
            "üîç Inference on [48] LUNG3-48.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (157, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (157, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-48.nii.gz\n",
            "üîç Inference on [49] LUNG3-49.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (89, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (89, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-49.nii.gz\n",
            "üîç Inference on [50] LUNG3-50.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (255, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (255, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-50.nii.gz\n",
            "üîç Inference on [51] LUNG3-51.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (76, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (76, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-51.nii.gz\n",
            "üîç Inference on [52] LUNG3-52.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (50, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (50, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-52.nii.gz\n",
            "üîç Inference on [53] LUNG3-53.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (255, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (255, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-53.nii.gz\n",
            "üîç Inference on [54] LUNG3-54.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (175, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (175, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-54.nii.gz\n",
            "üîç Inference on [55] LUNG3-55.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (72, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (72, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-55.nii.gz\n",
            "üîç Inference on [56] LUNG3-56.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (178, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (178, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-56.nii.gz\n",
            "üîç Inference on [57] LUNG3-57.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (178, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (178, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-57.nii.gz\n",
            "üîç Inference on [58] LUNG3-58.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (64, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (64, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-58.nii.gz\n",
            "üîç Inference on [59] LUNG3-59.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (62, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (62, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-59.nii.gz\n",
            "üîç Inference on [60] LUNG3-60.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (92, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (92, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-60.nii.gz\n",
            "üîç Inference on [61] LUNG3-61.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (203, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (203, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-61.nii.gz\n",
            "üîç Inference on [62] LUNG3-62.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (66, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (66, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-62.nii.gz\n",
            "üîç Inference on [63] LUNG3-63.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (57, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (57, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-63.nii.gz\n",
            "üîç Inference on [64] LUNG3-64.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (172, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (172, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-64.nii.gz\n",
            "üîç Inference on [65] LUNG3-65.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (175, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (175, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-65.nii.gz\n",
            "üîç Inference on [66] LUNG3-66.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (69, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (69, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-66.nii.gz\n",
            "üîç Inference on [67] LUNG3-67.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (74, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (74, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-67.nii.gz\n",
            "üîç Inference on [68] LUNG3-68.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (60, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (60, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-68.nii.gz\n",
            "üîç Inference on [69] LUNG3-69.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (158, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (158, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-69.nii.gz\n",
            "üîç Inference on [70] LUNG3-70.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (258, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (258, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-70.nii.gz\n",
            "üîç Inference on [71] LUNG3-71.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (287, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (287, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-71.nii.gz\n",
            "üîç Inference on [72] LUNG3-72.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (84, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (84, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-72.nii.gz\n",
            "üîç Inference on [73] LUNG3-73.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (218, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (218, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-73.nii.gz\n",
            "üîç Inference on [74] LUNG3-74.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (67, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (67, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-74.nii.gz\n",
            "üîç Inference on [75] LUNG3-75.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (88, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (88, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-75.nii.gz\n",
            "üîç Inference on [76] LUNG3-76.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (74, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (74, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-76.nii.gz\n",
            "üîç Inference on [77] LUNG3-77.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (99, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (99, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-77.nii.gz\n",
            "üîç Inference on [78] LUNG3-78.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (255, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (255, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-78.nii.gz\n",
            "üîç Inference on [79] LUNG3-79.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (240, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (240, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-79.nii.gz\n",
            "üîç Inference on [80] LUNG3-80.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (59, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (59, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-80.nii.gz\n",
            "üîç Inference on [81] LUNG3-81.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (307, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (307, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-81.nii.gz\n",
            "üîç Inference on [82] LUNG3-82.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (78, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (78, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-82.nii.gz\n",
            "üîç Inference on [83] LUNG3-83.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (178, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (178, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-83.nii.gz\n",
            "üîç Inference on [84] LUNG3-84.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (66, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (66, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-84.nii.gz\n",
            "üîç Inference on [85] LUNG3-85.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (234, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (234, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-85.nii.gz\n",
            "üîç Inference on [86] LUNG3-86.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (78, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (78, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-86.nii.gz\n",
            "üîç Inference on [87] LUNG3-87.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (79, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (79, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-87.nii.gz\n",
            "üîç Inference on [88] LUNG3-88.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (154, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (154, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-88.nii.gz\n",
            "üîç Inference on [89] LUNG3-89.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (158, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (158, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-89.nii.gz\n",
            "üéâ Inference completed successfully for all patients!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    ROOT_DIR = \"/content/drive/MyDrive/PhDwork/Segmentation\"\n",
        "    MODEL_PATH = os.path.join(ROOT_DIR, \"results\", \"Results_MONAI_Augmented\", \"model.pth\")\n",
        "    INPUT_CT_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"test\", \"ct\")\n",
        "    INPUT_SEG_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"test\", \"segment\")\n",
        "    OUTPUT_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"test_Predicted\")\n",
        "\n",
        "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "    os.chdir(ROOT_DIR)\n",
        "\n",
        "    try:\n",
        "        pipeline = UNetInferencePipeline(MODEL_PATH, INPUT_CT_FOLDER, INPUT_SEG_FOLDER, OUTPUT_FOLDER)\n",
        "        pipeline.infer()\n",
        "        print(\"üéâ Inference completed successfully for all patients!\")\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "id": "Ckpn4rjZJ9fn",
        "outputId": "b54762e0-6093-45f5-86b6-48c278f39d56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model loaded successfully from /content/drive/MyDrive/PhDwork/Segmentation/results/Results_MONAI_Augmented/model.pth\n",
            "üîç Found 38 NIfTI files for inference.\n",
            "üîç Inference on [1] LUNG1-001.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-001.nii.gz\n",
            "üîç Inference on [2] LUNG1-025.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (106, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (106, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-025.nii.gz\n",
            "üîç Inference on [3] LUNG1-027.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (108, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (108, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-027.nii.gz\n",
            "üîç Inference on [4] LUNG1-034.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (95, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (95, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-034.nii.gz\n",
            "üîç Inference on [5] LUNG1-039.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (95, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (95, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-039.nii.gz\n",
            "üîç Inference on [6] LUNG1-066.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (92, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (92, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-066.nii.gz\n",
            "üîç Inference on [7] LUNG1-078.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (136, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (136, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-078.nii.gz\n",
            "üîç Inference on [8] LUNG1-088.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (123, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (123, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-088.nii.gz\n",
            "üîç Inference on [9] LUNG1-107.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (116, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (116, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-107.nii.gz\n",
            "üîç Inference on [10] LUNG1-132.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (114, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (114, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-132.nii.gz\n",
            "üîç Inference on [11] LUNG1-133.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (184, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (184, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-133.nii.gz\n",
            "üîç Inference on [12] LUNG1-143.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-143.nii.gz\n",
            "üîç Inference on [13] LUNG1-149.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (118, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (118, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-149.nii.gz\n",
            "üîç Inference on [14] LUNG1-151.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (118, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (118, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-151.nii.gz\n",
            "üîç Inference on [15] LUNG1-158.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (115, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (115, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-158.nii.gz\n",
            "üîç Inference on [16] LUNG1-168.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-168.nii.gz\n",
            "üîç Inference on [17] LUNG1-175.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (112, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (112, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-175.nii.gz\n",
            "üîç Inference on [18] LUNG1-176.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (106, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (106, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-176.nii.gz\n",
            "üîç Inference on [19] LUNG1-201.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-201.nii.gz\n",
            "üîç Inference on [20] LUNG1-224.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (93, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (93, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-224.nii.gz\n",
            "üîç Inference on [21] LUNG1-225.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-225.nii.gz\n",
            "üîç Inference on [22] LUNG1-235.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (129, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (129, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-235.nii.gz\n",
            "üîç Inference on [23] LUNG1-239.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-239.nii.gz\n",
            "üîç Inference on [24] LUNG1-246.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (115, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (115, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-246.nii.gz\n",
            "üîç Inference on [25] LUNG1-263.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-263.nii.gz\n",
            "üîç Inference on [26] LUNG1-266.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (94, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (94, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-266.nii.gz\n",
            "üîç Inference on [27] LUNG1-281.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (101, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (101, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-281.nii.gz\n",
            "üîç Inference on [28] LUNG1-286.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (136, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (136, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-286.nii.gz\n",
            "üîç Inference on [29] LUNG1-312.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-312.nii.gz\n",
            "üîç Inference on [30] LUNG1-338.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-338.nii.gz\n",
            "üîç Inference on [31] LUNG1-352.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-352.nii.gz\n",
            "üîç Inference on [32] LUNG1-353.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-353.nii.gz\n",
            "üîç Inference on [33] LUNG1-365.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-365.nii.gz\n",
            "üîç Inference on [34] LUNG1-374.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (130, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (130, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-374.nii.gz\n",
            "üîç Inference on [35] LUNG1-383.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-383.nii.gz\n",
            "üîç Inference on [36] LUNG1-405.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-405.nii.gz\n",
            "üîç Inference on [37] LUNG1-408.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (107, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (107, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-408.nii.gz\n",
            "üîç Inference on [38] LUNG1-410.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-410.nii.gz\n",
            "üéâ Inference completed successfully for all patients!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    ROOT_DIR = \"/content/drive/MyDrive/PhDwork/Segmentation\"\n",
        "    MODEL_PATH = os.path.join(ROOT_DIR, \"results\", \"Results_MONAI_Augmented\", \"model.pth\")\n",
        "    INPUT_CT_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"valid\", \"ct\")\n",
        "    INPUT_SEG_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"valid\", \"segment\")\n",
        "    OUTPUT_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"valid_Predicted\")\n",
        "\n",
        "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "    os.chdir(ROOT_DIR)\n",
        "\n",
        "    try:\n",
        "        pipeline = UNetInferencePipeline(MODEL_PATH, INPUT_CT_FOLDER, INPUT_SEG_FOLDER, OUTPUT_FOLDER)\n",
        "        pipeline.infer()\n",
        "        print(\"üéâ Inference completed successfully for all patients!\")\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcIi6GZ347x8",
        "outputId": "5b5fc90e-eeaf-4b07-d6a5-bf3197b8e4a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model loaded successfully from /content/drive/MyDrive/PhDwork/Segmentation/results/Results_MONAI_Augmented/model.pth\n",
            "üîç Found 43 NIfTI files for inference.\n",
            "üîç Inference on [1] LUNG1-010.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (91, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (91, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-010.nii.gz\n",
            "üîç Inference on [2] LUNG1-031.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (153, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (153, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-031.nii.gz\n",
            "üîç Inference on [3] LUNG1-040.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (95, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (95, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-040.nii.gz\n",
            "üîç Inference on [4] LUNG1-056.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-056.nii.gz\n",
            "üîç Inference on [5] LUNG1-057.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (101, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (101, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-057.nii.gz\n",
            "üîç Inference on [6] LUNG1-071.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (135, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (135, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-071.nii.gz\n",
            "üîç Inference on [7] LUNG1-073.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (176, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (176, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-073.nii.gz\n",
            "üîç Inference on [8] LUNG1-074.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (115, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (115, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-074.nii.gz\n",
            "üîç Inference on [9] LUNG1-076.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (92, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (92, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-076.nii.gz\n",
            "üîç Inference on [10] LUNG1-077.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (117, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (117, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-077.nii.gz\n",
            "üîç Inference on [11] LUNG1-080.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (99, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (99, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-080.nii.gz\n",
            "üîç Inference on [12] LUNG1-091.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (135, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (135, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-091.nii.gz\n",
            "üîç Inference on [13] LUNG1-095.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (106, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (106, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-095.nii.gz\n",
            "üîç Inference on [14] LUNG1-117.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (90, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (90, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-117.nii.gz\n",
            "üîç Inference on [15] LUNG1-134.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (108, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (108, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-134.nii.gz\n",
            "üîç Inference on [16] LUNG1-139.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (107, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (107, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-139.nii.gz\n",
            "üîç Inference on [17] LUNG1-147.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (99, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (99, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-147.nii.gz\n",
            "üîç Inference on [18] LUNG1-170.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (110, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (110, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-170.nii.gz\n",
            "üîç Inference on [19] LUNG1-177.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (94, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (94, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-177.nii.gz\n",
            "üîç Inference on [20] LUNG1-186.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-186.nii.gz\n",
            "üîç Inference on [21] LUNG1-194.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (127, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (127, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-194.nii.gz\n",
            "üîç Inference on [22] LUNG1-196.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (94, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (94, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-196.nii.gz\n",
            "üîç Inference on [23] LUNG1-198.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (131, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (131, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-198.nii.gz\n",
            "üîç Inference on [24] LUNG1-210.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (131, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (131, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-210.nii.gz\n",
            "üîç Inference on [25] LUNG1-220.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (94, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (94, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-220.nii.gz\n",
            "üîç Inference on [26] LUNG1-230.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (93, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (93, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-230.nii.gz\n",
            "üîç Inference on [27] LUNG1-233.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-233.nii.gz\n",
            "üîç Inference on [28] LUNG1-241.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (136, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (136, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-241.nii.gz\n",
            "üîç Inference on [29] LUNG1-249.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (93, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (93, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-249.nii.gz\n",
            "üîç Inference on [30] LUNG1-264.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (130, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (130, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-264.nii.gz\n",
            "üîç Inference on [31] LUNG1-273.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (136, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (136, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-273.nii.gz\n",
            "üîç Inference on [32] LUNG1-299.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (93, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (93, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-299.nii.gz\n",
            "üîç Inference on [33] LUNG1-329.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-329.nii.gz\n",
            "üîç Inference on [34] LUNG1-337.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-337.nii.gz\n",
            "üîç Inference on [35] LUNG1-340.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (92, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (92, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-340.nii.gz\n",
            "üîç Inference on [36] LUNG1-356.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-356.nii.gz\n",
            "üîç Inference on [37] LUNG1-371.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (173, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (173, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-371.nii.gz\n",
            "üîç Inference on [38] LUNG1-372.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-372.nii.gz\n",
            "üîç Inference on [39] LUNG1-412.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-412.nii.gz\n",
            "üîç Inference on [40] LUNG1-415.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (122, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (122, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-415.nii.gz\n",
            "üîç Inference on [41] LUNG1-418.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (133, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (133, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-418.nii.gz\n",
            "üîç Inference on [42] LUNG1-419.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-419.nii.gz\n",
            "üîç Inference on [43] LUNG1-421.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "üîç Original volume shape = (134, 512, 512)\n",
            "‚úÖ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "‚úÖ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "‚úÖ Inverted mask shape: (134, 512, 512)\n",
            "‚úÖ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-421.nii.gz\n",
            "üéâ Inference completed successfully for all patients!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKImglO4-twL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "import os\n",
        "import csv\n",
        "\n",
        "\n",
        "class LossPlotter:\n",
        "    def __init__(self, csv_path: str):\n",
        "        self.csv_path = Path(csv_path)\n",
        "        self.data = self._load_data()\n",
        "\n",
        "    def _load_data(self):\n",
        "        if not self.csv_path.exists():\n",
        "            raise FileNotFoundError(f\"CSV file not found: {self.csv_path}\")\n",
        "        df = pd.read_csv(self.csv_path, index_col=0)  # Read row labels as index\n",
        "        return df  # Make rows into columns\n",
        "\n",
        "    def plot(self, title: str = \"Training and Validation Loss\", save_path= None):\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.plot(self.data.index, self.data['Train Loss'], label='Train Loss', color='blue')\n",
        "        plt.plot(self.data.index, self.data['Valid Loss'], label='Valid Loss', color='orange')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title(title)\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            save_path = Path(save_path)\n",
        "            save_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            plt.savefig(save_path, format='pdf')\n",
        "            print(f\"[INFO] Loss plot saved to {save_path}\")\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "        plt.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    target_dir = \"/content/drive/MyDrive/PhDwork/Segmentation\"\n",
        "    os.chdir(target_dir)\n",
        "    loss_result_file = os.path.join(\".\",\"results\",f\"Results_PreProcessedCT_Fifty_Fifty_DiceLoss_And_Strong_Augmentation\",\"train_and_valid_loss_results.csv\")\n",
        "    plotter = LossPlotter(loss_result_file)\n",
        "    plotter.plot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyeB21BYGQPu"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "os.chdir(\"/content/drive/MyDrive/PhDwork/Segmentation\")\n",
        "print(f\"üìÅ Current Directory: {os.getcwd()}\")\n",
        "with h5py.File('./datasets/Datasets_PreprocessedCT_clipping_uniformSpacing_With_Empty_NonEmpty_slices_In_Train/train_dataset.hdf5', 'r') as f:\n",
        "    print(list(f.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ud1cFDGmKQBK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "authorship_tag": "ABX9TyNcZkZMRIyDWa0qyzosEZhN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}