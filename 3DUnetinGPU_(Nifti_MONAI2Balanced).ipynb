{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sajidcsecu/radioGenomic/blob/main/3DUnetinGPU_(Nifti_MONAI2Balanced).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr2QqHlZ8GZB"
      },
      "source": [
        "# This is the Code for the Segmentation on Rider Dataset (LUNG1). The Code is worked on the 3D volume over GPU. The balanced sampler, preprocessed data (uniform volume spacing and clipping [-1000, 700]) and the strong augmentation is used in the code..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9Zo7tkcI1CX"
      },
      "source": [
        "# (1) Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d9OVdEeKXpMe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7daea1f9-87d5-447e-f3a6-32393f82fbc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.12/dist-packages (2.5.2)\n",
            "Requirement already satisfied: pydicom===2.4.3 in /usr/local/lib/python3.12/dist-packages (2.4.3)\n",
            "Requirement already satisfied: pydicom-seg in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: SimpleITK>1.2.4 in /usr/local/lib/python3.12/dist-packages (from pydicom-seg) (2.5.2)\n",
            "Requirement already satisfied: jsonschema<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from pydicom-seg) (3.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from pydicom-seg) (1.26.4)\n",
            "Requirement already satisfied: pydicom>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from pydicom-seg) (2.4.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (25.3.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (0.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (75.2.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (1.17.0)\n",
            "Collecting numpy==1.23.5\n",
            "  Using cached numpy-1.23.5.tar.gz (10.7 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "Collecting monai\n",
            "  Downloading monai-1.5.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.24 in /usr/local/lib/python3.12/dist-packages (from monai) (1.26.4)\n",
            "Requirement already satisfied: torch>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from monai) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.1->monai) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.1->monai) (3.0.2)\n",
            "Downloading monai-1.5.1-py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: monai\n",
            "Successfully installed monai-1.5.1\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.13.1 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0, 2.7.1, 2.8.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.13.1\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install SimpleITK\n",
        "!pip install pydicom===2.4.3\n",
        "!pip install pydicom-seg\n",
        "!pip install numpy==1.23.5\n",
        "!pip install monai\n",
        "!pip install torch==1.13.1\n",
        "!pip install nibabel>=5.0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JadHvjQcJ-qU"
      },
      "source": [
        "\n",
        "# (2) Import required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pmtDNjxMbfB4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import csv\n",
        "import time\n",
        "import random\n",
        "import shutil\n",
        "from glob import glob\n",
        "from typing import List\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.cuda.amp as amp\n",
        "from torch.optim import lr_scheduler\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.transforms import AsDiscrete\n",
        "from monai.networks.nets import UNet\n",
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    LoadImaged,\n",
        "    EnsureChannelFirstd,\n",
        "    Spacingd,\n",
        "    Orientationd,\n",
        "    ScaleIntensityRanged,\n",
        "    CropForegroundd,\n",
        "    ResizeWithPadOrCropd,\n",
        "    RandCropByPosNegLabeld,\n",
        "    RandFlipd,\n",
        "    RandAffined,\n",
        "    RandGaussianNoised,\n",
        "    RandScaleIntensityd,\n",
        "    ToTensord,\n",
        "    EnsureTyped,\n",
        "    EnsureChannelFirstD,\n",
        "    SpatialPadd\n",
        ")\n",
        "from monai.data import Dataset, DataLoader, CacheDataset, pad_list_data_collate\n",
        "from monai.networks.layers import Norm\n",
        "import nibabel as nib\n",
        "from sklearn.metrics import jaccard_score, f1_score, recall_score, precision_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import multiprocessing as mp\n",
        "from monai.transforms import EnsureTyped\n",
        "from monai.transforms import SaveImaged\n",
        "from monai.utils import set_determinism\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"Num foregrounds 0, Num backgrounds.*unable to generate class balanced samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyzguRDWI9bM"
      },
      "source": [
        "# (3) Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq6jVaaMXZz5",
        "outputId": "dd15ab84-394e-4515-c630-c88a1b8773fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cGu3zWdW3jje"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFrRJqgG7wxo"
      },
      "source": [
        "## (4). Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "944_4uJbmPPx"
      },
      "outputs": [],
      "source": [
        "# ---------- Combined Dice + BCE loss for 3D segmentation ----------\n",
        "class DiceBCELoss3D(nn.Module):\n",
        "    def __init__(self, smooth=1e-6, epsilon=1e-8, dice_w=1.0, bce_w=1.0):\n",
        "        super().__init__()\n",
        "        self.smooth = smooth\n",
        "        self.epsilon = epsilon\n",
        "        self.dice_w = dice_w\n",
        "        self.bce_w = bce_w\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def set_pos_weight(self, pos_weight=None, device=\"cpu\"):\n",
        "        if pos_weight is None:\n",
        "            self.bce = nn.BCEWithLogitsLoss()\n",
        "        else:\n",
        "            tensor_pw = torch.tensor([pos_weight], dtype=torch.float, device=device)\n",
        "            self.bce = nn.BCEWithLogitsLoss(pos_weight=tensor_pw)\n",
        "        return self\n",
        "\n",
        "    def forward(self, preds, targets):\n",
        "        preds_sigmoid = torch.sigmoid(preds)\n",
        "\n",
        "        inter = (preds_sigmoid * targets).sum()\n",
        "        dice_loss = 1 - (2. * inter + self.smooth) / (\n",
        "            preds_sigmoid.sum() + targets.sum() + self.smooth + self.epsilon\n",
        "        )\n",
        "\n",
        "        bce_loss = self.bce(preds, targets)\n",
        "        return self.dice_w * dice_loss + self.bce_w * bce_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZZ3Gu-DD88X"
      },
      "source": [
        "# (5). Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "T4VsKIzmFGLP"
      },
      "outputs": [],
      "source": [
        "class UnetTest:\n",
        "    def __init__(self, test_result_path: str, metrics_csv: str, device: torch.device):\n",
        "        self.test_result_path = test_result_path\n",
        "        self.metrics_csv = metrics_csv\n",
        "        self.device = device\n",
        "\n",
        "        os.makedirs(self.test_result_path, exist_ok=True)\n",
        "        self._init_metrics_csv()\n",
        "\n",
        "    def _init_metrics_csv(self):\n",
        "        if not os.path.exists(self.metrics_csv):\n",
        "            with open(self.metrics_csv, 'w', newline='') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([\"SampleID\", \"Jaccard\", \"F1\", \"Recall\", \"Precision\", \"Accuracy\", \"Time\"])\n",
        "\n",
        "    def calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray):\n",
        "        y_true = y_true.astype(bool).flatten()\n",
        "        y_pred = y_pred.astype(bool).flatten()\n",
        "\n",
        "        return [\n",
        "            jaccard_score(y_true, y_pred, zero_division=0),\n",
        "            f1_score(y_true, y_pred, zero_division=0),\n",
        "            recall_score(y_true, y_pred, zero_division=0),\n",
        "            precision_score(y_true, y_pred, zero_division=0),\n",
        "            accuracy_score(y_true, y_pred)\n",
        "        ]\n",
        "\n",
        "    def save_result_slices(self, image: np.ndarray, pred_mask: np.ndarray, true_mask: np.ndarray, sample_id: str):\n",
        "        sample_dir = os.path.join(self.test_result_path, sample_id)\n",
        "        os.makedirs(sample_dir, exist_ok=True)\n",
        "\n",
        "        for i in range(image.shape[0]):\n",
        "            try:\n",
        "                fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
        "                ax[0].imshow(image[i], cmap='gray')\n",
        "                ax[0].set_title('Image')\n",
        "\n",
        "                ax[1].imshow(true_mask[i], cmap='gray')\n",
        "                ax[1].set_title('Ground Truth')\n",
        "\n",
        "                ax[2].imshow(pred_mask[i], cmap='gray')\n",
        "                ax[2].set_title('Prediction')\n",
        "\n",
        "                for a in ax:\n",
        "                    a.axis('off')\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(sample_dir, f'slice_{i:03d}.png'))\n",
        "                plt.close()\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Could not save slice {i} for {sample_id}: {e}\")\n",
        "\n",
        "    def append_metrics_to_csv(self, sample_id: str, metrics: list, elapsed_time: float):\n",
        "        with open(self.metrics_csv, 'a', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([sample_id] + [f\"{m:.4f}\" for m in metrics] + [f\"{elapsed_time:.4f}\"])\n",
        "\n",
        "    def test(self, model: nn.Module, test_loader: DataLoader):\n",
        "        model.eval()\n",
        "        total_metrics = np.zeros(5)\n",
        "        total_times = []\n",
        "\n",
        "        roi_size = (96, 96, 96)\n",
        "        sw_batch_size = 1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(test_loader):\n",
        "                image, label = batch[\"vol\"].to(self.device), batch[\"seg\"].to(self.device)\n",
        "                start_time = time.time()\n",
        "\n",
        "                pred = sliding_window_inference(\n",
        "                    inputs=image,\n",
        "                    roi_size=roi_size,\n",
        "                    sw_batch_size=sw_batch_size,\n",
        "                    predictor=model\n",
        "                )\n",
        "                pred = torch.sigmoid(pred) > 0.5  # Binary thresholding\n",
        "\n",
        "                elapsed = time.time() - start_time\n",
        "                total_times.append(elapsed)\n",
        "\n",
        "                # Convert to NumPy\n",
        "                image_np = image[0, 0].cpu().numpy()\n",
        "                label_np = label[0, 0].cpu().numpy()\n",
        "                pred_np = pred[0, 0].cpu().numpy()\n",
        "\n",
        "                # Metrics\n",
        "                metrics = self.calculate_metrics(label_np, pred_np)\n",
        "                total_metrics += np.array(metrics)\n",
        "\n",
        "                # Identify sample name\n",
        "                sample_id = os.path.basename(batch[\"vol_meta_dict\"][\"filename_or_obj\"][0]).replace(\".nii.gz\", \"\")\n",
        "                self.save_result_slices(image_np, pred_np, label_np, sample_id)\n",
        "                self.append_metrics_to_csv(sample_id, metrics, elapsed)\n",
        "\n",
        "        # Print summary\n",
        "        num_samples = len(test_loader)\n",
        "        print(\"\\n📊 Average Test Metrics:\")\n",
        "        print(f\"Jaccard:  {total_metrics[0]/num_samples:.4f}\")\n",
        "        print(f\"F1:       {total_metrics[1]/num_samples:.4f}\")\n",
        "        print(f\"Recall:   {total_metrics[2]/num_samples:.4f}\")\n",
        "        print(f\"Precision:{total_metrics[3]/num_samples:.4f}\")\n",
        "        print(f\"Accuracy: {total_metrics[4]/num_samples:.4f}\")\n",
        "        print(f\"⚡ FPS:    {1 / np.mean(total_times):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (6) Early Stopping"
      ],
      "metadata": {
        "id": "3JLRqnw5L7xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, verbose=True, min_delta=0, path='checkpoint.pt',\n",
        "                 start_val_loss_min=None, start_patience_counter=0):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.min_delta = min_delta\n",
        "        self.path = path\n",
        "        self.val_loss_min = start_val_loss_min if start_val_loss_min is not None else np.inf\n",
        "        self.counter = start_patience_counter\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss, model, epoch=None, optimizer=None):\n",
        "        improved = False\n",
        "        if val_loss < self.val_loss_min - self.min_delta:\n",
        "            self.val_loss_min = val_loss\n",
        "            self.counter = 0\n",
        "            improved = True\n",
        "            if self.verbose:\n",
        "                print(f\"✅ Validation loss improved. Saving model...\")\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f\"⏳ EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "\n",
        "        # Always save a full checkpoint\n",
        "        self.save_checkpoint(model, epoch, optimizer)\n",
        "\n",
        "        if self.counter >= self.patience:\n",
        "            self.early_stop = True\n",
        "\n",
        "        return self.early_stop\n",
        "\n",
        "    def save_checkpoint(self, model, epoch=None, optimizer=None):\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict() if optimizer else None,\n",
        "            'val_loss': self.val_loss_min,\n",
        "            'patience_counter': self.counter\n",
        "        }\n",
        "        torch.save(checkpoint, self.path)"
      ],
      "metadata": {
        "id": "LA4lGL-xL6st"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa8L5nD2EVP_"
      },
      "source": [
        "# (7). Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "281KQS_iEIDX"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ---------- Trainer ----------\n",
        "class UnetTrain:\n",
        "    def __init__(self, model_file, loss_result_path, lr, num_epochs, device):\n",
        "        self.model_file = model_file\n",
        "        self.loss_result_path = loss_result_path\n",
        "        self.lr = lr\n",
        "        self.num_epochs = num_epochs\n",
        "        self.device = device\n",
        "        self.seeding(42)\n",
        "\n",
        "    def seeding(self, seed):\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    def epoch_time(self, start_time, end_time):\n",
        "        elapsed = end_time - start_time\n",
        "        return int(elapsed / 60), int(elapsed % 60)\n",
        "\n",
        "    def compute_pos_weight(self, train_loader, max_batches=None):\n",
        "        \"\"\"Compute pos_weight = (#neg / #pos) from dataset.\"\"\"\n",
        "        pos, neg = 0, 0\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            seg = batch[\"seg\"].to(self.device)\n",
        "            pos += seg.sum().item()\n",
        "            neg += (seg.numel() - seg.sum().item())\n",
        "            if max_batches and (i + 1) >= max_batches:\n",
        "                break\n",
        "        pos_weight = neg / (pos + 1e-8)\n",
        "        print(f\"📊 Auto-computed pos_weight = {pos_weight:.3f} \"\n",
        "              f\"(pos voxels={pos:.0f}, neg voxels={neg:.0f})\")\n",
        "        return pos_weight\n",
        "\n",
        "    def train_one_epoch(self, model, loader, optimizer, loss_fn):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        scaler = torch.amp.GradScaler()\n",
        "        device_type = 'cuda' if self.device.type == 'cuda' else 'cpu'\n",
        "\n",
        "        for x in loader:\n",
        "            inputs, labels = x[\"vol\"].to(self.device), x[\"seg\"].to(self.device)\n",
        "            optimizer.zero_grad()\n",
        "            with torch.amp.autocast(device_type=device_type):\n",
        "                outputs = model(inputs)\n",
        "                loss = loss_fn(outputs, labels)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            epoch_loss += loss.item()\n",
        "        return epoch_loss / len(loader)\n",
        "\n",
        "    def evaluate(self, model, loader, loss_fn):\n",
        "        model.eval()\n",
        "        epoch_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for x in loader:\n",
        "                inputs, labels = x[\"vol\"].to(self.device), x[\"seg\"].to(self.device)\n",
        "                outputs = model(inputs)\n",
        "                loss = loss_fn(outputs, labels)\n",
        "                epoch_loss += loss.item()\n",
        "        return epoch_loss / len(loader)\n",
        "\n",
        "    def execute(self, train_loader, valid_loader):\n",
        "        model = UNet(\n",
        "            spatial_dims=3,\n",
        "            in_channels=1,\n",
        "            out_channels=1,\n",
        "            channels=(16, 32, 64, 128, 256),\n",
        "            strides=(2, 2, 2, 2),\n",
        "            num_res_units=2,\n",
        "            norm=Norm.BATCH\n",
        "        ).to(self.device)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=self.lr, weight_decay=1e-5)\n",
        "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "\n",
        "        # 🔹 Auto-compute pos_weight before training\n",
        "        pos_weight = self.compute_pos_weight(train_loader, max_batches=50)\n",
        "        loss_fn = DiceBCELoss3D().set_pos_weight(pos_weight, device=self.device)\n",
        "\n",
        "        # ---- Resume training state ----\n",
        "        start_epoch = 1\n",
        "        start_val_loss_min = None\n",
        "        start_patience_counter = 0\n",
        "        history = {\"train_loss\": [], \"valid_loss\": []}\n",
        "\n",
        "        if os.path.exists(self.model_file):\n",
        "            checkpoint = torch.load(self.model_file, map_location=self.device)\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            if checkpoint.get('optimizer_state_dict'):\n",
        "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            start_epoch = checkpoint.get('epoch', 1) + 1\n",
        "            start_val_loss_min = checkpoint.get('val_loss', None)\n",
        "            start_patience_counter = checkpoint.get('patience_counter', 0)\n",
        "\n",
        "        if os.path.exists(self.loss_result_path):\n",
        "            with open(self.loss_result_path, 'r') as f:\n",
        "                reader = csv.reader(f)\n",
        "                next(reader)\n",
        "                rows = list(reader)\n",
        "                if rows:\n",
        "                    last_epoch = int(rows[-1][0])\n",
        "                    start_epoch = last_epoch + 1\n",
        "                    history['train_loss'] = [float(r[1]) for r in rows]\n",
        "                    history['valid_loss'] = [float(r[2]) for r in rows]\n",
        "                    if start_val_loss_min is None:\n",
        "                        start_val_loss_min = min(history['valid_loss'])\n",
        "            shutil.copy(self.loss_result_path, self.loss_result_path.replace(\".csv\", \"_backup.csv\"))\n",
        "\n",
        "        early_stopping = EarlyStopping(\n",
        "            patience=10,\n",
        "            min_delta=0.0005,\n",
        "            path=self.model_file,\n",
        "            start_val_loss_min=start_val_loss_min,\n",
        "            start_patience_counter=start_patience_counter\n",
        "        )\n",
        "\n",
        "        if not os.path.exists(self.loss_result_path):\n",
        "            with open(self.loss_result_path, \"w\", newline=\"\") as f:\n",
        "                csv.writer(f).writerow([\"Epoch\", \"Train Loss\", \"Valid Loss\"])\n",
        "\n",
        "        # ---- Training loop ----\n",
        "        for epoch in range(start_epoch, self.num_epochs + 1):\n",
        "            start_time = time.time()\n",
        "            train_loss = self.train_one_epoch(model, train_loader, optimizer, loss_fn)\n",
        "            valid_loss = self.evaluate(model, valid_loader, loss_fn)\n",
        "            scheduler.step()\n",
        "\n",
        "            mins, secs = self.epoch_time(start_time, time.time())\n",
        "            print(f\"Epoch {epoch:03d} | Time: {mins}m {secs}s | \"\n",
        "                  f\"Train: {train_loss:.6f} | Val: {valid_loss:.6f}\")\n",
        "\n",
        "            history['train_loss'].append(train_loss)\n",
        "            history['valid_loss'].append(valid_loss)\n",
        "\n",
        "            with open(self.loss_result_path, \"a\", newline=\"\") as f:\n",
        "                csv.writer(f).writerow([epoch, train_loss, valid_loss])\n",
        "\n",
        "            if early_stopping(valid_loss, model, epoch, optimizer):\n",
        "                print(\"🛑 Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "            torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (8). Pipeline"
      ],
      "metadata": {
        "id": "BZrW1cHg4OlM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwcTHPshqu0O",
        "outputId": "9e770437-c057-46f8-9bbf-7e3f85fb41d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📁 Current Directory: /content/drive/MyDrive/PhDwork/Segmentation\n",
            "📦 Preparing datasets and transforms...\n",
            "✅ Dataset sizes — Train: 340, Valid: 43, Test: 38\n",
            "📊 Auto-computed pos_weight = 534.107 (pos voxels=165338, neg voxels=88308262)\n",
            "Epoch 001 | Time: 69m 26s | Train: 2.789326 | Val: 3.488296\n",
            "✅ Validation loss improved. Saving model...\n",
            "Epoch 002 | Time: 66m 48s | Train: 2.702567 | Val: 3.285268\n",
            "✅ Validation loss improved. Saving model...\n",
            "Epoch 003 | Time: 67m 32s | Train: 2.620983 | Val: 3.209697\n",
            "✅ Validation loss improved. Saving model...\n",
            "Epoch 004 | Time: 67m 13s | Train: 2.600965 | Val: 3.113213\n",
            "✅ Validation loss improved. Saving model...\n",
            "Epoch 005 | Time: 67m 34s | Train: 2.558125 | Val: 3.082911\n",
            "✅ Validation loss improved. Saving model...\n",
            "Epoch 006 | Time: 68m 11s | Train: 2.547104 | Val: 3.094644\n",
            "⏳ EarlyStopping counter: 1 out of 10\n"
          ]
        }
      ],
      "source": [
        "class UnetPipeline:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        set_determinism(seed=42)\n",
        "\n",
        "        # paths\n",
        "        os.chdir(self.config['target_dir'])\n",
        "        print(f\"📁 Current Directory: {os.getcwd()}\")\n",
        "        self.output_dir = os.path.join(\".\", \"results\", self.config['output_folder_name'])\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "        self.loss_result_file = os.path.join(self.output_dir, \"train_and_valid_loss_results.csv\")\n",
        "        self.model_file = os.path.join(self.output_dir, \"model.pth\")\n",
        "        self.test_metrics_file = os.path.join(self.output_dir, \"test_metrics.csv\")\n",
        "        self.test_result_path = os.path.join(self.output_dir, \"test_outputs\")\n",
        "        os.makedirs(self.test_result_path, exist_ok=True)\n",
        "\n",
        "        self.dataset_dir = os.path.join(\"./datasets\", f\"Datasets_{self.config['transformation']}\")\n",
        "\n",
        "        # prepare loaders\n",
        "        print(\"📦 Preparing datasets and transforms...\")\n",
        "        self.train_loader, self.valid_loader, self.test_loader, self.full_train_loader = self.prepare_loaders()\n",
        "\n",
        "        # # run sanity checks on full-volume loader (not patch loader)\n",
        "        # print(\"🔎 Running dataset sanity checks on full volumes (not patches)...\")\n",
        "        # self.print_dataset_stats(self.full_train_loader, name=\"train_full\", n=30)\n",
        "        # self.print_dataset_stats(self.valid_loader, name=\"valid\", n=30)\n",
        "        # self.print_dataset_stats(self.test_loader, name=\"test\", n=30)\n",
        "\n",
        "    def print_dataset_stats(self, loader, name=\"train\", n=20):\n",
        "        tot_pos = 0.0\n",
        "        tot_vox = 0\n",
        "        samples = 0\n",
        "        unique_vals = set()\n",
        "        for i, batch in enumerate(loader):\n",
        "            seg = batch[\"seg\"][0, 0]  # [D,H,W]\n",
        "            seg_cpu = seg.cpu()\n",
        "            tot_pos += float(seg_cpu.sum().item())\n",
        "            tot_vox += seg_cpu.numel()\n",
        "            samples += 1\n",
        "            vals = torch.unique(seg_cpu)\n",
        "            for v in vals.tolist():\n",
        "                unique_vals.add(v)\n",
        "            if i + 1 >= n:\n",
        "                break\n",
        "        frac = tot_pos / (tot_vox + 1e-12)\n",
        "        print(f\"  [{name}] checked {samples} samples. total pos voxels: {tot_pos:.1f}, total voxels: {tot_vox}, pos fraction: {frac:.6g}\")\n",
        "        print(f\"  [{name}] example mask unique values (approx): {sorted(list(unique_vals))[:10]}\")\n",
        "        if frac == 0:\n",
        "            print(f\"  ⚠️ [{name}] No positives found in the inspected samples.\")\n",
        "        if any(v not in (0.0, 1.0) for v in unique_vals):\n",
        "            print(f\"  ⚠️ [{name}] Found non-binary mask values (not only 0/1). Use nearest interpolation for segs.\")\n",
        "\n",
        "        return frac\n",
        "\n",
        "    def prepare_loaders(self):\n",
        "        pixdim = (1, 1, 1)\n",
        "        a_min, a_max = -1000, 700\n",
        "        patch_size = (96, 96, 96)\n",
        "\n",
        "        def get_files(split):\n",
        "            ct = sorted(glob(os.path.join(self.dataset_dir, split, \"ct\", \"*.nii.gz\")))\n",
        "            seg = sorted(glob(os.path.join(self.dataset_dir, split, \"segment\", \"*.nii.gz\")))\n",
        "            if len(ct) != len(seg):\n",
        "                raise RuntimeError(f\"Mismatch CT/SEG for {split}: {len(ct)} vs {len(seg)}\")\n",
        "            return [{\"vol\": c, \"seg\": s} for c, s in zip(ct, seg)]\n",
        "\n",
        "\n",
        "       # full-volume transforms for validation/test + sanity checks\n",
        "        base_transforms = Compose([\n",
        "            LoadImaged(keys=[\"vol\", \"seg\"]),\n",
        "            EnsureChannelFirstd(keys=[\"vol\", \"seg\"]),\n",
        "            # Spacingd: interpolation mode for vol then seg (image->bilinear, seg->nearest)\n",
        "            Spacingd(keys=[\"vol\", \"seg\"], pixdim=pixdim, mode=(\"bilinear\", \"nearest\")),\n",
        "            Orientationd(keys=[\"vol\", \"seg\"], axcodes=\"RAS\", labels=None),\n",
        "            ScaleIntensityRanged(keys=[\"vol\"], a_min=a_min, a_max=a_max, b_min=0.0, b_max=1.0, clip=True),\n",
        "            CropForegroundd(keys=[\"vol\", \"seg\"], source_key=\"vol\"),\n",
        "            # pad full volumes up to patch_size (constant padding)\n",
        "            ResizeWithPadOrCropd(keys=[\"vol\", \"seg\"], spatial_size=patch_size, mode=\"constant\"),\n",
        "            ToTensord(keys=[\"vol\", \"seg\"]),\n",
        "        ])\n",
        "\n",
        "        # train transforms: pad/crop to >= patch_size, then sample patches with RandCropByPosNegLabeld\n",
        "        train_transforms = Compose([\n",
        "            LoadImaged(keys=[\"vol\", \"seg\"]),\n",
        "            EnsureChannelFirstd(keys=[\"vol\", \"seg\"]),\n",
        "            Spacingd(keys=[\"vol\", \"seg\"], pixdim=pixdim, mode=(\"bilinear\", \"nearest\")),\n",
        "            Orientationd(keys=[\"vol\", \"seg\"], axcodes=\"RAS\", labels=None),\n",
        "            ScaleIntensityRanged(keys=[\"vol\"], a_min=a_min, a_max=a_max, b_min=0.0, b_max=1.0, clip=True),\n",
        "            CropForegroundd(keys=[\"vol\", \"seg\"], source_key=\"vol\"),\n",
        "            # Resize/pad so volume is at least patch_size (constant padding)\n",
        "            ResizeWithPadOrCropd(keys=[\"vol\", \"seg\"], spatial_size=patch_size, mode=\"constant\"),\n",
        "\n",
        "            # Balanced sampling by positive labels (num_samples=1; batch_size controls effective patches per step)\n",
        "            RandCropByPosNegLabeld(\n",
        "                keys=[\"vol\", \"seg\"],\n",
        "                label_key=\"seg\",\n",
        "                spatial_size=patch_size,\n",
        "                pos=1,\n",
        "                neg=1,\n",
        "                num_samples=1,\n",
        "                image_key=\"vol\",\n",
        "                image_threshold=0,\n",
        "            ),\n",
        "\n",
        "            # augmentations (image: bilinear / seg: nearest is handled above on Spacingd)\n",
        "            RandFlipd(keys=[\"vol\", \"seg\"], prob=0.5, spatial_axis=0),\n",
        "            RandFlipd(keys=[\"vol\", \"seg\"], prob=0.5, spatial_axis=1),\n",
        "            RandAffined(keys=[\"vol\", \"seg\"], prob=0.3,\n",
        "                        rotate_range=(0.1, 0.1, 0.1),\n",
        "                        scale_range=(0.1, 0.1, 0.1),\n",
        "                        mode=(\"bilinear\", \"nearest\")),  # interpolation for images/segs\n",
        "            RandGaussianNoised(keys=[\"vol\"], prob=0.2, mean=0.0, std=0.1),\n",
        "            RandScaleIntensityd(keys=[\"vol\"], factors=0.1, prob=0.5),\n",
        "\n",
        "            ToTensord(keys=[\"vol\", \"seg\"]),\n",
        "            EnsureTyped(keys=[\"vol\", \"seg\"]),\n",
        "        ])\n",
        "\n",
        "        # Create datasets and loaders\n",
        "        train_ds = Dataset(get_files(\"train\"), train_transforms)\n",
        "        valid_ds = Dataset(get_files(\"valid\"), base_transforms)\n",
        "        test_ds  = Dataset(get_files(\"test\"), base_transforms)\n",
        "\n",
        "        # Optionally you can use CacheDataset for speed (uncomment & tune)\n",
        "        # from monai.data import CacheDataset\n",
        "        # train_ds = CacheDataset(data=get_files(\"train\"), transform=train_transforms, cache_rate=0.5, num_workers=4)\n",
        "\n",
        "        train_loader = DataLoader(train_ds, batch_size=self.config['batch_size'], shuffle=True, num_workers=0, collate_fn=pad_list_data_collate)\n",
        "        valid_loader = DataLoader(valid_ds, batch_size=self.config['batch_size'], shuffle=False, num_workers=0, collate_fn=pad_list_data_collate)\n",
        "        test_loader  = DataLoader(test_ds,  batch_size=1, shuffle=False, num_workers=0, collate_fn=pad_list_data_collate)\n",
        "\n",
        "        # full volume train loader for sanity checks (no patching)\n",
        "        full_train_ds = Dataset(get_files(\"train\"), base_transforms)\n",
        "        full_train_loader = DataLoader(full_train_ds, batch_size=1, shuffle=False, num_workers=0, collate_fn=pad_list_data_collate)\n",
        "\n",
        "        print(f\"✅ Dataset sizes — Train: {len(train_ds)}, Valid: {len(valid_ds)}, Test: {len(test_ds)}\")\n",
        "\n",
        "        return train_loader, valid_loader, test_loader, full_train_loader\n",
        "\n",
        "    def train(self):\n",
        "        trainer = UnetTrain(\n",
        "            model_file=self.model_file,\n",
        "            loss_result_path=self.loss_result_file,\n",
        "            lr=self.config['learning_rate'],\n",
        "            num_epochs=self.config['num_epochs'],\n",
        "            device=self.device\n",
        "        )\n",
        "        trainer.execute(self.train_loader, self.valid_loader)\n",
        "\n",
        "    def test(self):\n",
        "        # Create model and load checkpoint for testing\n",
        "        model = UNet(\n",
        "            spatial_dims=3,\n",
        "            in_channels=1,\n",
        "            out_channels=1,\n",
        "            channels=(16, 32, 64, 128, 256),\n",
        "            strides=(2, 2, 2, 2),\n",
        "            num_res_units=2,\n",
        "            norm=Norm.BATCH\n",
        "        ).to(self.device)\n",
        "\n",
        "        if os.path.exists(self.model_file):\n",
        "            checkpoint = torch.load(self.model_file, map_location=self.device)\n",
        "            if checkpoint.get('model_state_dict') is not None:\n",
        "                model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            else:\n",
        "                model.load_state_dict(checkpoint)\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"Model file not found: {self.model_file}\")\n",
        "\n",
        "        tester = None  # your UnetTest implementation can be used here\n",
        "        # tester = UnetTest(self.test_result_path, self.test_metrics_file, self.device)\n",
        "        # tester.test(model, self.test_loader)\n",
        "\n",
        "    def run(self):\n",
        "        self.train()\n",
        "        self.test()\n",
        "\n",
        "\n",
        "# ---------- Run ----------\n",
        "def main():\n",
        "    import multiprocessing as mp\n",
        "    mp.set_start_method('spawn', force=True)\n",
        "\n",
        "    config = {\n",
        "        'target_dir': \"/content/drive/MyDrive/PhDwork/Segmentation\",\n",
        "        'output_folder_name': \"Results_Nifti_MONAI2\",\n",
        "        'transformation': \"OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\",\n",
        "        'batch_size': 2,   # effective patches per step = num_samples * batch_size (we set num_samples=1)\n",
        "        'num_epochs': 100,\n",
        "        'learning_rate': 1e-4,\n",
        "    }\n",
        "\n",
        "    pipeline = UnetPipeline(config)\n",
        "    pipeline.run()   # uncomment to actually train/test\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#(8) Mask Generation"
      ],
      "metadata": {
        "id": "hSP35kUBOaDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from monai.networks.nets import UNet\n",
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    Resized,\n",
        "    CopyItemsd,\n",
        "    Invertd,\n",
        "    LoadImaged,\n",
        "    EnsureChannelFirstd,\n",
        "    Spacingd,\n",
        "    Orientationd,\n",
        "    ScaleIntensityRanged,\n",
        "    CropForegroundd,\n",
        "    EnsureTyped,\n",
        "    SaveImaged,\n",
        "    ToTensord,\n",
        ")\n",
        "from monai.data import Dataset, DataLoader, decollate_batch\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.utils import set_determinism\n",
        "from monai.networks.layers import Norm\n",
        "# from monai.transforms.utils import SaveTransform\n",
        "\n",
        "\n",
        "\n",
        "class UNetInferencePipeline:\n",
        "    def __init__(self, model_path, input_ct_dir, input_seg_dir, output_dir, device=\"cuda:0\"):\n",
        "        self.device = device if torch.cuda.is_available() else \"cpu\"\n",
        "        self.input_ct_dir = input_ct_dir\n",
        "        self.input_seg_dir = input_seg_dir\n",
        "        self.output_dir = output_dir\n",
        "        self.ct_out_dir = os.path.join(output_dir, \"ct\")\n",
        "        self.seg_out_dir = os.path.join(output_dir, \"segment\")\n",
        "        os.makedirs(self.ct_out_dir, exist_ok=True)\n",
        "        os.makedirs(self.seg_out_dir, exist_ok=True)\n",
        "        self.model_path = model_path\n",
        "        self.model = self._load_model()\n",
        "        set_determinism(seed=42)\n",
        "        self.forward_transforms = self._get_forward_transforms()\n",
        "        self.inverse_transforms = None\n",
        "        self.dataloader = self._prepare_dataloader()\n",
        "\n",
        "    def _load_model(self):\n",
        "        if not os.path.exists(self.model_path):\n",
        "            raise FileNotFoundError(f\"Model file not found at: {self.model_path}\")\n",
        "\n",
        "        model = UNet(\n",
        "            spatial_dims=3,\n",
        "            in_channels=1,\n",
        "            out_channels=1,\n",
        "            channels=(16, 32, 64, 128, 256),\n",
        "            strides=(2, 2, 2, 2),\n",
        "            num_res_units=2,\n",
        "            norm=Norm.BATCH\n",
        "        ).to(self.device)\n",
        "\n",
        "        state_dict = torch.load(self.model_path, map_location=self.device)\n",
        "        model.load_state_dict(state_dict.get('model_state_dict', state_dict))\n",
        "\n",
        "        print(f\"✅ Model loaded successfully from {self.model_path}\")\n",
        "        return model\n",
        "\n",
        "\n",
        "\n",
        "    def _get_forward_transforms(self):\n",
        "        return Compose([\n",
        "            LoadImaged(keys=[\"vol\"]),\n",
        "            EnsureChannelFirstd(keys=[\"vol\"]),\n",
        "            CopyItemsd(keys=[\"vol\"], names=[\"vol_meta_dict\"]),\n",
        "            Spacingd(keys=[\"vol\"], pixdim=(1.0, 1.0, 1.0), mode=\"bilinear\"),\n",
        "            Orientationd(keys=[\"vol\"], axcodes=\"RAS\"),\n",
        "            ScaleIntensityRanged(keys=[\"vol\"], a_min=-1000, a_max=700, b_min=0.0, b_max=1.0, clip=True),\n",
        "            CropForegroundd(keys=[\"vol\"], source_key=\"vol\"),\n",
        "            Resized(keys=[\"vol\"], spatial_size=(96, 96, 96)),\n",
        "            EnsureTyped(keys=[\"vol\"]),\n",
        "        ])\n",
        "\n",
        "    def _get_inverse_transforms(self):\n",
        "        return Compose([\n",
        "            Invertd(\n",
        "                keys=[\"seg\"],\n",
        "                transform=self.forward_transforms,\n",
        "                orig_keys=[\"vol\"],\n",
        "                meta_keys=[\"vol_meta_dict\"],\n",
        "                nearest_interp=True,\n",
        "                to_tensor=False,\n",
        "            ),\n",
        "            EnsureTyped(keys=[\"seg\"])\n",
        "        ])\n",
        "\n",
        "    def _prepare_dataloader(self):\n",
        "        data = []\n",
        "        for f in os.listdir(self.input_ct_dir):\n",
        "            if f.endswith(('.nii', '.nii.gz')):\n",
        "                ct_path = os.path.join(self.input_ct_dir, f)\n",
        "                data.append({\"vol\": ct_path})\n",
        "        print(f\"🔍 Found {len(data)} NIfTI files for inference.\")\n",
        "        return DataLoader(Dataset(data=data, transform=self.forward_transforms), batch_size=1, num_workers=0)\n",
        "\n",
        "    def infer(self):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(self.dataloader):\n",
        "                batch = decollate_batch(batch)[0]\n",
        "                vol_meta = batch[\"vol_meta_dict\"]\n",
        "                ct = batch[\"vol\"]\n",
        "\n",
        "                if ct.dim() == 4:\n",
        "                    ct = ct.unsqueeze(0)\n",
        "                ct = ct.to(self.device)\n",
        "\n",
        "                filename = os.path.basename(vol_meta.meta[\"filename_or_obj\"])\n",
        "                orig_vol = nib.load(vol_meta.meta[\"filename_or_obj\"]).get_fdata()\n",
        "                print(f\"🔍 Inference on [{i+1}] {filename} | shape = {ct.shape}\")\n",
        "                print(f\"🔍 Original volume shape = {orig_vol.shape}\")\n",
        "                pred = self.model(ct)\n",
        "                pred = (torch.sigmoid(pred) > 0.5).float()\n",
        "\n",
        "                print(f\"✅ Predicted mask shape: {pred.shape}\")\n",
        "\n",
        "                batch[\"seg\"] = pred.cpu().squeeze(0)\n",
        "                print(f\"✅ Batch shape: {batch['seg'].shape}\")\n",
        "\n",
        "                if self.inverse_transforms is None:\n",
        "                    self.inverse_transforms = self._get_inverse_transforms()\n",
        "\n",
        "                inverted = self.inverse_transforms(batch)\n",
        "                inv_seg = inverted[\"seg\"].squeeze(0).numpy()\n",
        "                inv_seg = (inv_seg > 0.5).astype(np.uint8)\n",
        "                print(f\"✅ Inverted mask shape: {inv_seg.shape}\")\n",
        "\n",
        "                self._save_nifti(inv_seg, vol_meta, self.seg_out_dir, filename, is_segmentation=True)\n",
        "\n",
        "\n",
        "    def _save_nifti(self, array, meta_tensor, out_dir, filename, is_segmentation=False):\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "        affine = meta_tensor.meta.get(\"original_affine\", meta_tensor.meta.get(\"affine\", np.eye(4)))\n",
        "        dtype = np.uint8 if is_segmentation else np.float32\n",
        "        nib_img = nib.Nifti1Image(array.astype(dtype), affine)\n",
        "        nib.save(nib_img, os.path.join(out_dir, filename))\n",
        "        print(f\"✅ Saved: {os.path.join(out_dir, filename)}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ROOT_DIR = \"/content/drive/MyDrive/PhDwork/Segmentation\"\n",
        "    MODEL_PATH = os.path.join(ROOT_DIR, \"results\", \"Results_MONAI_Augmented\", \"model.pth\")\n",
        "    INPUT_CT_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"Lung3\", \"ct\")\n",
        "    INPUT_SEG_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"Lung3\", \"segment\")\n",
        "    OUTPUT_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"Lung3_Predicted\")\n",
        "\n",
        "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "    os.chdir(ROOT_DIR)\n",
        "\n",
        "    try:\n",
        "        pipeline = UNetInferencePipeline(MODEL_PATH, INPUT_CT_FOLDER, INPUT_SEG_FOLDER, OUTPUT_FOLDER)\n",
        "        pipeline.infer()\n",
        "        print(\"🎉 Inference completed successfully for all patients!\")\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "z6a0G1DXTIV8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81a85b9f-c61a-4fd8-b379-3bd00e32c605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model loaded successfully from /content/drive/MyDrive/PhDwork/Segmentation/results/Results_MONAI_Augmented/model.pth\n",
            "🔍 Found 89 NIfTI files for inference.\n",
            "🔍 Inference on [1] LUNG3-01.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (59, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (59, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-01.nii.gz\n",
            "🔍 Inference on [2] LUNG3-02.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (57, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (57, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-02.nii.gz\n",
            "🔍 Inference on [3] LUNG3-03.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (61, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (61, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-03.nii.gz\n",
            "🔍 Inference on [4] LUNG3-04.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (61, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (61, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-04.nii.gz\n",
            "🔍 Inference on [5] LUNG3-05.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (229, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (229, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-05.nii.gz\n",
            "🔍 Inference on [6] LUNG3-06.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (61, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (61, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-06.nii.gz\n",
            "🔍 Inference on [7] LUNG3-07.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (86, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (86, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-07.nii.gz\n",
            "🔍 Inference on [8] LUNG3-08.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (158, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (158, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-08.nii.gz\n",
            "🔍 Inference on [9] LUNG3-09.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (140, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (140, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-09.nii.gz\n",
            "🔍 Inference on [10] LUNG3-10.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (95, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (95, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-10.nii.gz\n",
            "🔍 Inference on [11] LUNG3-11.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (252, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (252, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-11.nii.gz\n",
            "🔍 Inference on [12] LUNG3-12.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (206, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (206, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-12.nii.gz\n",
            "🔍 Inference on [13] LUNG3-13.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (71, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (71, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-13.nii.gz\n",
            "🔍 Inference on [14] LUNG3-14.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (325, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (325, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-14.nii.gz\n",
            "🔍 Inference on [15] LUNG3-15.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (234, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (234, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-15.nii.gz\n",
            "🔍 Inference on [16] LUNG3-16.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (192, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (192, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-16.nii.gz\n",
            "🔍 Inference on [17] LUNG3-17.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (226, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (226, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-17.nii.gz\n",
            "🔍 Inference on [18] LUNG3-18.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (178, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (178, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-18.nii.gz\n",
            "🔍 Inference on [19] LUNG3-19.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (178, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (178, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-19.nii.gz\n",
            "🔍 Inference on [20] LUNG3-20.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (176, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (176, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-20.nii.gz\n",
            "🔍 Inference on [21] LUNG3-21.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (74, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (74, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-21.nii.gz\n",
            "🔍 Inference on [22] LUNG3-22.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (236, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (236, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-22.nii.gz\n",
            "🔍 Inference on [23] LUNG3-23.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (52, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (52, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-23.nii.gz\n",
            "🔍 Inference on [24] LUNG3-24.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (134, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (134, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-24.nii.gz\n",
            "🔍 Inference on [25] LUNG3-25.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (202, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (202, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-25.nii.gz\n",
            "🔍 Inference on [26] LUNG3-26.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (83, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (83, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-26.nii.gz\n",
            "🔍 Inference on [27] LUNG3-27.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (149, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (149, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-27.nii.gz\n",
            "🔍 Inference on [28] LUNG3-28.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (86, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (86, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-28.nii.gz\n",
            "🔍 Inference on [29] LUNG3-29.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (173, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (173, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-29.nii.gz\n",
            "🔍 Inference on [30] LUNG3-30.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (72, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (72, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-30.nii.gz\n",
            "🔍 Inference on [31] LUNG3-31.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (242, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (242, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-31.nii.gz\n",
            "🔍 Inference on [32] LUNG3-32.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (58, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (58, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-32.nii.gz\n",
            "🔍 Inference on [33] LUNG3-33.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (276, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (276, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-33.nii.gz\n",
            "🔍 Inference on [34] LUNG3-34.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (255, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (255, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-34.nii.gz\n",
            "🔍 Inference on [35] LUNG3-35.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (253, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (253, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-35.nii.gz\n",
            "🔍 Inference on [36] LUNG3-36.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (356, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (356, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-36.nii.gz\n",
            "🔍 Inference on [37] LUNG3-37.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (97, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (97, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-37.nii.gz\n",
            "🔍 Inference on [38] LUNG3-38.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (223, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (223, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-38.nii.gz\n",
            "🔍 Inference on [39] LUNG3-39.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (82, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (82, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-39.nii.gz\n",
            "🔍 Inference on [40] LUNG3-40.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (239, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (239, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-40.nii.gz\n",
            "🔍 Inference on [41] LUNG3-41.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (110, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (110, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-41.nii.gz\n",
            "🔍 Inference on [42] LUNG3-42.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (255, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (255, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-42.nii.gz\n",
            "🔍 Inference on [43] LUNG3-43.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (68, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (68, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-43.nii.gz\n",
            "🔍 Inference on [44] LUNG3-44.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (227, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (227, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-44.nii.gz\n",
            "🔍 Inference on [45] LUNG3-45.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (178, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (178, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-45.nii.gz\n",
            "🔍 Inference on [46] LUNG3-46.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (184, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (184, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-46.nii.gz\n",
            "🔍 Inference on [47] LUNG3-47.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (275, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (275, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-47.nii.gz\n",
            "🔍 Inference on [48] LUNG3-48.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (157, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (157, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-48.nii.gz\n",
            "🔍 Inference on [49] LUNG3-49.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (89, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (89, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-49.nii.gz\n",
            "🔍 Inference on [50] LUNG3-50.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (255, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (255, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-50.nii.gz\n",
            "🔍 Inference on [51] LUNG3-51.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (76, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (76, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-51.nii.gz\n",
            "🔍 Inference on [52] LUNG3-52.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (50, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (50, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-52.nii.gz\n",
            "🔍 Inference on [53] LUNG3-53.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (255, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (255, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-53.nii.gz\n",
            "🔍 Inference on [54] LUNG3-54.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (175, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (175, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-54.nii.gz\n",
            "🔍 Inference on [55] LUNG3-55.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (72, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (72, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-55.nii.gz\n",
            "🔍 Inference on [56] LUNG3-56.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (178, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (178, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-56.nii.gz\n",
            "🔍 Inference on [57] LUNG3-57.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (178, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (178, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-57.nii.gz\n",
            "🔍 Inference on [58] LUNG3-58.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (64, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (64, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-58.nii.gz\n",
            "🔍 Inference on [59] LUNG3-59.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (62, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (62, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-59.nii.gz\n",
            "🔍 Inference on [60] LUNG3-60.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (92, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (92, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-60.nii.gz\n",
            "🔍 Inference on [61] LUNG3-61.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (203, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (203, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-61.nii.gz\n",
            "🔍 Inference on [62] LUNG3-62.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (66, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (66, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-62.nii.gz\n",
            "🔍 Inference on [63] LUNG3-63.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (57, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (57, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-63.nii.gz\n",
            "🔍 Inference on [64] LUNG3-64.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (172, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (172, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-64.nii.gz\n",
            "🔍 Inference on [65] LUNG3-65.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (175, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (175, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-65.nii.gz\n",
            "🔍 Inference on [66] LUNG3-66.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (69, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (69, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-66.nii.gz\n",
            "🔍 Inference on [67] LUNG3-67.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (74, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (74, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-67.nii.gz\n",
            "🔍 Inference on [68] LUNG3-68.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (60, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (60, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-68.nii.gz\n",
            "🔍 Inference on [69] LUNG3-69.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (158, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (158, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-69.nii.gz\n",
            "🔍 Inference on [70] LUNG3-70.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (258, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (258, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-70.nii.gz\n",
            "🔍 Inference on [71] LUNG3-71.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (287, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (287, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-71.nii.gz\n",
            "🔍 Inference on [72] LUNG3-72.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (84, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (84, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-72.nii.gz\n",
            "🔍 Inference on [73] LUNG3-73.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (218, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (218, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-73.nii.gz\n",
            "🔍 Inference on [74] LUNG3-74.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (67, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (67, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-74.nii.gz\n",
            "🔍 Inference on [75] LUNG3-75.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (88, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (88, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-75.nii.gz\n",
            "🔍 Inference on [76] LUNG3-76.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (74, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (74, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-76.nii.gz\n",
            "🔍 Inference on [77] LUNG3-77.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (99, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (99, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-77.nii.gz\n",
            "🔍 Inference on [78] LUNG3-78.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (255, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (255, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-78.nii.gz\n",
            "🔍 Inference on [79] LUNG3-79.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (240, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (240, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-79.nii.gz\n",
            "🔍 Inference on [80] LUNG3-80.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (59, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (59, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-80.nii.gz\n",
            "🔍 Inference on [81] LUNG3-81.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (307, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (307, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-81.nii.gz\n",
            "🔍 Inference on [82] LUNG3-82.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (78, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (78, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-82.nii.gz\n",
            "🔍 Inference on [83] LUNG3-83.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (178, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (178, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-83.nii.gz\n",
            "🔍 Inference on [84] LUNG3-84.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (66, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (66, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-84.nii.gz\n",
            "🔍 Inference on [85] LUNG3-85.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (234, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (234, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-85.nii.gz\n",
            "🔍 Inference on [86] LUNG3-86.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (78, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (78, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-86.nii.gz\n",
            "🔍 Inference on [87] LUNG3-87.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (79, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (79, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-87.nii.gz\n",
            "🔍 Inference on [88] LUNG3-88.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (154, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (154, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-88.nii.gz\n",
            "🔍 Inference on [89] LUNG3-89.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (158, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (158, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3_Predicted/segment/LUNG3-89.nii.gz\n",
            "🎉 Inference completed successfully for all patients!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    ROOT_DIR = \"/content/drive/MyDrive/PhDwork/Segmentation\"\n",
        "    MODEL_PATH = os.path.join(ROOT_DIR, \"results\", \"Results_MONAI_Augmented\", \"model.pth\")\n",
        "    INPUT_CT_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"test\", \"ct\")\n",
        "    INPUT_SEG_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"test\", \"segment\")\n",
        "    OUTPUT_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"test_Predicted\")\n",
        "\n",
        "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "    os.chdir(ROOT_DIR)\n",
        "\n",
        "    try:\n",
        "        pipeline = UNetInferencePipeline(MODEL_PATH, INPUT_CT_FOLDER, INPUT_SEG_FOLDER, OUTPUT_FOLDER)\n",
        "        pipeline.infer()\n",
        "        print(\"🎉 Inference completed successfully for all patients!\")\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "id": "Ckpn4rjZJ9fn",
        "outputId": "b54762e0-6093-45f5-86b6-48c278f39d56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model loaded successfully from /content/drive/MyDrive/PhDwork/Segmentation/results/Results_MONAI_Augmented/model.pth\n",
            "🔍 Found 38 NIfTI files for inference.\n",
            "🔍 Inference on [1] LUNG1-001.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (134, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (134, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-001.nii.gz\n",
            "🔍 Inference on [2] LUNG1-025.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (106, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (106, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-025.nii.gz\n",
            "🔍 Inference on [3] LUNG1-027.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (108, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (108, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-027.nii.gz\n",
            "🔍 Inference on [4] LUNG1-034.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (95, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (95, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-034.nii.gz\n",
            "🔍 Inference on [5] LUNG1-039.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (95, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (95, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-039.nii.gz\n",
            "🔍 Inference on [6] LUNG1-066.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (92, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (92, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-066.nii.gz\n",
            "🔍 Inference on [7] LUNG1-078.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (136, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (136, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-078.nii.gz\n",
            "🔍 Inference on [8] LUNG1-088.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (123, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (123, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-088.nii.gz\n",
            "🔍 Inference on [9] LUNG1-107.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (116, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (116, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-107.nii.gz\n",
            "🔍 Inference on [10] LUNG1-132.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (114, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (114, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-132.nii.gz\n",
            "🔍 Inference on [11] LUNG1-133.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (184, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (184, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-133.nii.gz\n",
            "🔍 Inference on [12] LUNG1-143.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (134, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (134, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-143.nii.gz\n",
            "🔍 Inference on [13] LUNG1-149.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (118, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (118, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-149.nii.gz\n",
            "🔍 Inference on [14] LUNG1-151.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (118, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (118, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-151.nii.gz\n",
            "🔍 Inference on [15] LUNG1-158.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (115, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (115, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-158.nii.gz\n",
            "🔍 Inference on [16] LUNG1-168.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (134, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (134, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-168.nii.gz\n",
            "🔍 Inference on [17] LUNG1-175.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (112, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (112, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-175.nii.gz\n",
            "🔍 Inference on [18] LUNG1-176.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (106, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (106, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-176.nii.gz\n",
            "🔍 Inference on [19] LUNG1-201.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (134, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (134, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-201.nii.gz\n",
            "🔍 Inference on [20] LUNG1-224.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (93, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (93, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-224.nii.gz\n",
            "🔍 Inference on [21] LUNG1-225.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (134, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (134, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-225.nii.gz\n",
            "🔍 Inference on [22] LUNG1-235.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (129, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (129, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-235.nii.gz\n",
            "🔍 Inference on [23] LUNG1-239.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (134, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (134, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-239.nii.gz\n",
            "🔍 Inference on [24] LUNG1-246.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (115, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (115, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-246.nii.gz\n",
            "🔍 Inference on [25] LUNG1-263.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (134, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (134, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-263.nii.gz\n",
            "🔍 Inference on [26] LUNG1-266.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (94, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (94, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-266.nii.gz\n",
            "🔍 Inference on [27] LUNG1-281.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (101, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (101, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-281.nii.gz\n",
            "🔍 Inference on [28] LUNG1-286.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (136, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (136, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-286.nii.gz\n",
            "🔍 Inference on [29] LUNG1-312.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (134, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (134, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-312.nii.gz\n",
            "🔍 Inference on [30] LUNG1-338.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (134, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (134, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-338.nii.gz\n",
            "🔍 Inference on [31] LUNG1-352.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (134, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (134, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-352.nii.gz\n",
            "🔍 Inference on [32] LUNG1-353.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (134, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (134, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-353.nii.gz\n",
            "🔍 Inference on [33] LUNG1-365.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (134, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (134, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-365.nii.gz\n",
            "🔍 Inference on [34] LUNG1-374.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (130, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (130, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-374.nii.gz\n",
            "🔍 Inference on [35] LUNG1-383.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (134, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (134, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-383.nii.gz\n",
            "🔍 Inference on [36] LUNG1-405.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (134, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (134, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-405.nii.gz\n",
            "🔍 Inference on [37] LUNG1-408.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (107, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (107, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-408.nii.gz\n",
            "🔍 Inference on [38] LUNG1-410.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (134, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (134, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/test_Predicted/segment/LUNG1-410.nii.gz\n",
            "🎉 Inference completed successfully for all patients!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    ROOT_DIR = \"/content/drive/MyDrive/PhDwork/Segmentation\"\n",
        "    MODEL_PATH = os.path.join(ROOT_DIR, \"results\", \"Results_MONAI_Augmented\", \"model.pth\")\n",
        "    INPUT_CT_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"valid\", \"ct\")\n",
        "    INPUT_SEG_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"valid\", \"segment\")\n",
        "    OUTPUT_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"valid_Predicted\")\n",
        "\n",
        "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "    os.chdir(ROOT_DIR)\n",
        "\n",
        "    try:\n",
        "        pipeline = UNetInferencePipeline(MODEL_PATH, INPUT_CT_FOLDER, INPUT_SEG_FOLDER, OUTPUT_FOLDER)\n",
        "        pipeline.infer()\n",
        "        print(\"🎉 Inference completed successfully for all patients!\")\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcIi6GZ347x8",
        "outputId": "5b5fc90e-eeaf-4b07-d6a5-bf3197b8e4a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model loaded successfully from /content/drive/MyDrive/PhDwork/Segmentation/results/Results_MONAI_Augmented/model.pth\n",
            "🔍 Found 43 NIfTI files for inference.\n",
            "🔍 Inference on [1] LUNG1-010.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (91, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (91, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-010.nii.gz\n",
            "🔍 Inference on [2] LUNG1-031.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (153, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (153, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-031.nii.gz\n",
            "🔍 Inference on [3] LUNG1-040.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (95, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (95, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-040.nii.gz\n",
            "🔍 Inference on [4] LUNG1-056.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (134, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (134, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-056.nii.gz\n",
            "🔍 Inference on [5] LUNG1-057.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (101, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (101, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-057.nii.gz\n",
            "🔍 Inference on [6] LUNG1-071.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (135, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (135, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-071.nii.gz\n",
            "🔍 Inference on [7] LUNG1-073.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (176, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (176, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-073.nii.gz\n",
            "🔍 Inference on [8] LUNG1-074.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (115, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (115, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-074.nii.gz\n",
            "🔍 Inference on [9] LUNG1-076.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (92, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (92, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-076.nii.gz\n",
            "🔍 Inference on [10] LUNG1-077.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (117, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (117, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-077.nii.gz\n",
            "🔍 Inference on [11] LUNG1-080.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (99, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (99, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-080.nii.gz\n",
            "🔍 Inference on [12] LUNG1-091.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (135, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (135, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-091.nii.gz\n",
            "🔍 Inference on [13] LUNG1-095.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (106, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (106, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-095.nii.gz\n",
            "🔍 Inference on [14] LUNG1-117.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (90, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (90, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-117.nii.gz\n",
            "🔍 Inference on [15] LUNG1-134.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (108, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (108, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-134.nii.gz\n",
            "🔍 Inference on [16] LUNG1-139.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (107, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (107, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-139.nii.gz\n",
            "🔍 Inference on [17] LUNG1-147.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (99, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (99, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-147.nii.gz\n",
            "🔍 Inference on [18] LUNG1-170.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (110, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (110, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-170.nii.gz\n",
            "🔍 Inference on [19] LUNG1-177.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (94, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (94, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-177.nii.gz\n",
            "🔍 Inference on [20] LUNG1-186.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (134, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (134, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-186.nii.gz\n",
            "🔍 Inference on [21] LUNG1-194.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (127, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (127, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-194.nii.gz\n",
            "🔍 Inference on [22] LUNG1-196.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (94, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (94, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-196.nii.gz\n",
            "🔍 Inference on [23] LUNG1-198.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (131, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (131, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-198.nii.gz\n",
            "🔍 Inference on [24] LUNG1-210.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (131, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (131, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-210.nii.gz\n",
            "🔍 Inference on [25] LUNG1-220.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (94, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (94, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-220.nii.gz\n",
            "🔍 Inference on [26] LUNG1-230.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (93, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (93, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-230.nii.gz\n",
            "🔍 Inference on [27] LUNG1-233.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (134, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (134, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-233.nii.gz\n",
            "🔍 Inference on [28] LUNG1-241.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (136, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (136, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-241.nii.gz\n",
            "🔍 Inference on [29] LUNG1-249.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (93, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (93, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-249.nii.gz\n",
            "🔍 Inference on [30] LUNG1-264.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (130, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (130, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-264.nii.gz\n",
            "🔍 Inference on [31] LUNG1-273.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (136, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (136, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-273.nii.gz\n",
            "🔍 Inference on [32] LUNG1-299.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (93, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (93, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-299.nii.gz\n",
            "🔍 Inference on [33] LUNG1-329.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (134, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (134, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-329.nii.gz\n",
            "🔍 Inference on [34] LUNG1-337.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (134, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (134, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-337.nii.gz\n",
            "🔍 Inference on [35] LUNG1-340.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (92, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (92, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-340.nii.gz\n",
            "🔍 Inference on [36] LUNG1-356.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (134, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (134, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-356.nii.gz\n",
            "🔍 Inference on [37] LUNG1-371.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (173, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (173, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-371.nii.gz\n",
            "🔍 Inference on [38] LUNG1-372.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (134, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (134, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-372.nii.gz\n",
            "🔍 Inference on [39] LUNG1-412.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (134, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (134, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-412.nii.gz\n",
            "🔍 Inference on [40] LUNG1-415.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (122, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (122, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-415.nii.gz\n",
            "🔍 Inference on [41] LUNG1-418.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (133, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (133, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-418.nii.gz\n",
            "🔍 Inference on [42] LUNG1-419.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (134, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (134, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-419.nii.gz\n",
            "🔍 Inference on [43] LUNG1-421.nii.gz | shape = torch.Size([1, 1, 96, 96, 96])\n",
            "🔍 Original volume shape = (134, 512, 512)\n",
            "✅ Predicted mask shape: torch.Size([1, 1, 96, 96, 96])\n",
            "✅ Batch shape: torch.Size([1, 96, 96, 96])\n",
            "✅ Inverted mask shape: (134, 512, 512)\n",
            "✅ Saved: /content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/valid_Predicted/segment/LUNG1-421.nii.gz\n",
            "🎉 Inference completed successfully for all patients!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKImglO4-twL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "import os\n",
        "import csv\n",
        "\n",
        "\n",
        "class LossPlotter:\n",
        "    def __init__(self, csv_path: str):\n",
        "        self.csv_path = Path(csv_path)\n",
        "        self.data = self._load_data()\n",
        "\n",
        "    def _load_data(self):\n",
        "        if not self.csv_path.exists():\n",
        "            raise FileNotFoundError(f\"CSV file not found: {self.csv_path}\")\n",
        "        df = pd.read_csv(self.csv_path, index_col=0)  # Read row labels as index\n",
        "        return df  # Make rows into columns\n",
        "\n",
        "    def plot(self, title: str = \"Training and Validation Loss\", save_path= None):\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.plot(self.data.index, self.data['Train Loss'], label='Train Loss', color='blue')\n",
        "        plt.plot(self.data.index, self.data['Valid Loss'], label='Valid Loss', color='orange')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title(title)\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            save_path = Path(save_path)\n",
        "            save_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            plt.savefig(save_path, format='pdf')\n",
        "            print(f\"[INFO] Loss plot saved to {save_path}\")\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "        plt.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    target_dir = \"/content/drive/MyDrive/PhDwork/Segmentation\"\n",
        "    os.chdir(target_dir)\n",
        "    loss_result_file = os.path.join(\".\",\"results\",f\"Results_PreProcessedCT_Fifty_Fifty_DiceLoss_And_Strong_Augmentation\",\"train_and_valid_loss_results.csv\")\n",
        "    plotter = LossPlotter(loss_result_file)\n",
        "    plotter.plot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyeB21BYGQPu"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "os.chdir(\"/content/drive/MyDrive/PhDwork/Segmentation\")\n",
        "print(f\"📁 Current Directory: {os.getcwd()}\")\n",
        "with h5py.File('./datasets/Datasets_PreprocessedCT_clipping_uniformSpacing_With_Empty_NonEmpty_slices_In_Train/train_dataset.hdf5', 'r') as f:\n",
        "    print(list(f.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ud1cFDGmKQBK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "authorship_tag": "ABX9TyPGKtWWW2qBnIi8zZpEVchu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}