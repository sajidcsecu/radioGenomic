{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sajidcsecu/radioGenomic/blob/main/3DUnetinGPU_(Nifti_MONAI).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr2QqHlZ8GZB"
      },
      "source": [
        "# This is the Code for the Segmentation on Rider Dataset (LUNG1). The Code is worked on the 3D volume over GPU. The balanced sampler, preprocessed data (uniform volume spacing and clipping [-1000, 700]) and the strong augmentation is used in the code..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9Zo7tkcI1CX"
      },
      "source": [
        "# (1) Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d9OVdEeKXpMe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cfe7feb1-42bf-4694-fdc4-743acf4e8320"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.11/dist-packages (2.5.2)\n",
            "Requirement already satisfied: pydicom===2.4.3 in /usr/local/lib/python3.11/dist-packages (2.4.3)\n",
            "Requirement already satisfied: pydicom-seg in /usr/local/lib/python3.11/dist-packages (0.4.1)\n",
            "Requirement already satisfied: SimpleITK>1.2.4 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (2.5.2)\n",
            "Requirement already satisfied: jsonschema<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (3.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (1.23.5)\n",
            "Requirement already satisfied: pydicom>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (2.4.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (25.3.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (0.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (75.2.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (1.17.0)\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n",
            "Collecting monai\n",
            "  Using cached monai-1.5.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting numpy<3.0,>=1.24 (from monai)\n",
            "  Using cached numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: torch<2.7.0,>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from monai) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<2.7.0,>=2.4.1->monai) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<2.7.0,>=2.4.1->monai) (3.0.2)\n",
            "Using cached monai-1.5.0-py3-none-any.whl (2.7 MB)\n",
            "Using cached numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl (16.9 MB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, monai\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydicom-seg 0.4.1 requires numpy<2.0.0,>=1.18.0, but you have numpy 2.3.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.1 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.1 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.1 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed monai-1.5.0 numpy-2.3.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "b2c2ce0a6e234e7c9c9cfcadd6e216c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.13.1\n",
            "  Using cached torch-1.13.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==1.13.1) (4.14.1)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.1)\n",
            "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.1)\n",
            "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.1)\n",
            "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.1)\n",
            "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.45.1)\n",
            "Using cached torch-1.13.1-cp311-cp311-manylinux1_x86_64.whl (887.4 MB)\n",
            "Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/317.1 MB\u001b[0m \u001b[31m148.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        }
      ],
      "source": [
        "!pip install SimpleITK\n",
        "!pip install pydicom===2.4.3\n",
        "!pip install pydicom-seg\n",
        "!pip install numpy==1.23.5\n",
        "!pip install monai\n",
        "!pip install torch==1.13.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JadHvjQcJ-qU"
      },
      "source": [
        "\n",
        "# (2) Import required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pmtDNjxMbfB4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import time\n",
        "import random\n",
        "import shutil\n",
        "from glob import glob\n",
        "from typing import List\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.cuda.amp as amp\n",
        "from torch.optim import lr_scheduler\n",
        "from monai.data import DataLoader, Dataset\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.transforms import AsDiscrete\n",
        "from monai.networks.nets import UNet\n",
        "from monai.transforms import (\n",
        "    Compose, LoadImaged, EnsureChannelFirstD, Spacingd,\n",
        "    Orientationd, ScaleIntensityRanged, CropForegroundd, Resized,\n",
        "    RandFlipd, RandFlipd, RandAffined, RandGaussianNoised, RandScaleIntensityd, ToTensord\n",
        ")\n",
        "from monai.data import Dataset, CacheDataset\n",
        "from monai.networks.layers import Norm\n",
        "import nibabel as nib\n",
        "from sklearn.metrics import jaccard_score, f1_score, recall_score, precision_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import multiprocessing as mp\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyzguRDWI9bM"
      },
      "source": [
        "# (3) Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq6jVaaMXZz5",
        "outputId": "1ada9252-e0cf-4577-903e-4a0921fa1cf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFrRJqgG7wxo"
      },
      "source": [
        "## (4). Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "944_4uJbmPPx"
      },
      "outputs": [],
      "source": [
        "class DiceBCELoss3D(nn.Module):\n",
        "    def __init__(self, smooth=1e-6, epsilon=1e-8):\n",
        "        super().__init__()\n",
        "        self.smooth = smooth\n",
        "        self.epsilon = epsilon\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def forward(self, preds, targets):\n",
        "        preds = preds.flatten()\n",
        "        targets = targets.flatten()\n",
        "        preds_sigmoid = torch.sigmoid(preds)\n",
        "        intersection = (preds_sigmoid * targets).sum()\n",
        "        dice_loss = 1 - (2. * intersection + self.smooth) / (\n",
        "            preds_sigmoid.sum() + targets.sum() + self.smooth + self.epsilon)\n",
        "        bce_loss = self.bce(preds, targets)\n",
        "        return dice_loss + bce_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZZ3Gu-DD88X"
      },
      "source": [
        "# (5). Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "T4VsKIzmFGLP"
      },
      "outputs": [],
      "source": [
        "class UnetTest:\n",
        "    def __init__(self, test_result_path: str, metrics_csv: str, device: torch.device):\n",
        "        self.test_result_path = test_result_path\n",
        "        self.metrics_csv = metrics_csv\n",
        "        self.device = device\n",
        "\n",
        "        os.makedirs(self.test_result_path, exist_ok=True)\n",
        "        self._init_metrics_csv()\n",
        "\n",
        "    def _init_metrics_csv(self):\n",
        "        if not os.path.exists(self.metrics_csv):\n",
        "            with open(self.metrics_csv, 'w', newline='') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([\"SampleID\", \"Jaccard\", \"F1\", \"Recall\", \"Precision\", \"Accuracy\", \"Time\"])\n",
        "\n",
        "    def calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray):\n",
        "        y_true = y_true.astype(bool).flatten()\n",
        "        y_pred = y_pred.astype(bool).flatten()\n",
        "\n",
        "        return [\n",
        "            jaccard_score(y_true, y_pred, zero_division=0),\n",
        "            f1_score(y_true, y_pred, zero_division=0),\n",
        "            recall_score(y_true, y_pred, zero_division=0),\n",
        "            precision_score(y_true, y_pred, zero_division=0),\n",
        "            accuracy_score(y_true, y_pred)\n",
        "        ]\n",
        "\n",
        "    def save_result_slices(self, image: np.ndarray, pred_mask: np.ndarray, true_mask: np.ndarray, sample_id: str):\n",
        "        sample_dir = os.path.join(self.test_result_path, sample_id)\n",
        "        os.makedirs(sample_dir, exist_ok=True)\n",
        "\n",
        "        for i in range(image.shape[0]):\n",
        "            try:\n",
        "                fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
        "                ax[0].imshow(image[i], cmap='gray')\n",
        "                ax[0].set_title('Image')\n",
        "\n",
        "                ax[1].imshow(true_mask[i], cmap='gray')\n",
        "                ax[1].set_title('Ground Truth')\n",
        "\n",
        "                ax[2].imshow(pred_mask[i], cmap='gray')\n",
        "                ax[2].set_title('Prediction')\n",
        "\n",
        "                for a in ax:\n",
        "                    a.axis('off')\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(sample_dir, f'slice_{i:03d}.png'))\n",
        "                plt.close()\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Could not save slice {i} for {sample_id}: {e}\")\n",
        "\n",
        "    def append_metrics_to_csv(self, sample_id: str, metrics: list, elapsed_time: float):\n",
        "        with open(self.metrics_csv, 'a', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([sample_id] + [f\"{m:.4f}\" for m in metrics] + [f\"{elapsed_time:.4f}\"])\n",
        "\n",
        "    def test(self, model: nn.Module, test_loader: DataLoader):\n",
        "        model.eval()\n",
        "        total_metrics = np.zeros(5)\n",
        "        total_times = []\n",
        "\n",
        "        roi_size = (96, 96, 96)\n",
        "        sw_batch_size = 1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(test_loader):\n",
        "                image, label = batch[\"vol\"].to(self.device), batch[\"seg\"].to(self.device)\n",
        "                start_time = time.time()\n",
        "\n",
        "                pred = sliding_window_inference(\n",
        "                    inputs=image,\n",
        "                    roi_size=roi_size,\n",
        "                    sw_batch_size=sw_batch_size,\n",
        "                    predictor=model\n",
        "                )\n",
        "                pred = torch.sigmoid(pred) > 0.5  # Binary thresholding\n",
        "\n",
        "                elapsed = time.time() - start_time\n",
        "                total_times.append(elapsed)\n",
        "\n",
        "                # Convert to NumPy\n",
        "                image_np = image[0, 0].cpu().numpy()\n",
        "                label_np = label[0, 0].cpu().numpy()\n",
        "                pred_np = pred[0, 0].cpu().numpy()\n",
        "\n",
        "                # Metrics\n",
        "                metrics = self.calculate_metrics(label_np, pred_np)\n",
        "                total_metrics += np.array(metrics)\n",
        "\n",
        "                # Identify sample name\n",
        "                sample_id = os.path.basename(batch[\"vol_meta_dict\"][\"filename_or_obj\"][0]).replace(\".nii.gz\", \"\")\n",
        "                self.save_result_slices(image_np, pred_np, label_np, sample_id)\n",
        "                self.append_metrics_to_csv(sample_id, metrics, elapsed)\n",
        "\n",
        "        # Print summary\n",
        "        num_samples = len(test_loader)\n",
        "        print(\"\\nüìä Average Test Metrics:\")\n",
        "        print(f\"Jaccard:  {total_metrics[0]/num_samples:.4f}\")\n",
        "        print(f\"F1:       {total_metrics[1]/num_samples:.4f}\")\n",
        "        print(f\"Recall:   {total_metrics[2]/num_samples:.4f}\")\n",
        "        print(f\"Precision:{total_metrics[3]/num_samples:.4f}\")\n",
        "        print(f\"Accuracy: {total_metrics[4]/num_samples:.4f}\")\n",
        "        print(f\"‚ö° FPS:    {1 / np.mean(total_times):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa8L5nD2EVP_"
      },
      "source": [
        "# (6). Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "281KQS_iEIDX"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, verbose=True, min_delta=0, path='checkpoint.pt',\n",
        "                 start_val_loss_min=None, start_patience_counter=0):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.min_delta = min_delta\n",
        "        self.path = path\n",
        "        self.val_loss_min = start_val_loss_min if start_val_loss_min is not None else np.inf\n",
        "        self.counter = start_patience_counter\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss, model, epoch=None, optimizer=None):\n",
        "        improved = False\n",
        "        if val_loss < self.val_loss_min - self.min_delta:\n",
        "            self.val_loss_min = val_loss\n",
        "            self.counter = 0\n",
        "            improved = True\n",
        "            if self.verbose:\n",
        "                print(f\"‚úÖ Validation loss improved. Saving model...\")\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f\"‚è≥ EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "\n",
        "        # ‚úÖ Always save full checkpoint (model + optimizer + val_loss + patience_counter)\n",
        "        self.save_checkpoint(model, epoch, optimizer)\n",
        "\n",
        "        if self.counter >= self.patience:\n",
        "            self.early_stop = True\n",
        "\n",
        "        return self.early_stop\n",
        "\n",
        "    def save_checkpoint(self, model, epoch=None, optimizer=None):\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict() if optimizer else None,\n",
        "            'val_loss': self.val_loss_min,\n",
        "            'patience_counter': self.counter\n",
        "        }\n",
        "        torch.save(checkpoint, self.path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class UnetTrain:\n",
        "    def __init__(self, model_file, loss_result_path, lr, num_epochs, device):\n",
        "        self.model_file = model_file\n",
        "        self.loss_result_path = loss_result_path\n",
        "        self.lr = lr\n",
        "        self.num_epochs = num_epochs\n",
        "        self.device = device\n",
        "        self.seeding(42)\n",
        "\n",
        "    def seeding(self, seed):\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    def epoch_time(self, start_time, end_time):\n",
        "        elapsed = end_time - start_time\n",
        "        return int(elapsed / 60), int(elapsed % 60)\n",
        "\n",
        "    def train_one_epoch(self, model, loader, optimizer, loss_fn):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        scaler = torch.amp.GradScaler()  # no device_type here\n",
        "\n",
        "        device_type = 'cuda' if self.device.type == 'cuda' else 'cpu'\n",
        "\n",
        "        for x in loader:\n",
        "            inputs, labels = x[\"vol\"].to(self.device), x[\"seg\"].to(self.device)\n",
        "            optimizer.zero_grad()\n",
        "            with torch.amp.autocast(device_type=device_type):\n",
        "                outputs = model(inputs)\n",
        "                loss = loss_fn(outputs, labels)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        return epoch_loss / len(loader)\n",
        "\n",
        "    def evaluate(self, model, loader, loss_fn):\n",
        "        model.eval()\n",
        "        epoch_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for x in loader:\n",
        "                inputs, labels = x[\"vol\"].to(self.device), x[\"seg\"].to(self.device)\n",
        "                outputs = model(inputs)\n",
        "                loss = loss_fn(outputs, labels)\n",
        "                epoch_loss += loss.item()\n",
        "        return epoch_loss / len(loader)\n",
        "\n",
        "    def execute(self, train_loader, valid_loader):\n",
        "        model = UNet(\n",
        "            spatial_dims=3,\n",
        "            in_channels=1,\n",
        "            out_channels=1,\n",
        "            channels=(16, 32, 64, 128, 256),\n",
        "            strides=(2, 2, 2, 2),\n",
        "            num_res_units=2,\n",
        "            norm=Norm.BATCH\n",
        "        ).to(self.device)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=self.lr, weight_decay=1e-5)\n",
        "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "        loss_fn = DiceBCELoss3D()\n",
        "\n",
        "        # Initialize state\n",
        "        start_epoch = 1\n",
        "        start_val_loss_min = None\n",
        "        start_patience_counter = 0\n",
        "        history = {\"train_loss\": [], \"valid_loss\": []}\n",
        "\n",
        "        # üì¶ Restore from model checkpoint if exists\n",
        "        if os.path.exists(self.model_file):\n",
        "            checkpoint = torch.load(self.model_file, map_location=self.device)\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            if checkpoint.get('optimizer_state_dict'):\n",
        "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            start_epoch = checkpoint.get('epoch', 1) + 1\n",
        "            start_val_loss_min = checkpoint.get('val_loss', None)\n",
        "            start_patience_counter = checkpoint.get('patience_counter', 0)\n",
        "\n",
        "        # üìä Restore training history from loss CSV\n",
        "        if os.path.exists(self.loss_result_path):\n",
        "            with open(self.loss_result_path, 'r') as f:\n",
        "                reader = csv.reader(f)\n",
        "                next(reader)\n",
        "                rows = list(reader)\n",
        "                if rows:\n",
        "                    last_epoch = int(rows[-1][0])\n",
        "                    start_epoch = last_epoch + 1\n",
        "                    history['train_loss'] = [float(r[1]) for r in rows]\n",
        "                    history['valid_loss'] = [float(r[2]) for r in rows]\n",
        "                    if start_val_loss_min is None:\n",
        "                        start_val_loss_min = min(history['valid_loss'])\n",
        "\n",
        "            # üíæ Make a backup copy\n",
        "            backup_path = self.loss_result_path.replace(\".csv\", \"_backup.csv\")\n",
        "            shutil.copy(self.loss_result_path, backup_path)\n",
        "\n",
        "        # üõë Initialize EarlyStopping\n",
        "        early_stopping = EarlyStopping(\n",
        "            patience=10,\n",
        "            min_delta=0.0005,\n",
        "            path=self.model_file,\n",
        "            start_val_loss_min=start_val_loss_min,\n",
        "            start_patience_counter=start_patience_counter\n",
        "        )\n",
        "\n",
        "        # üìÅ If loss file not present, create CSV header\n",
        "        if not os.path.exists(self.loss_result_path):\n",
        "            with open(self.loss_result_path, \"w\", newline=\"\") as f:\n",
        "                csv.writer(f).writerow([\"Epoch\", \"Train Loss\", \"Valid Loss\"])\n",
        "\n",
        "        # üöÇ Training Loop\n",
        "        for epoch in range(start_epoch, self.num_epochs + 1):\n",
        "            start_time = time.time()\n",
        "\n",
        "            train_loss = self.train_one_epoch(model, train_loader, optimizer, loss_fn)\n",
        "            valid_loss = self.evaluate(model, valid_loader, loss_fn)\n",
        "            scheduler.step()\n",
        "\n",
        "            epoch_mins, epoch_secs = self.epoch_time(start_time, time.time())\n",
        "            print(f\"Epoch {epoch:03d} | Time: {epoch_mins}m {epoch_secs}s | \"\n",
        "                  f\"Train: {train_loss:.6f} | Val: {valid_loss:.6f}\")\n",
        "\n",
        "            history['train_loss'].append(train_loss)\n",
        "            history['valid_loss'].append(valid_loss)\n",
        "\n",
        "            with open(self.loss_result_path, \"a\", newline=\"\") as f:\n",
        "                csv.writer(f).writerow([epoch, train_loss, valid_loss])\n",
        "\n",
        "            # üõë Early stopping check\n",
        "            if early_stopping(valid_loss, model, epoch, optimizer):\n",
        "                print(\"üõë Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "            torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (7). Pipeline"
      ],
      "metadata": {
        "id": "BZrW1cHg4OlM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwcTHPshqu0O",
        "outputId": "826b5d3e-af03-4738-e8e2-cfeead5a08c6"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ Loading datasets...\n",
            "Epoch 071 | Time: 71m 35s | Train: 0.674845 | Val: 0.805939\n",
            "‚è≥ EarlyStopping counter: 2 out of 10\n",
            "Epoch 072 | Time: 68m 22s | Train: 0.711546 | Val: 0.810964\n",
            "‚è≥ EarlyStopping counter: 3 out of 10\n",
            "Epoch 073 | Time: 68m 42s | Train: 0.706732 | Val: 0.791821\n",
            "‚è≥ EarlyStopping counter: 4 out of 10\n",
            "Epoch 074 | Time: 68m 55s | Train: 0.681764 | Val: 0.789887\n",
            "‚è≥ EarlyStopping counter: 5 out of 10\n",
            "Epoch 075 | Time: 69m 1s | Train: 0.665208 | Val: 0.781649\n",
            "‚úÖ Validation loss improved. Saving model...\n",
            "Epoch 076 | Time: 69m 7s | Train: 0.641370 | Val: 0.784236\n",
            "‚è≥ EarlyStopping counter: 1 out of 10\n",
            "Epoch 077 | Time: 69m 18s | Train: 0.660566 | Val: 0.789251\n",
            "‚è≥ EarlyStopping counter: 2 out of 10\n",
            "Epoch 078 | Time: 69m 23s | Train: 0.639254 | Val: 0.780697\n",
            "‚úÖ Validation loss improved. Saving model...\n",
            "Epoch 079 | Time: 69m 26s | Train: 0.631953 | Val: 0.774602\n",
            "‚úÖ Validation loss improved. Saving model...\n",
            "Epoch 080 | Time: 69m 35s | Train: 0.628322 | Val: 0.770767\n",
            "‚úÖ Validation loss improved. Saving model...\n",
            "Epoch 081 | Time: 69m 34s | Train: 0.663994 | Val: 0.798359\n",
            "‚è≥ EarlyStopping counter: 1 out of 10\n",
            "Epoch 082 | Time: 69m 34s | Train: 0.663142 | Val: 0.796649\n",
            "‚è≥ EarlyStopping counter: 2 out of 10\n",
            "Epoch 083 | Time: 69m 40s | Train: 0.670170 | Val: 0.752426\n",
            "‚úÖ Validation loss improved. Saving model...\n",
            "Epoch 084 | Time: 69m 44s | Train: 0.654404 | Val: 0.807204\n",
            "‚è≥ EarlyStopping counter: 1 out of 10\n",
            "Epoch 085 | Time: 69m 44s | Train: 0.641575 | Val: 0.787742\n",
            "‚è≥ EarlyStopping counter: 2 out of 10\n",
            "Epoch 086 | Time: 69m 49s | Train: 0.657449 | Val: 0.757477\n",
            "‚è≥ EarlyStopping counter: 3 out of 10\n",
            "Epoch 087 | Time: 69m 48s | Train: 0.638449 | Val: 0.815259\n",
            "‚è≥ EarlyStopping counter: 4 out of 10\n",
            "Epoch 088 | Time: 69m 50s | Train: 0.643433 | Val: 0.777593\n",
            "‚è≥ EarlyStopping counter: 5 out of 10\n",
            "Epoch 089 | Time: 69m 47s | Train: 0.637775 | Val: 0.783900\n",
            "‚è≥ EarlyStopping counter: 6 out of 10\n",
            "Epoch 090 | Time: 69m 48s | Train: 0.624687 | Val: 0.788911\n",
            "‚è≥ EarlyStopping counter: 7 out of 10\n"
          ]
        }
      ],
      "source": [
        "class UnetPipeline:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.setup_paths()\n",
        "        print(\"üì¶ Loading datasets...\")\n",
        "        self.train_loader, self.valid_loader, self.test_loader = self.prepare_loaders()\n",
        "\n",
        "    def setup_paths(self):\n",
        "        os.chdir(self.config['target_dir'])\n",
        "        self.output_dir = os.path.join(\".\", \"results\", self.config['output_folder_name'])\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "        self.loss_result_file = os.path.join(self.output_dir, \"train_and_valid_loss_results.csv\")\n",
        "        self.model_file = os.path.join(self.output_dir, \"model.pth\")\n",
        "        self.test_metrics_file = os.path.join(self.output_dir, \"test_metrics.csv\")\n",
        "        self.test_result_path = os.path.join(self.output_dir, \"test_outputs\")\n",
        "        os.makedirs(self.test_result_path, exist_ok=True)\n",
        "\n",
        "        self.dataset_dir = os.path.join(\"./datasets\", f\"Datasets_{self.config['transformation']}\")\n",
        "\n",
        "    def prepare_loaders(self):\n",
        "        from glob import glob\n",
        "        from monai.transforms import (\n",
        "            Compose, LoadImaged, EnsureChannelFirstD, Spacingd, Orientationd,\n",
        "            ScaleIntensityRanged, CropForegroundd, Resized, ToTensord,\n",
        "            RandFlipd, RandAffined, RandGaussianNoised, RandScaleIntensityd\n",
        "        )\n",
        "        from monai.data import Dataset, DataLoader\n",
        "\n",
        "        pixdim = (1, 1, 1)\n",
        "        a_min, a_max = -1000, 700\n",
        "        spatial_size = (96, 96, 96)\n",
        "\n",
        "        def get_files(split):\n",
        "            ct = sorted(glob(os.path.join(self.dataset_dir, split, \"ct\", \"*.nii.gz\")))\n",
        "            seg = sorted(glob(os.path.join(self.dataset_dir, split, \"segment\", \"*.nii.gz\")))\n",
        "            return [{\"vol\": c, \"seg\": s} for c, s in zip(ct, seg)]\n",
        "\n",
        "        # Training transforms with augmentation\n",
        "        train_transforms = Compose([\n",
        "            LoadImaged(keys=[\"vol\", \"seg\"]),\n",
        "            EnsureChannelFirstD(keys=[\"vol\", \"seg\"]),\n",
        "            Spacingd(keys=[\"vol\", \"seg\"], pixdim=pixdim, mode=(\"bilinear\", \"nearest\")),\n",
        "            Orientationd(keys=[\"vol\", \"seg\"], axcodes=\"RAS\"),\n",
        "            ScaleIntensityRanged(keys=[\"vol\"], a_min=a_min, a_max=a_max, b_min=0.0, b_max=1.0, clip=True),\n",
        "            CropForegroundd(keys=[\"vol\", \"seg\"], source_key=\"vol\"),\n",
        "            Resized(keys=[\"vol\", \"seg\"], spatial_size=spatial_size),\n",
        "\n",
        "            # ‚ûï Augmentations\n",
        "            RandFlipd(keys=[\"vol\", \"seg\"], prob=0.5, spatial_axis=0),\n",
        "            RandFlipd(keys=[\"vol\", \"seg\"], prob=0.5, spatial_axis=1),\n",
        "            RandAffined(\n",
        "                keys=[\"vol\", \"seg\"],\n",
        "                prob=0.3,\n",
        "                rotate_range=(0.1, 0.1, 0.1),\n",
        "                scale_range=(0.1, 0.1, 0.1),\n",
        "                mode=(\"bilinear\", \"nearest\")\n",
        "            ),\n",
        "            RandGaussianNoised(keys=[\"vol\"], prob=0.2, mean=0.0, std=0.1),\n",
        "            RandScaleIntensityd(keys=[\"vol\"], factors=0.1, prob=0.5),\n",
        "\n",
        "            ToTensord(keys=[\"vol\", \"seg\"])\n",
        "        ])\n",
        "\n",
        "        # Validation/test transforms without augmentation\n",
        "        base_transforms = Compose([\n",
        "            LoadImaged(keys=[\"vol\", \"seg\"]),\n",
        "            EnsureChannelFirstD(keys=[\"vol\", \"seg\"]),\n",
        "            Spacingd(keys=[\"vol\", \"seg\"], pixdim=pixdim, mode=(\"bilinear\", \"nearest\")),\n",
        "            Orientationd(keys=[\"vol\", \"seg\"], axcodes=\"RAS\"),\n",
        "            ScaleIntensityRanged(keys=[\"vol\"], a_min=a_min, a_max=a_max, b_min=0.0, b_max=1.0, clip=True),\n",
        "            CropForegroundd(keys=[\"vol\", \"seg\"], source_key=\"vol\"),\n",
        "            Resized(keys=[\"vol\", \"seg\"], spatial_size=spatial_size),\n",
        "            ToTensord(keys=[\"vol\", \"seg\"])\n",
        "        ])\n",
        "\n",
        "        train_loader = DataLoader(Dataset(get_files(\"train\"), train_transforms), batch_size=self.config['batch_size'], shuffle=True)\n",
        "        valid_loader = DataLoader(Dataset(get_files(\"valid\"), base_transforms), batch_size=self.config['batch_size'])\n",
        "        test_loader = DataLoader(Dataset(get_files(\"test\"), base_transforms), batch_size=1)\n",
        "\n",
        "        return train_loader, valid_loader, test_loader\n",
        "\n",
        "    def train(self):\n",
        "        trainer = UnetTrain(\n",
        "            model_file=self.model_file,\n",
        "            loss_result_path=self.loss_result_file,\n",
        "            lr=self.config['learning_rate'],\n",
        "            num_epochs=self.config['num_epochs'],\n",
        "            device=self.device\n",
        "        )\n",
        "        trainer.execute(self.train_loader, self.valid_loader)\n",
        "\n",
        "    def test(self):\n",
        "        model = UNet(\n",
        "            spatial_dims=3,\n",
        "            in_channels=1,\n",
        "            out_channels=1,\n",
        "            channels=(16, 32, 64, 128, 256),\n",
        "            strides=(2, 2, 2, 2),\n",
        "            num_res_units=2,\n",
        "            norm=Norm.BATCH\n",
        "        ).to(self.device)\n",
        "        checkpoint = torch.load(self.model_file, map_location=self.device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "        tester = UnetTest(self.test_result_path, self.test_metrics_file, self.device)\n",
        "        tester.test(model, self.test_loader)\n",
        "\n",
        "    def run(self):\n",
        "        self.train()\n",
        "        self.test()\n",
        "\n",
        "\n",
        "def main():\n",
        "    config = {\n",
        "        'target_dir': \"/content/drive/MyDrive/PhDwork/Segmentation\",\n",
        "        'output_folder_name': \"Results_MONAI_Augmented\",\n",
        "        'transformation': \"OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\",\n",
        "        'batch_size': 2,\n",
        "        'num_epochs': 100,\n",
        "        'learning_rate': 1e-4,\n",
        "    }\n",
        "    pipeline = UnetPipeline(config)\n",
        "    pipeline.run()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import multiprocessing as mp\n",
        "    mp.set_start_method('spawn')\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKImglO4-twL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "import os\n",
        "import csv\n",
        "\n",
        "\n",
        "class LossPlotter:\n",
        "    def __init__(self, csv_path: str):\n",
        "        self.csv_path = Path(csv_path)\n",
        "        self.data = self._load_data()\n",
        "\n",
        "    def _load_data(self):\n",
        "        if not self.csv_path.exists():\n",
        "            raise FileNotFoundError(f\"CSV file not found: {self.csv_path}\")\n",
        "        df = pd.read_csv(self.csv_path, index_col=0)  # Read row labels as index\n",
        "        return df  # Make rows into columns\n",
        "\n",
        "    def plot(self, title: str = \"Training and Validation Loss\", save_path= None):\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.plot(self.data.index, self.data['Train Loss'], label='Train Loss', color='blue')\n",
        "        plt.plot(self.data.index, self.data['Valid Loss'], label='Valid Loss', color='orange')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title(title)\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            save_path = Path(save_path)\n",
        "            save_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            plt.savefig(save_path, format='pdf')\n",
        "            print(f\"[INFO] Loss plot saved to {save_path}\")\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "        plt.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    target_dir = \"/content/drive/MyDrive/PhDwork/Segmentation\"\n",
        "    os.chdir(target_dir)\n",
        "    loss_result_file = os.path.join(\".\",\"results\",f\"Results_PreProcessedCT_Fifty_Fifty_DiceLoss_And_Strong_Augmentation\",\"train_and_valid_loss_results.csv\")\n",
        "    plotter = LossPlotter(loss_result_file)\n",
        "    plotter.plot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyeB21BYGQPu"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "os.chdir(\"/content/drive/MyDrive/PhDwork/Segmentation\")\n",
        "print(f\"üìÅ Current Directory: {os.getcwd()}\")\n",
        "with h5py.File('./datasets/Datasets_PreprocessedCT_clipping_uniformSpacing_With_Empty_NonEmpty_slices_In_Train/train_dataset.hdf5', 'r') as f:\n",
        "    print(list(f.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ud1cFDGmKQBK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyM0p1QHiRrAcIChWj46Q/LI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}