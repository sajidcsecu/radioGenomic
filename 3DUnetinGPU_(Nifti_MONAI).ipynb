{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sajidcsecu/radioGenomic/blob/main/3DUnetinGPU_(Nifti_MONAI).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr2QqHlZ8GZB"
      },
      "source": [
        "# This is the Code for the Segmentation on Rider Dataset (LUNG1). The Code is worked on the 3D volume over GPU. The balanced sampler, preprocessed data (uniform volume spacing and clipping [-1000, 700]) and the strong augmentation is used in the code..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9Zo7tkcI1CX"
      },
      "source": [
        "# (1) Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9OVdEeKXpMe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "447b23cd-160e-4332-f595-457323b677c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.11/dist-packages (2.5.2)\n",
            "Requirement already satisfied: pydicom===2.4.3 in /usr/local/lib/python3.11/dist-packages (2.4.3)\n",
            "Requirement already satisfied: pydicom-seg in /usr/local/lib/python3.11/dist-packages (0.4.1)\n",
            "Requirement already satisfied: SimpleITK>1.2.4 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (2.5.2)\n",
            "Requirement already satisfied: jsonschema<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (3.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (1.23.5)\n",
            "Requirement already satisfied: pydicom>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (2.4.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (25.3.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (0.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (75.2.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (1.17.0)\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n",
            "Collecting monai\n",
            "  Using cached monai-1.5.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting numpy<3.0,>=1.24 (from monai)\n",
            "  Using cached numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: torch<2.7.0,>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from monai) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<2.7.0,>=2.4.1->monai) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<2.7.0,>=2.4.1->monai) (3.0.2)\n",
            "Downloading monai-1.5.0-py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, monai\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydicom-seg 0.4.1 requires numpy<2.0.0,>=1.18.0, but you have numpy 2.3.2 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.2 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.2 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed monai-1.5.0 numpy-2.3.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "247212035aa74e39816ec5d2b27a7fa4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.13.1\n",
            "  Downloading torch-1.13.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==1.13.1) (4.14.1)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.1)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.1)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.1)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.45.1)\n",
            "Downloading torch-1.13.1-cp311-cp311-manylinux1_x86_64.whl (887.4 MB)\n",
            "\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m225.5/887.4 MB\u001b[0m \u001b[31m180.4 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m^C\n"
          ]
        }
      ],
      "source": [
        "!pip install SimpleITK\n",
        "!pip install pydicom===2.4.3\n",
        "!pip install pydicom-seg\n",
        "!pip install numpy==1.23.5\n",
        "!pip install monai\n",
        "!pip install torch==1.13.1\n",
        "!pip install nibabel>=5.0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JadHvjQcJ-qU"
      },
      "source": [
        "\n",
        "# (2) Import required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pmtDNjxMbfB4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import csv\n",
        "import time\n",
        "import random\n",
        "import shutil\n",
        "from glob import glob\n",
        "from typing import List\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.cuda.amp as amp\n",
        "from torch.optim import lr_scheduler\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.transforms import AsDiscrete\n",
        "from monai.networks.nets import UNet\n",
        "from monai.transforms import (\n",
        "    Compose, LoadImaged, EnsureChannelFirstd, Spacingd,\n",
        "    Orientationd, ScaleIntensityRanged, CropForegroundd, Resized,\n",
        "    RandFlipd, RandFlipd, RandAffined, RandGaussianNoised, RandScaleIntensityd,EnsureTyped, ToTensord\n",
        ")\n",
        "from monai.data import Dataset, DataLoader, CacheDataset, pad_list_data_collate\n",
        "from monai.networks.layers import Norm\n",
        "import nibabel as nib\n",
        "from sklearn.metrics import jaccard_score, f1_score, recall_score, precision_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import multiprocessing as mp\n",
        "from monai.transforms import EnsureTyped\n",
        "from monai.transforms import SaveImaged\n",
        "from monai.utils import set_determinism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyzguRDWI9bM"
      },
      "source": [
        "# (3) Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq6jVaaMXZz5",
        "outputId": "dda19947-3215-4a43-955b-89d3bd8ec92d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFrRJqgG7wxo"
      },
      "source": [
        "## (4). Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "944_4uJbmPPx"
      },
      "outputs": [],
      "source": [
        "class DiceBCELoss3D(nn.Module):\n",
        "    def __init__(self, smooth=1e-6, epsilon=1e-8):\n",
        "        super().__init__()\n",
        "        self.smooth = smooth\n",
        "        self.epsilon = epsilon\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def forward(self, preds, targets):\n",
        "        preds = preds.flatten()\n",
        "        targets = targets.flatten()\n",
        "        preds_sigmoid = torch.sigmoid(preds)\n",
        "        intersection = (preds_sigmoid * targets).sum()\n",
        "        dice_loss = 1 - (2. * intersection + self.smooth) / (\n",
        "            preds_sigmoid.sum() + targets.sum() + self.smooth + self.epsilon)\n",
        "        bce_loss = self.bce(preds, targets)\n",
        "        return dice_loss + bce_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZZ3Gu-DD88X"
      },
      "source": [
        "# (5). Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4VsKIzmFGLP"
      },
      "outputs": [],
      "source": [
        "class UnetTest:\n",
        "    def __init__(self, test_result_path: str, metrics_csv: str, device: torch.device):\n",
        "        self.test_result_path = test_result_path\n",
        "        self.metrics_csv = metrics_csv\n",
        "        self.device = device\n",
        "\n",
        "        os.makedirs(self.test_result_path, exist_ok=True)\n",
        "        self._init_metrics_csv()\n",
        "\n",
        "    def _init_metrics_csv(self):\n",
        "        if not os.path.exists(self.metrics_csv):\n",
        "            with open(self.metrics_csv, 'w', newline='') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([\"SampleID\", \"Jaccard\", \"F1\", \"Recall\", \"Precision\", \"Accuracy\", \"Time\"])\n",
        "\n",
        "    def calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray):\n",
        "        y_true = y_true.astype(bool).flatten()\n",
        "        y_pred = y_pred.astype(bool).flatten()\n",
        "\n",
        "        return [\n",
        "            jaccard_score(y_true, y_pred, zero_division=0),\n",
        "            f1_score(y_true, y_pred, zero_division=0),\n",
        "            recall_score(y_true, y_pred, zero_division=0),\n",
        "            precision_score(y_true, y_pred, zero_division=0),\n",
        "            accuracy_score(y_true, y_pred)\n",
        "        ]\n",
        "\n",
        "    def save_result_slices(self, image: np.ndarray, pred_mask: np.ndarray, true_mask: np.ndarray, sample_id: str):\n",
        "        sample_dir = os.path.join(self.test_result_path, sample_id)\n",
        "        os.makedirs(sample_dir, exist_ok=True)\n",
        "\n",
        "        for i in range(image.shape[0]):\n",
        "            try:\n",
        "                fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
        "                ax[0].imshow(image[i], cmap='gray')\n",
        "                ax[0].set_title('Image')\n",
        "\n",
        "                ax[1].imshow(true_mask[i], cmap='gray')\n",
        "                ax[1].set_title('Ground Truth')\n",
        "\n",
        "                ax[2].imshow(pred_mask[i], cmap='gray')\n",
        "                ax[2].set_title('Prediction')\n",
        "\n",
        "                for a in ax:\n",
        "                    a.axis('off')\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(sample_dir, f'slice_{i:03d}.png'))\n",
        "                plt.close()\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Could not save slice {i} for {sample_id}: {e}\")\n",
        "\n",
        "    def append_metrics_to_csv(self, sample_id: str, metrics: list, elapsed_time: float):\n",
        "        with open(self.metrics_csv, 'a', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([sample_id] + [f\"{m:.4f}\" for m in metrics] + [f\"{elapsed_time:.4f}\"])\n",
        "\n",
        "    def test(self, model: nn.Module, test_loader: DataLoader):\n",
        "        model.eval()\n",
        "        total_metrics = np.zeros(5)\n",
        "        total_times = []\n",
        "\n",
        "        roi_size = (96, 96, 96)\n",
        "        sw_batch_size = 1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(test_loader):\n",
        "                image, label = batch[\"vol\"].to(self.device), batch[\"seg\"].to(self.device)\n",
        "                start_time = time.time()\n",
        "\n",
        "                pred = sliding_window_inference(\n",
        "                    inputs=image,\n",
        "                    roi_size=roi_size,\n",
        "                    sw_batch_size=sw_batch_size,\n",
        "                    predictor=model\n",
        "                )\n",
        "                pred = torch.sigmoid(pred) > 0.5  # Binary thresholding\n",
        "\n",
        "                elapsed = time.time() - start_time\n",
        "                total_times.append(elapsed)\n",
        "\n",
        "                # Convert to NumPy\n",
        "                image_np = image[0, 0].cpu().numpy()\n",
        "                label_np = label[0, 0].cpu().numpy()\n",
        "                pred_np = pred[0, 0].cpu().numpy()\n",
        "\n",
        "                # Metrics\n",
        "                metrics = self.calculate_metrics(label_np, pred_np)\n",
        "                total_metrics += np.array(metrics)\n",
        "\n",
        "                # Identify sample name\n",
        "                sample_id = os.path.basename(batch[\"vol_meta_dict\"][\"filename_or_obj\"][0]).replace(\".nii.gz\", \"\")\n",
        "                self.save_result_slices(image_np, pred_np, label_np, sample_id)\n",
        "                self.append_metrics_to_csv(sample_id, metrics, elapsed)\n",
        "\n",
        "        # Print summary\n",
        "        num_samples = len(test_loader)\n",
        "        print(\"\\nğŸ“Š Average Test Metrics:\")\n",
        "        print(f\"Jaccard:  {total_metrics[0]/num_samples:.4f}\")\n",
        "        print(f\"F1:       {total_metrics[1]/num_samples:.4f}\")\n",
        "        print(f\"Recall:   {total_metrics[2]/num_samples:.4f}\")\n",
        "        print(f\"Precision:{total_metrics[3]/num_samples:.4f}\")\n",
        "        print(f\"Accuracy: {total_metrics[4]/num_samples:.4f}\")\n",
        "        print(f\"âš¡ FPS:    {1 / np.mean(total_times):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa8L5nD2EVP_"
      },
      "source": [
        "# (6). Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "281KQS_iEIDX"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, verbose=True, min_delta=0, path='checkpoint.pt',\n",
        "                 start_val_loss_min=None, start_patience_counter=0):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.min_delta = min_delta\n",
        "        self.path = path\n",
        "        self.val_loss_min = start_val_loss_min if start_val_loss_min is not None else np.inf\n",
        "        self.counter = start_patience_counter\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss, model, epoch=None, optimizer=None):\n",
        "        improved = False\n",
        "        if val_loss < self.val_loss_min - self.min_delta:\n",
        "            self.val_loss_min = val_loss\n",
        "            self.counter = 0\n",
        "            improved = True\n",
        "            if self.verbose:\n",
        "                print(f\"âœ… Validation loss improved. Saving model...\")\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f\"â³ EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "\n",
        "        # âœ… Always save full checkpoint (model + optimizer + val_loss + patience_counter)\n",
        "        self.save_checkpoint(model, epoch, optimizer)\n",
        "\n",
        "        if self.counter >= self.patience:\n",
        "            self.early_stop = True\n",
        "\n",
        "        return self.early_stop\n",
        "\n",
        "    def save_checkpoint(self, model, epoch=None, optimizer=None):\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict() if optimizer else None,\n",
        "            'val_loss': self.val_loss_min,\n",
        "            'patience_counter': self.counter\n",
        "        }\n",
        "        torch.save(checkpoint, self.path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class UnetTrain:\n",
        "    def __init__(self, model_file, loss_result_path, lr, num_epochs, device):\n",
        "        self.model_file = model_file\n",
        "        self.loss_result_path = loss_result_path\n",
        "        self.lr = lr\n",
        "        self.num_epochs = num_epochs\n",
        "        self.device = device\n",
        "        self.seeding(42)\n",
        "\n",
        "    def seeding(self, seed):\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    def epoch_time(self, start_time, end_time):\n",
        "        elapsed = end_time - start_time\n",
        "        return int(elapsed / 60), int(elapsed % 60)\n",
        "\n",
        "    def train_one_epoch(self, model, loader, optimizer, loss_fn):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        scaler = torch.amp.GradScaler()  # no device_type here\n",
        "\n",
        "        device_type = 'cuda' if self.device.type == 'cuda' else 'cpu'\n",
        "\n",
        "        for x in loader:\n",
        "            inputs, labels = x[\"vol\"].to(self.device), x[\"seg\"].to(self.device)\n",
        "            optimizer.zero_grad()\n",
        "            with torch.amp.autocast(device_type=device_type):\n",
        "                outputs = model(inputs)\n",
        "                loss = loss_fn(outputs, labels)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        return epoch_loss / len(loader)\n",
        "\n",
        "    def evaluate(self, model, loader, loss_fn):\n",
        "        model.eval()\n",
        "        epoch_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for x in loader:\n",
        "                inputs, labels = x[\"vol\"].to(self.device), x[\"seg\"].to(self.device)\n",
        "                outputs = model(inputs)\n",
        "                loss = loss_fn(outputs, labels)\n",
        "                epoch_loss += loss.item()\n",
        "        return epoch_loss / len(loader)\n",
        "\n",
        "    def execute(self, train_loader, valid_loader):\n",
        "        model = UNet(\n",
        "            spatial_dims=3,\n",
        "            in_channels=1,\n",
        "            out_channels=1,\n",
        "            channels=(16, 32, 64, 128, 256),\n",
        "            strides=(2, 2, 2, 2),\n",
        "            num_res_units=2,\n",
        "            norm=Norm.BATCH\n",
        "        ).to(self.device)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=self.lr, weight_decay=1e-5)\n",
        "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "        loss_fn = DiceBCELoss3D()\n",
        "\n",
        "        # Initialize state\n",
        "        start_epoch = 1\n",
        "        start_val_loss_min = None\n",
        "        start_patience_counter = 0\n",
        "        history = {\"train_loss\": [], \"valid_loss\": []}\n",
        "\n",
        "        # ğŸ“¦ Restore from model checkpoint if exists\n",
        "        if os.path.exists(self.model_file):\n",
        "            checkpoint = torch.load(self.model_file, map_location=self.device)\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            if checkpoint.get('optimizer_state_dict'):\n",
        "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            start_epoch = checkpoint.get('epoch', 1) + 1\n",
        "            start_val_loss_min = checkpoint.get('val_loss', None)\n",
        "            start_patience_counter = checkpoint.get('patience_counter', 0)\n",
        "\n",
        "        # ğŸ“Š Restore training history from loss CSV\n",
        "        if os.path.exists(self.loss_result_path):\n",
        "            with open(self.loss_result_path, 'r') as f:\n",
        "                reader = csv.reader(f)\n",
        "                next(reader)\n",
        "                rows = list(reader)\n",
        "                if rows:\n",
        "                    last_epoch = int(rows[-1][0])\n",
        "                    start_epoch = last_epoch + 1\n",
        "                    history['train_loss'] = [float(r[1]) for r in rows]\n",
        "                    history['valid_loss'] = [float(r[2]) for r in rows]\n",
        "                    if start_val_loss_min is None:\n",
        "                        start_val_loss_min = min(history['valid_loss'])\n",
        "\n",
        "            # ğŸ’¾ Make a backup copy\n",
        "            backup_path = self.loss_result_path.replace(\".csv\", \"_backup.csv\")\n",
        "            shutil.copy(self.loss_result_path, backup_path)\n",
        "\n",
        "        # ğŸ›‘ Initialize EarlyStopping\n",
        "        early_stopping = EarlyStopping(\n",
        "            patience=10,\n",
        "            min_delta=0.0005,\n",
        "            path=self.model_file,\n",
        "            start_val_loss_min=start_val_loss_min,\n",
        "            start_patience_counter=start_patience_counter\n",
        "        )\n",
        "\n",
        "        # ğŸ“ If loss file not present, create CSV header\n",
        "        if not os.path.exists(self.loss_result_path):\n",
        "            with open(self.loss_result_path, \"w\", newline=\"\") as f:\n",
        "                csv.writer(f).writerow([\"Epoch\", \"Train Loss\", \"Valid Loss\"])\n",
        "\n",
        "        # ğŸš‚ Training Loop\n",
        "        for epoch in range(start_epoch, self.num_epochs + 1):\n",
        "            start_time = time.time()\n",
        "\n",
        "            train_loss = self.train_one_epoch(model, train_loader, optimizer, loss_fn)\n",
        "            valid_loss = self.evaluate(model, valid_loader, loss_fn)\n",
        "            scheduler.step()\n",
        "\n",
        "            epoch_mins, epoch_secs = self.epoch_time(start_time, time.time())\n",
        "            print(f\"Epoch {epoch:03d} | Time: {epoch_mins}m {epoch_secs}s | \"\n",
        "                  f\"Train: {train_loss:.6f} | Val: {valid_loss:.6f}\")\n",
        "\n",
        "            history['train_loss'].append(train_loss)\n",
        "            history['valid_loss'].append(valid_loss)\n",
        "\n",
        "            with open(self.loss_result_path, \"a\", newline=\"\") as f:\n",
        "                csv.writer(f).writerow([epoch, train_loss, valid_loss])\n",
        "\n",
        "            # ğŸ›‘ Early stopping check\n",
        "            if early_stopping(valid_loss, model, epoch, optimizer):\n",
        "                print(\"ğŸ›‘ Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "            torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (7). Pipeline"
      ],
      "metadata": {
        "id": "BZrW1cHg4OlM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwcTHPshqu0O",
        "outputId": "826b5d3e-af03-4738-e8e2-cfeead5a08c6"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“¦ Loading datasets...\n",
            "Epoch 071 | Time: 71m 35s | Train: 0.674845 | Val: 0.805939\n",
            "â³ EarlyStopping counter: 2 out of 10\n",
            "Epoch 072 | Time: 68m 22s | Train: 0.711546 | Val: 0.810964\n",
            "â³ EarlyStopping counter: 3 out of 10\n",
            "Epoch 073 | Time: 68m 42s | Train: 0.706732 | Val: 0.791821\n",
            "â³ EarlyStopping counter: 4 out of 10\n",
            "Epoch 074 | Time: 68m 55s | Train: 0.681764 | Val: 0.789887\n",
            "â³ EarlyStopping counter: 5 out of 10\n",
            "Epoch 075 | Time: 69m 1s | Train: 0.665208 | Val: 0.781649\n",
            "âœ… Validation loss improved. Saving model...\n",
            "Epoch 076 | Time: 69m 7s | Train: 0.641370 | Val: 0.784236\n",
            "â³ EarlyStopping counter: 1 out of 10\n",
            "Epoch 077 | Time: 69m 18s | Train: 0.660566 | Val: 0.789251\n",
            "â³ EarlyStopping counter: 2 out of 10\n",
            "Epoch 078 | Time: 69m 23s | Train: 0.639254 | Val: 0.780697\n",
            "âœ… Validation loss improved. Saving model...\n",
            "Epoch 079 | Time: 69m 26s | Train: 0.631953 | Val: 0.774602\n",
            "âœ… Validation loss improved. Saving model...\n",
            "Epoch 080 | Time: 69m 35s | Train: 0.628322 | Val: 0.770767\n",
            "âœ… Validation loss improved. Saving model...\n",
            "Epoch 081 | Time: 69m 34s | Train: 0.663994 | Val: 0.798359\n",
            "â³ EarlyStopping counter: 1 out of 10\n",
            "Epoch 082 | Time: 69m 34s | Train: 0.663142 | Val: 0.796649\n",
            "â³ EarlyStopping counter: 2 out of 10\n",
            "Epoch 083 | Time: 69m 40s | Train: 0.670170 | Val: 0.752426\n",
            "âœ… Validation loss improved. Saving model...\n",
            "Epoch 084 | Time: 69m 44s | Train: 0.654404 | Val: 0.807204\n",
            "â³ EarlyStopping counter: 1 out of 10\n",
            "Epoch 085 | Time: 69m 44s | Train: 0.641575 | Val: 0.787742\n",
            "â³ EarlyStopping counter: 2 out of 10\n",
            "Epoch 086 | Time: 69m 49s | Train: 0.657449 | Val: 0.757477\n",
            "â³ EarlyStopping counter: 3 out of 10\n",
            "Epoch 087 | Time: 69m 48s | Train: 0.638449 | Val: 0.815259\n",
            "â³ EarlyStopping counter: 4 out of 10\n",
            "Epoch 088 | Time: 69m 50s | Train: 0.643433 | Val: 0.777593\n",
            "â³ EarlyStopping counter: 5 out of 10\n",
            "Epoch 089 | Time: 69m 47s | Train: 0.637775 | Val: 0.783900\n",
            "â³ EarlyStopping counter: 6 out of 10\n",
            "Epoch 090 | Time: 69m 48s | Train: 0.624687 | Val: 0.788911\n",
            "â³ EarlyStopping counter: 7 out of 10\n"
          ]
        }
      ],
      "source": [
        "class UnetPipeline:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.setup_paths()\n",
        "        print(\"ğŸ“¦ Loading datasets...\")\n",
        "        self.train_loader, self.valid_loader, self.test_loader = self.prepare_loaders()\n",
        "\n",
        "    def setup_paths(self):\n",
        "        os.chdir(self.config['target_dir'])\n",
        "        self.output_dir = os.path.join(\".\", \"results\", self.config['output_folder_name'])\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "        self.loss_result_file = os.path.join(self.output_dir, \"train_and_valid_loss_results.csv\")\n",
        "        self.model_file = os.path.join(self.output_dir, \"model.pth\")\n",
        "        self.test_metrics_file = os.path.join(self.output_dir, \"test_metrics.csv\")\n",
        "        self.test_result_path = os.path.join(self.output_dir, \"test_outputs\")\n",
        "        os.makedirs(self.test_result_path, exist_ok=True)\n",
        "\n",
        "        self.dataset_dir = os.path.join(\"./datasets\", f\"Datasets_{self.config['transformation']}\")\n",
        "\n",
        "    def prepare_loaders(self):\n",
        "        from glob import glob\n",
        "        from monai.transforms import (\n",
        "            Compose, LoadImaged, EnsureChannelFirstD, Spacingd, Orientationd,\n",
        "            ScaleIntensityRanged, CropForegroundd, Resized, ToTensord,\n",
        "            RandFlipd, RandAffined, RandGaussianNoised, RandScaleIntensityd\n",
        "        )\n",
        "        from monai.data import Dataset, DataLoader\n",
        "\n",
        "        pixdim = (1, 1, 1)\n",
        "        a_min, a_max = -1000, 700\n",
        "        spatial_size = (96, 96, 96)\n",
        "\n",
        "        def get_files(split):\n",
        "            ct = sorted(glob(os.path.join(self.dataset_dir, split, \"ct\", \"*.nii.gz\")))\n",
        "            seg = sorted(glob(os.path.join(self.dataset_dir, split, \"segment\", \"*.nii.gz\")))\n",
        "            return [{\"vol\": c, \"seg\": s} for c, s in zip(ct, seg)]\n",
        "\n",
        "        # Training transforms with augmentation\n",
        "        train_transforms = Compose([\n",
        "            LoadImaged(keys=[\"vol\", \"seg\"]),\n",
        "            EnsureChannelFirstD(keys=[\"vol\", \"seg\"]),\n",
        "            Spacingd(keys=[\"vol\", \"seg\"], pixdim=pixdim, mode=(\"bilinear\", \"nearest\")),\n",
        "            Orientationd(keys=[\"vol\", \"seg\"], axcodes=\"RAS\"),\n",
        "            ScaleIntensityRanged(keys=[\"vol\"], a_min=a_min, a_max=a_max, b_min=0.0, b_max=1.0, clip=True),\n",
        "            CropForegroundd(keys=[\"vol\", \"seg\"], source_key=\"vol\"),\n",
        "            Resized(keys=[\"vol\", \"seg\"], spatial_size=spatial_size),\n",
        "\n",
        "            # â• Augmentations\n",
        "            RandFlipd(keys=[\"vol\", \"seg\"], prob=0.5, spatial_axis=0),\n",
        "            RandFlipd(keys=[\"vol\", \"seg\"], prob=0.5, spatial_axis=1),\n",
        "            RandAffined(\n",
        "                keys=[\"vol\", \"seg\"],\n",
        "                prob=0.3,\n",
        "                rotate_range=(0.1, 0.1, 0.1),\n",
        "                scale_range=(0.1, 0.1, 0.1),\n",
        "                mode=(\"bilinear\", \"nearest\")\n",
        "            ),\n",
        "            RandGaussianNoised(keys=[\"vol\"], prob=0.2, mean=0.0, std=0.1),\n",
        "            RandScaleIntensityd(keys=[\"vol\"], factors=0.1, prob=0.5),\n",
        "\n",
        "            ToTensord(keys=[\"vol\", \"seg\"])\n",
        "        ])\n",
        "\n",
        "        # Validation/test transforms without augmentation\n",
        "        base_transforms = Compose([\n",
        "            LoadImaged(keys=[\"vol\", \"seg\"]),\n",
        "            EnsureChannelFirstD(keys=[\"vol\", \"seg\"]),\n",
        "            Spacingd(keys=[\"vol\", \"seg\"], pixdim=pixdim, mode=(\"bilinear\", \"nearest\")),\n",
        "            Orientationd(keys=[\"vol\", \"seg\"], axcodes=\"RAS\"),\n",
        "            ScaleIntensityRanged(keys=[\"vol\"], a_min=a_min, a_max=a_max, b_min=0.0, b_max=1.0, clip=True),\n",
        "            CropForegroundd(keys=[\"vol\", \"seg\"], source_key=\"vol\"),\n",
        "            Resized(keys=[\"vol\", \"seg\"], spatial_size=spatial_size),\n",
        "            ToTensord(keys=[\"vol\", \"seg\"])\n",
        "        ])\n",
        "\n",
        "        train_loader = DataLoader(Dataset(get_files(\"train\"), train_transforms), batch_size=self.config['batch_size'], shuffle=True)\n",
        "        valid_loader = DataLoader(Dataset(get_files(\"valid\"), base_transforms), batch_size=self.config['batch_size'])\n",
        "        test_loader = DataLoader(Dataset(get_files(\"test\"), base_transforms), batch_size=1)\n",
        "\n",
        "        return train_loader, valid_loader, test_loader\n",
        "\n",
        "    def train(self):\n",
        "        trainer = UnetTrain(\n",
        "            model_file=self.model_file,\n",
        "            loss_result_path=self.loss_result_file,\n",
        "            lr=self.config['learning_rate'],\n",
        "            num_epochs=self.config['num_epochs'],\n",
        "            device=self.device\n",
        "        )\n",
        "        trainer.execute(self.train_loader, self.valid_loader)\n",
        "\n",
        "    def test(self):\n",
        "        model = UNet(\n",
        "            spatial_dims=3,\n",
        "            in_channels=1,\n",
        "            out_channels=1,\n",
        "            channels=(16, 32, 64, 128, 256),\n",
        "            strides=(2, 2, 2, 2),\n",
        "            num_res_units=2,\n",
        "            norm=Norm.BATCH\n",
        "        ).to(self.device)\n",
        "        checkpoint = torch.load(self.model_file, map_location=self.device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "        tester = UnetTest(self.test_result_path, self.test_metrics_file, self.device)\n",
        "        tester.test(model, self.test_loader)\n",
        "\n",
        "    def run(self):\n",
        "        self.train()\n",
        "        self.test()\n",
        "\n",
        "\n",
        "def main():\n",
        "    config = {\n",
        "        'target_dir': \"/content/drive/MyDrive/PhDwork/Segmentation\",\n",
        "        'output_folder_name': \"Results_MONAI_Augmented\",\n",
        "        'transformation': \"OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\",\n",
        "        'batch_size': 2,\n",
        "        'num_epochs': 100,\n",
        "        'learning_rate': 1e-4,\n",
        "    }\n",
        "    pipeline = UnetPipeline(config)\n",
        "    pipeline.run()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mp.set_start_method('spawn')\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#(8) Mask Generation"
      ],
      "metadata": {
        "id": "hSP35kUBOaDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from monai.transforms import (\n",
        "    Compose, LoadImaged, EnsureChannelFirstD, Spacingd, Orientationd,\n",
        "    ScaleIntensityRanged, CropForegroundd, Resized, EnsureTyped,\n",
        "    Invertd, CopyItemsd\n",
        ")\n",
        "from monai.data import Dataset, DataLoader, decollate_batch\n",
        "from skimage import measure\n",
        "\n",
        "\n",
        "class CTTransformInverter:\n",
        "    def __init__(self, ct_dir, seg_dir, output_dir, patient=None, device=None):\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.ct_dir = ct_dir\n",
        "        self.seg_dir = seg_dir\n",
        "        self.output_dir = output_dir\n",
        "        self.patient = patient\n",
        "        self.ct_out_dir = os.path.join(output_dir, \"ct\")\n",
        "        self.seg_out_dir = os.path.join(output_dir, \"segment\")\n",
        "\n",
        "        os.makedirs(self.ct_out_dir, exist_ok=True)\n",
        "        os.makedirs(self.seg_out_dir, exist_ok=True)\n",
        "\n",
        "        self.forward_transforms = self._get_forward_transforms()\n",
        "        self.inverse_transforms = None\n",
        "\n",
        "    def _get_forward_transforms(self):\n",
        "        return Compose([\n",
        "            LoadImaged(keys=[\"vol\", \"seg\"], allow_missing_keys=True),\n",
        "            EnsureChannelFirstD(keys=[\"vol\", \"seg\"], allow_missing_keys=True),\n",
        "            Spacingd(keys=[\"vol\", \"seg\"], pixdim=(1.0, 1.0, 1.0),\n",
        "                     mode=(\"bilinear\", \"nearest\"), allow_missing_keys=True),\n",
        "            Orientationd(keys=[\"vol\", \"seg\"], axcodes=\"RAS\", allow_missing_keys=True),\n",
        "            ScaleIntensityRanged(keys=[\"vol\"], a_min=-1000, a_max=700,\n",
        "                                 b_min=0.0, b_max=1.0, clip=True, allow_missing_keys=True),\n",
        "            CropForegroundd(keys=[\"vol\", \"seg\"], source_key=\"vol\", allow_missing_keys=True),\n",
        "            Resized(keys=[\"vol\", \"seg\"], spatial_size=(96, 96, 96), allow_missing_keys=True),\n",
        "            CopyItemsd(keys=[\"vol\", \"seg\"], names=[\"vol_meta_dict\", \"seg_meta_dict\"], allow_missing_keys=True),\n",
        "            EnsureTyped(keys=[\"vol\", \"seg\"], allow_missing_keys=True),\n",
        "        ])\n",
        "\n",
        "    def _load_data(self):\n",
        "        filenames = [f for f in os.listdir(self.ct_dir) if f.endswith((\".nii\", \".nii.gz\"))]\n",
        "\n",
        "        if self.patient:\n",
        "            if self.patient not in filenames:\n",
        "                raise FileNotFoundError(f\"[âœ—] Specified patient '{self.patient}' not found in {self.ct_dir}\")\n",
        "            filenames = [self.patient]\n",
        "\n",
        "        data = []\n",
        "        for f in filenames:\n",
        "            ct_path = os.path.join(self.ct_dir, f)\n",
        "            seg_path = os.path.join(self.seg_dir, f)\n",
        "            if os.path.exists(seg_path):\n",
        "                data.append({\"vol\": ct_path, \"seg\": seg_path})\n",
        "            else:\n",
        "                print(f\"[âš ] Missing segmentation file for {f}\")\n",
        "        return Dataset(data=data, transform=self.forward_transforms)\n",
        "\n",
        "    def _get_inverse_transforms(self):\n",
        "        return Compose([\n",
        "            Invertd(\n",
        "                keys=[\"vol\", \"seg\"],\n",
        "                transform=self.forward_transforms,\n",
        "                orig_keys=[\"vol\", \"seg\"],\n",
        "                meta_keys=[\"vol_meta_dict\", \"seg_meta_dict\"],\n",
        "                nearest_interp=True,\n",
        "                to_tensor=False\n",
        "            )\n",
        "        ])\n",
        "\n",
        "    def process(self):\n",
        "        dataset = self._load_data()\n",
        "        loader = DataLoader(dataset, batch_size=1)\n",
        "\n",
        "        for batch in loader:\n",
        "            batch = decollate_batch(batch)[0]\n",
        "\n",
        "            print(\"Batch keys:\", batch.keys())\n",
        "            if \"vol_meta_dict\" not in batch or \"seg_meta_dict\" not in batch:\n",
        "                raise KeyError(\"Missing 'vol_meta_dict' or 'seg_meta_dict'. Check CopyItemsd or LoadImaged step.\")\n",
        "\n",
        "            vol_meta = batch[\"vol_meta_dict\"]\n",
        "            seg_meta = batch[\"seg_meta_dict\"]\n",
        "            filename = os.path.basename(vol_meta.meta[\"filename_or_obj\"])\n",
        "            pro_vol = batch[\"vol\"]\n",
        "            pro_seg = batch[\"seg\"]\n",
        "            pro_vol = pro_vol.squeeze(0).numpy()\n",
        "            pro_seg = pro_seg.squeeze(0).numpy()\n",
        "            print(\"shape of processed volume:\", pro_vol.shape)\n",
        "            print(\"min of processed volume:\", np.min(pro_vol))\n",
        "            print(\"max of processed volume:\", np.max(pro_vol))\n",
        "            print(\"shape of processed segmentation:\", pro_seg.shape)\n",
        "            print(\"min of processed segmentation:\", np.min(pro_seg))\n",
        "            print(\"max of processed segmentation:\", np.max(pro_seg))\n",
        "            self.inverse_transforms = self._get_inverse_transforms()\n",
        "            inverted = self.inverse_transforms(batch)\n",
        "            inv_vol = inverted[\"vol\"]\n",
        "            inv_seg = inverted[\"seg\"]\n",
        "            inv_vol = inv_vol.squeeze(0).numpy()\n",
        "            inv_vol = inv_vol * 1700 - 1000\n",
        "            inv_seg = inv_seg.squeeze(0).numpy()\n",
        "            # inv_seg = inv_seg>0.5\n",
        "            orig_vol = nib.load(vol_meta.meta[\"filename_or_obj\"]).get_fdata()\n",
        "            orig_seg = nib.load(seg_meta.meta[\"filename_or_obj\"]).get_fdata()\n",
        "            print(\"shape of original volume:\", orig_vol.shape)\n",
        "            print(\"min of original volume:\", np.min(orig_vol))\n",
        "            print(\"max of original volume:\", np.max(orig_vol))\n",
        "            print(\"shape of inverted volume:\", inv_vol.shape)\n",
        "            print(\"min of inverted volume:\", np.min(inv_vol))\n",
        "            print(\"max of inverted volume:\", np.max(inv_vol))\n",
        "            print(\"shape of original segmentation:\", orig_seg.shape)\n",
        "            print(\"shape of inverted segmentation:\", inv_seg.shape)\n",
        "            print(\"min of original segmentation:\", np.min(orig_seg))\n",
        "            print(\"max of original segmentation:\", np.max(orig_seg))\n",
        "            self._save_nifti(inv_vol, vol_meta, self.ct_out_dir, filename)\n",
        "            self._save_nifti(inv_seg, seg_meta, self.seg_out_dir, filename)\n",
        "            orig_vol = np.clip(orig_vol, -1000, 700)\n",
        "            msre = self._compute_msre(orig_vol, inv_vol)\n",
        "            print(f\"[âœ“] MSRE of CT for {filename}: {msre:.6f}\")\n",
        "            msre = self._compute_msre(orig_seg, inv_seg)\n",
        "            print(f\"[âœ“] MSRE of Seg for {filename}: {msre:.6f}\")\n",
        "            self._plot_non_empty_slices(orig_vol, inv_vol, orig_seg, inv_seg)\n",
        "            break  # Remove this if you want to process all cases\n",
        "\n",
        "    def _compute_msre(self, original, reconstructed):\n",
        "        squared_error = np.sum((original - reconstructed) ** 2) / (np.sum(original ** 2) + 1e-8)\n",
        "        return squared_error\n",
        "\n",
        "    def _save_nifti(self, array, meta_tensor, out_dir, filename):\n",
        "        original_affine = meta_tensor.meta.get(\"original_affine\", None)\n",
        "        affine = original_affine if original_affine is not None else meta_tensor.meta.get(\"affine\")\n",
        "        nib_img = nib.Nifti1Image(array.astype(np.float32), affine)\n",
        "        nib.save(nib_img, os.path.join(out_dir, filename))\n",
        "\n",
        "    def _plot_non_empty_slices(self, orig_ct, inv_ct, orig_seg, inv_seg, contour_color1='red', contour_color2='red'):\n",
        "        \"\"\"\n",
        "        Visualize non-empty segmentation slices in XY plane with CT overlays and segmentation contours.\n",
        "        \"\"\"\n",
        "        # Assume shape: (Z, Y, X)\n",
        "        non_empty = np.any(orig_seg > 0, axis=(1, 2))\n",
        "        slice_indices = np.where(non_empty)[0][:5]  # First 5 non-empty slices\n",
        "\n",
        "        if len(slice_indices) == 0:\n",
        "            print(\"No non-empty slices found.\")\n",
        "            return\n",
        "\n",
        "        fig, axs = plt.subplots(len(slice_indices), 6, figsize=(18, 3 * len(slice_indices)))\n",
        "\n",
        "        for i, idx in enumerate(slice_indices):\n",
        "            axs[i, 0].imshow(orig_ct[idx], cmap='gray')\n",
        "            axs[i, 0].set_title(f\"Original CT - Slice {idx}\")\n",
        "            axs[i, 0].axis(\"off\")\n",
        "\n",
        "            axs[i, 1].imshow(orig_seg[idx], cmap='gray')\n",
        "            axs[i, 1].set_title(f\"Original SEG - Slice {idx}\")\n",
        "            axs[i, 1].axis(\"off\")\n",
        "\n",
        "            axs[i, 2].imshow(orig_ct[idx], cmap='gray')\n",
        "            for contour in measure.find_contours(orig_seg[idx], level=0.5):\n",
        "                axs[i, 2].plot(contour[:, 1], contour[:, 0], color=contour_color1, linewidth=1.5)\n",
        "            axs[i, 2].set_title(f\"Original CT + Seg Contour - Slice {idx}\")\n",
        "            axs[i, 2].axis(\"off\")\n",
        "\n",
        "            axs[i, 3].imshow(inv_ct[idx], cmap='gray')\n",
        "            axs[i, 3].set_title(f\"Inverted CT - Slice {idx}\")\n",
        "            axs[i, 3].axis(\"off\")\n",
        "\n",
        "            axs[i, 4].imshow(inv_seg[idx], cmap='gray')\n",
        "            axs[i, 4].set_title(f\"Inverted SEG - Slice {idx}\")\n",
        "            axs[i, 4].axis(\"off\")\n",
        "\n",
        "            axs[i, 5].imshow(inv_ct[idx], cmap='gray')\n",
        "            for contour in measure.find_contours(inv_seg[idx], level=0.5):\n",
        "                axs[i, 5].plot(contour[:, 1], contour[:, 0], color=contour_color2, linewidth=1.5)\n",
        "            axs[i, 5].set_title(f\"Inverted CT + Seg Contour - Slice {idx}\")\n",
        "            axs[i, 5].axis(\"off\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_ct_path = r\"F:\\Idiot Developer\\radioGenomic\\Final Code\\Segmentation\\datasets\\nifti\\train\\ct\"\n",
        "    input_seg_path = r\"F:\\Idiot Developer\\radioGenomic\\Final Code\\Segmentation\\datasets\\nifti\\train\\segment\"\n",
        "    output_path = r\"F:\\Idiot Developer\\radioGenomic\\Final Code\\Segmentation\\datasets\\nifti\\inverse_output\"\n",
        "    patient = \"LUNG1-100.nii.gz\"\n",
        "\n",
        "    inverter = CTTransformInverter(input_ct_path, input_seg_path, output_path, patient)\n",
        "    inverter.process()\n"
      ],
      "metadata": {
        "id": "fQr9kvHdOdl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from monai.networks.nets import UNet\n",
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    LoadImaged,\n",
        "    EnsureChannelFirstd,\n",
        "    Spacingd,\n",
        "    Orientationd,\n",
        "    ScaleIntensityRanged,\n",
        "    CropForegroundd,\n",
        "    EnsureTyped,\n",
        "    ToTensord,\n",
        ")\n",
        "from monai.data import Dataset, DataLoader, list_data_collate\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.utils import set_determinism\n",
        "from monai.networks.layers import Norm\n",
        "\n",
        "\n",
        "class UNetInferencePipeline:\n",
        "    def __init__(self, model_path, input_dir, output_dir, device=\"cuda:0\"):\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.input_dir = input_dir\n",
        "        self.output_dir = output_dir\n",
        "        self.ct_out_dir = os.path.join(output_dir, \"ct\")\n",
        "        self.seg_out_dir = os.path.join(output_dir, \"segment\")\n",
        "        os.makedirs(self.ct_out_dir, exist_ok=True)\n",
        "        os.makedirs(self.seg_out_dir, exist_ok=True)\n",
        "        self.model_path = model_path\n",
        "        self.model = self._load_model()\n",
        "        set_determinism(seed=42)\n",
        "        self.forward_transforms = self._get_forward_transforms()\n",
        "        self.inverse_transforms = None\n",
        "        self.dataloader = self._prepare_dataloader()\n",
        "\n",
        "    def _load_model(self):\n",
        "        if not os.path.exists(self.model_path):\n",
        "            raise FileNotFoundError(f\"Model file not found at: {self.model_path}\")\n",
        "\n",
        "        model = UNet(\n",
        "            spatial_dims=3,\n",
        "            in_channels=1,\n",
        "            out_channels=1,\n",
        "            channels=(16, 32, 64, 128, 256),\n",
        "            strides=(2, 2, 2, 2),\n",
        "            num_res_units=2,\n",
        "            norm=Norm.BATCH\n",
        "        ).to(self.device)\n",
        "\n",
        "        state_dict = torch.load(self.model_path, map_location=self.device)\n",
        "        if 'model_state_dict' in state_dict:\n",
        "            model.load_state_dict(state_dict['model_state_dict'])\n",
        "        else:\n",
        "            model.load_state_dict(state_dict)\n",
        "\n",
        "        print(f\"âœ… Model loaded successfully from {self.model_path}\")\n",
        "        return model\n",
        "\n",
        "    def _get_forward_transforms(self):\n",
        "          return Compose([\n",
        "              LoadImaged(keys=[\"vol\"], allow_missing_keys=True),\n",
        "              EnsureChannelFirstD(keys=[\"vol\"], allow_missing_keys=True),\n",
        "              Spacingd(keys=[\"vol\"], pixdim=(1.0, 1.0, 1.0),\n",
        "                      mode=(\"bilinear\"), allow_missing_keys=True),\n",
        "              Orientationd(keys=[\"vol\"], axcodes=\"RAS\", allow_missing_keys=True),\n",
        "              ScaleIntensityRanged(keys=[\"vol\"], a_min=-1000, a_max=700,\n",
        "                                  b_min=0.0, b_max=1.0, clip=True, allow_missing_keys=True),\n",
        "              CropForegroundd(keys=[\"vol\"], source_key=\"vol\", allow_missing_keys=True),\n",
        "              Resized(keys=[\"vol\"], spatial_size=(96, 96, 96), allow_missing_keys=True),\n",
        "              CopyItemsd(keys=[\"vol\"], names=[\"vol_meta_dict\"], allow_missing_keys=True),\n",
        "              EnsureTyped(keys=[\"vol\"], allow_missing_keys=True),\n",
        "            ])\n",
        "\n",
        "    def _get_inverse_transforms(self):\n",
        "        return Compose([\n",
        "            Invertd(\n",
        "                keys=[\"vol\", \"seg\"],\n",
        "                transform=self.forward_transforms,\n",
        "                orig_keys=[\"vol\", \"seg\"],\n",
        "                meta_keys=[\"vol_meta_dict\", \"seg_meta_dict\"],\n",
        "                nearest_interp=True,\n",
        "                to_tensor=False\n",
        "            )\n",
        "        ])\n",
        "\n",
        "    def _prepare_dataloader(self):\n",
        "        nifti_files = sorted(Path(self.input_folder).glob(\"*.nii.gz\"))\n",
        "        if not nifti_files:\n",
        "            raise FileNotFoundError(f\"No NIfTI files found in input folder: {self.input_folder}\")\n",
        "\n",
        "        data_dicts = [{\"vol\": str(f)} for f in nifti_files]\n",
        "        print(f\"ğŸ” Found {len(data_dicts)} NIfTI files for inference.\")\n",
        "        return DataLoader(Dataset(data=data_dicts, self.forward_transforms),batch_size=1, num_workers=0)\n",
        "\n",
        "    def infer(self):\n",
        "        roi_size = (96, 96, 96)\n",
        "        sw_batch_size = 1\n",
        "        overlap = 0.5\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(self.dataloader):\n",
        "                batch = decollate_batch(batch)[0]\n",
        "                vol_meta = batch[\"vol_meta_dict\"]\n",
        "                images = batch[\"vol\"].to(self.device)\n",
        "                print(f\"ğŸ” Inference on [{i+1}] {input_filename} | shape = {images.shape}\")\n",
        "                pred = sliding_window_inference(\n",
        "                    inputs=image,\n",
        "                    roi_size=roi_size,\n",
        "                    sw_batch_size=sw_batch_size,\n",
        "                    predictor=model\n",
        "                )\n",
        "                pred = torch.sigmoid(pred) > 0.5  # Binary thresholding\n",
        "                pred = pred.float()\n",
        "                self.inverse_transforms = self._get_inverse_transforms()\n",
        "                inverted = self.inverse_transforms(batch)\n",
        "\n",
        "                try:\n",
        "                    meta = batch[\"vol\"].meta if hasattr(batch[\"vol\"], \"meta\") else {}\n",
        "                    input_path = meta.get(\"filename_or_obj\", [f\"case_{i}\"])[0]\n",
        "                    input_filename = Path(input_path).stem\n",
        "\n",
        "                    images = batch[\"vol\"].to(self.device)\n",
        "                    print(f\"ğŸ” Inference on [{i+1}] {input_filename} | shape = {images.shape}\")\n",
        "\n",
        "                    # Sliding window inference\n",
        "                    preds = sliding_window_inference(\n",
        "                        images, roi_size=roi_size,\n",
        "                        sw_batch_size=sw_batch_size,\n",
        "                        predictor=self.model,\n",
        "                        overlap=overlap\n",
        "                    )\n",
        "\n",
        "                    # Apply sigmoid and threshold\n",
        "                    probs = torch.sigmoid(preds)\n",
        "                    pred_label = (probs > 0.5).float()\n",
        "\n",
        "                    # Save output\n",
        "                    self._save_prediction(\n",
        "                        pred=pred_label,\n",
        "                        meta=meta,\n",
        "                        input_filename=input_filename\n",
        "                    )\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"âŒ Error during inference on batch {i}: {e}\")\n",
        "\n",
        "\n",
        "    def _save_prediction(self, pred, meta, input_filename):\n",
        "        # Remove batch and channel dimensions: [1, 1, D, H, W] â†’ [D, H, W]\n",
        "        pred_np = pred.squeeze().cpu().numpy()\n",
        "\n",
        "        # --- Robust affine extraction ---\n",
        "        affine = meta.get(\"affine\")\n",
        "        if isinstance(affine, list):\n",
        "            affine = affine[0]\n",
        "        affine = np.asarray(affine)\n",
        "        if affine.shape != (4, 4):\n",
        "            print(f\"âš ï¸ Invalid affine shape: {affine.shape}, using identity affine.\")\n",
        "            affine = np.eye(4)\n",
        "\n",
        "        # Save with nibabel\n",
        "        out_path = Path(self.output_folder) / f\"{input_filename}_seg.nii.gz\"\n",
        "        nib_img = nib.Nifti1Image(pred_np.astype(np.uint8), affine)\n",
        "        nib.save(nib_img, str(out_path))\n",
        "        print(f\"âœ… Saved: {out_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ROOT_DIR = \"/content/drive/MyDrive/PhDwork/Segmentation\"\n",
        "    MODEL_PATH = os.path.join(ROOT_DIR, \"results\", \"Results_MONAI_Augmented\", \"model.pth\")\n",
        "    INPUT_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"Lung3\", \"ct\")\n",
        "    OUTPUT_FOLDER = os.path.join(ROOT_DIR, \"datasets\", \"Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train\", \"Lung3\", \"segment\")\n",
        "\n",
        "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "    os.chdir(ROOT_DIR)\n",
        "\n",
        "    try:\n",
        "        pipeline = UNetInferencePipeline(MODEL_PATH, INPUT_FOLDER, OUTPUT_FOLDER)\n",
        "        pipeline.infer()\n",
        "        print(\"ğŸ‰ Inference completed successfully for all patients!\")\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "z6a0G1DXTIV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import numpy as np\n",
        "import monai\n",
        "from monai.networks.nets import UNet\n",
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    EnsureChannelFirstd,\n",
        "    Spacingd,\n",
        "    Orientationd,\n",
        "    ScaleIntensityRanged,\n",
        "    CropForegroundd,\n",
        "    Resized,\n",
        "    ToTensord,\n",
        "    Invertd,\n",
        ")\n",
        "from monai.data import Dataset, DataLoader\n",
        "import SimpleITK as sitk\n",
        "import nibabel as nib\n",
        "from monai.data.meta_tensor import MetaTensor\n",
        "\n",
        "class Lung3SegmentationInference:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path: str,\n",
        "        ct_dir: str,\n",
        "        output_dir: str,\n",
        "        device: str = \"cuda:0\",\n",
        "        pixdim=(1, 1, 1),\n",
        "        a_min=-1000,\n",
        "        a_max=700,\n",
        "        spatial_size=(96, 96, 96),\n",
        "    ):\n",
        "        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model_path = model_path\n",
        "        self.ct_dir = ct_dir\n",
        "        self.output_dir = output_dir\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "        self.pixdim = pixdim\n",
        "        self.a_min = a_min\n",
        "        self.a_max = a_max\n",
        "        self.spatial_size = spatial_size\n",
        "\n",
        "        self._load_model()\n",
        "        self._prepare_transforms()\n",
        "\n",
        "    def _load_model(self):\n",
        "        self.model = UNet(\n",
        "            spatial_dims=3,\n",
        "            in_channels=1,\n",
        "            out_channels=1,\n",
        "            channels=(16, 32, 64, 128, 256),\n",
        "            strides=(2, 2, 2, 2),\n",
        "            num_res_units=2,\n",
        "            norm=\"batch\",\n",
        "        ).to(self.device)\n",
        "\n",
        "        checkpoint = torch.load(self.model_path, map_location=self.device)\n",
        "        if \"model_state_dict\" in checkpoint:\n",
        "            self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        else:\n",
        "            self.model.load_state_dict(checkpoint)\n",
        "        self.model.eval()\n",
        "        print(f\"Model loaded from {self.model_path}\")\n",
        "\n",
        "    def _prepare_transforms(self):\n",
        "        # Base transforms WITHOUT LoadImaged to avoid load_meta=True errors\n",
        "        self.base_transforms = [\n",
        "            EnsureChannelFirstd(keys=[\"vol\"]),\n",
        "            Spacingd(keys=[\"vol\"], pixdim=self.pixdim, mode=\"bilinear\"),\n",
        "            Orientationd(keys=[\"vol\"], axcodes=\"RAS\"),\n",
        "            ScaleIntensityRanged(\n",
        "                keys=[\"vol\"], a_min=self.a_min, a_max=self.a_max, b_min=0.0, b_max=1.0, clip=True\n",
        "            ),\n",
        "            CropForegroundd(keys=[\"vol\"], source_key=\"vol\"),\n",
        "        ]\n",
        "\n",
        "        # Forward transforms: includes resizing and ToTensor for inference input\n",
        "        self.test_transforms = Compose(\n",
        "            self.base_transforms + [Resized(keys=[\"vol\"], spatial_size=self.spatial_size), ToTensord(keys=[\"vol\"])]\n",
        "        )\n",
        "\n",
        "        # Inverse transforms: exclude Resized and ToTensor (inverse causes errors)\n",
        "        self.inverse_transforms = Compose(self.base_transforms)\n",
        "\n",
        "    def _load_nifti_with_meta(self, nifti_path):\n",
        "        \"\"\"Load NIfTI image and manually prepare metadata dictionary.\"\"\"\n",
        "        img = nib.load(nifti_path)\n",
        "        data = img.get_fdata()\n",
        "        meta_dict = {\n",
        "            \"affine\": img.affine,\n",
        "            \"header\": img.header,\n",
        "            \"filename_or_obj\": nifti_path,\n",
        "        }\n",
        "        return data, meta_dict\n",
        "\n",
        "    def _get_data_dicts(self):\n",
        "        ct_files = sorted(glob.glob(os.path.join(self.ct_dir, \"*.nii*\")))\n",
        "        data_dicts = []\n",
        "        for f in ct_files:\n",
        "            img_data, meta = self._load_nifti_with_meta(f)\n",
        "            data_dicts.append({\"vol\": img_data, \"vol_meta_dict\": meta, \"orig_path\": f})\n",
        "        return data_dicts\n",
        "\n",
        "\n",
        "    def run_inference(self):\n",
        "        data_dicts = self._get_data_dicts()\n",
        "        dataset = Dataset(data=data_dicts, transform=self.test_transforms)\n",
        "        dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "        for idx, batch in enumerate(dataloader):\n",
        "            with torch.no_grad():\n",
        "                vol = batch[\"vol\"].to(self.device)\n",
        "                outputs = self.model(vol)\n",
        "                probs = torch.sigmoid(outputs)\n",
        "                pred_mask = (probs > 0.5).float()\n",
        "\n",
        "            meta_dict = batch[\"vol_meta_dict\"][0]  # unpack metadata from batch\n",
        "\n",
        "            invert_dict = {\n",
        "                \"pred\": pred_mask.cpu(),\n",
        "                \"vol\": batch[\"vol\"].cpu(),\n",
        "                \"vol_meta_dict\": meta_dict,\n",
        "            }\n",
        "\n",
        "            invert_transform = Compose(\n",
        "                [\n",
        "                    Invertd(\n",
        "                        keys=\"pred\",\n",
        "                        transform=self.inverse_transforms,\n",
        "                        orig_keys=\"vol\",\n",
        "                        meta_keys=\"vol_meta_dict\",\n",
        "                        nearest_interp=True,\n",
        "                        to_tensor=False,\n",
        "                    )\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            inverted_pred = invert_transform(invert_dict)[\"pred\"]\n",
        "\n",
        "            orig_ct_path = data_dicts[idx][\"orig_path\"]\n",
        "            self._save_mask_nifti(inverted_pred, orig_ct_path)\n",
        "            break\n",
        "\n",
        "        print(f\"Inference complete. Masks saved to {self.output_dir}\")\n",
        "\n",
        "    def _save_mask_nifti(self, mask_np: np.ndarray, orig_ct_path: str):\n",
        "        sitk_img = sitk.ReadImage(orig_ct_path)\n",
        "        mask_sitk = sitk.GetImageFromArray(mask_np.astype(np.uint8))\n",
        "        mask_sitk.SetSpacing(sitk_img.GetSpacing())\n",
        "        mask_sitk.SetOrigin(sitk_img.GetOrigin())\n",
        "        mask_sitk.SetDirection(sitk_img.GetDirection())\n",
        "\n",
        "        base_name = os.path.basename(orig_ct_path)\n",
        "        if base_name.endswith(\".nii.gz\"):\n",
        "            base_name = base_name[:-7]\n",
        "        elif base_name.endswith(\".nii\"):\n",
        "            base_name = base_name[:-4]\n",
        "\n",
        "        save_filename = base_name + \".nii.gz\"\n",
        "        save_path = os.path.join(self.output_dir, save_filename)\n",
        "        sitk.WriteImage(mask_sitk, save_path)\n",
        "        print(f\"Saved mask: {save_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_path = \"/content/drive/MyDrive/PhDwork/Segmentation/results/Results_MONAI_Augmented/model.pth\"\n",
        "    ct_dir = \"/content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3/ct\"\n",
        "    output_dir = \"/content/drive/MyDrive/PhDwork/Segmentation/datasets/Datasets_OriginalCT_Nifti_Empty_NonEmpty_slices_In_Train/Lung3/segment\"\n",
        "    pixdim = (1, 1, 1)\n",
        "    a_min, a_max = -1000, 700\n",
        "    spatial_size = (96, 96, 96)\n",
        "\n",
        "    lung3_infer = Lung3SegmentationInference(\n",
        "        model_path=model_path,\n",
        "        ct_dir=ct_dir,\n",
        "        output_dir=output_dir,\n",
        "        device=\"cuda:0\",\n",
        "        pixdim=pixdim,\n",
        "        a_min=a_min,\n",
        "        a_max=a_max,\n",
        "        spatial_size=spatial_size,\n",
        "    )\n",
        "    lung3_infer.run_inference()\n"
      ],
      "metadata": {
        "id": "Ckpn4rjZJ9fn",
        "outputId": "c5261a99-0818-4f7a-c2b0-23f9c264939a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from /content/drive/MyDrive/PhDwork/Segmentation/results/Results_MONAI_Augmented/model.pth\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "applying transform <monai.transforms.utility.dictionary.EnsureChannelFirstd object at 0x7afc54a6d1d0>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/monai/transforms/transform.py\u001b[0m in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[0m\n\u001b[1;32m    149\u001b[0m             ]\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_apply_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munpack_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/monai/transforms/transform.py\u001b[0m in \u001b[0;36m_apply_transform\u001b[0;34m(transform, data, unpack_parameters, lazy, overrides, logger_name)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlazy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyTrait\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/monai/transforms/utility/dictionary.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0mmeta_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMetaTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjuster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/monai/transforms/utility/array.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img, meta_dict)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrict_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unknown original_channel_dim in the MetaTensor meta dict or `meta_dict` or `channel_dim`.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-2442920170.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mspatial_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspatial_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     )\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0mlung3_infer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1-2442920170.py\u001b[0m in \u001b[0;36mrun_inference\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mvol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vol\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/monai/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;31m# dataset[[1, 3, 4]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/monai/data/dataset.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m     93\u001b[0m         \u001b[0mdata_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mslice\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/monai/transforms/compose.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_, start, end, threading, lazy)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0m_lazy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlazy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         result = execute_compose(\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mtransforms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/monai/transforms/compose.py\u001b[0m in \u001b[0;36mexecute_compose\u001b[0;34m(data, transforms, map_items, unpack_items, start, end, lazy, overrides, threading, log_stats)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0m_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mThreadUnsafe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         data = apply_transform(\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0m_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munpack_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlazy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_stats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/monai/transforms/transform.py\u001b[0m in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0m_log_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"applying transform {transform}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: applying transform <monai.transforms.utility.dictionary.EnsureChannelFirstd object at 0x7afc54a6d1d0>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKImglO4-twL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "import os\n",
        "import csv\n",
        "\n",
        "\n",
        "class LossPlotter:\n",
        "    def __init__(self, csv_path: str):\n",
        "        self.csv_path = Path(csv_path)\n",
        "        self.data = self._load_data()\n",
        "\n",
        "    def _load_data(self):\n",
        "        if not self.csv_path.exists():\n",
        "            raise FileNotFoundError(f\"CSV file not found: {self.csv_path}\")\n",
        "        df = pd.read_csv(self.csv_path, index_col=0)  # Read row labels as index\n",
        "        return df  # Make rows into columns\n",
        "\n",
        "    def plot(self, title: str = \"Training and Validation Loss\", save_path= None):\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.plot(self.data.index, self.data['Train Loss'], label='Train Loss', color='blue')\n",
        "        plt.plot(self.data.index, self.data['Valid Loss'], label='Valid Loss', color='orange')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title(title)\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            save_path = Path(save_path)\n",
        "            save_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            plt.savefig(save_path, format='pdf')\n",
        "            print(f\"[INFO] Loss plot saved to {save_path}\")\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "        plt.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    target_dir = \"/content/drive/MyDrive/PhDwork/Segmentation\"\n",
        "    os.chdir(target_dir)\n",
        "    loss_result_file = os.path.join(\".\",\"results\",f\"Results_PreProcessedCT_Fifty_Fifty_DiceLoss_And_Strong_Augmentation\",\"train_and_valid_loss_results.csv\")\n",
        "    plotter = LossPlotter(loss_result_file)\n",
        "    plotter.plot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyeB21BYGQPu"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "os.chdir(\"/content/drive/MyDrive/PhDwork/Segmentation\")\n",
        "print(f\"ğŸ“ Current Directory: {os.getcwd()}\")\n",
        "with h5py.File('./datasets/Datasets_PreprocessedCT_clipping_uniformSpacing_With_Empty_NonEmpty_slices_In_Train/train_dataset.hdf5', 'r') as f:\n",
        "    print(list(f.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ud1cFDGmKQBK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "history_visible": true,
      "authorship_tag": "ABX9TyNdw8zgwKOPGlVYY3vYUkh2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}