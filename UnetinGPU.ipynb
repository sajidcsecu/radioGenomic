{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMkFZ+k12LAoKzR/M4cz/en",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sajidcsecu/radioGenomic/blob/main/UnetinGPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is the Code for the Segmentation on Rider Dataset (LUNG1). The Code is worked on the 2D slices over GPU"
      ],
      "metadata": {
        "id": "zr2QqHlZ8GZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (1) Import Required Libraries"
      ],
      "metadata": {
        "id": "C9Zo7tkcI1CX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SimpleITK\n",
        "!pip install pydicom===2.4.3\n",
        "!pip install pydicom-seg\n",
        "!pip install numpy==1.23.5"
      ],
      "metadata": {
        "id": "d9OVdEeKXpMe",
        "outputId": "4fa64844-e0a5-4078-c278-aa86233fc850",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.11/dist-packages (2.4.1)\n",
            "Requirement already satisfied: pydicom===2.4.3 in /usr/local/lib/python3.11/dist-packages (2.4.3)\n",
            "Requirement already satisfied: pydicom-seg in /usr/local/lib/python3.11/dist-packages (0.4.1)\n",
            "Requirement already satisfied: SimpleITK>1.2.4 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (2.4.1)\n",
            "Requirement already satisfied: jsonschema<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (3.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (1.23.5)\n",
            "Requirement already satisfied: pydicom>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (2.4.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (25.3.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (0.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (75.2.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (1.17.0)\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (2) Import required Libraries"
      ],
      "metadata": {
        "id": "JadHvjQcJ-qU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import csv\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import (\n",
        "    jaccard_score,\n",
        "    f1_score,\n",
        "    recall_score,\n",
        "    precision_score,\n",
        "    accuracy_score,\n",
        ")\n",
        "from tqdm import tqdm\n",
        "\n",
        "import cv2\n",
        "from typing import List\n",
        "import torch.multiprocessing as mp\n",
        "import h5py\n",
        "from google.colab import drive\n",
        "import torch.amp as amp"
      ],
      "metadata": {
        "id": "pmtDNjxMbfB4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (3) Mount Google Drive"
      ],
      "metadata": {
        "id": "uyzguRDWI9bM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lq6jVaaMXZz5",
        "outputId": "0380089c-a887-477b-a4ea-84d7afc4de1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (4) Data Preperation"
      ],
      "metadata": {
        "id": "p4PdRsL8DChf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HDF5SegmentationDataset(Dataset):\n",
        "    def __init__(self, hdf5_path, transform=None):\n",
        "        self.hdf5_path = hdf5_path\n",
        "        self.transform = transform\n",
        "        self.patient_ids = []\n",
        "\n",
        "        # Read only keys for indexing\n",
        "        with h5py.File(self.hdf5_path, 'r') as f:\n",
        "            self.patient_ids = list(f.keys())\n",
        "            self.slice_indices = [(pid, i) for pid in self.patient_ids for i in range(f[pid]['ct'].shape[0])]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.slice_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pid, slice_idx = self.slice_indices[idx]\n",
        "\n",
        "        with h5py.File(self.hdf5_path, 'r') as f:\n",
        "            ct_slice = f[pid]['ct'][slice_idx]\n",
        "            mask_slice = f[pid]['mask'][slice_idx]\n",
        "\n",
        "        # Normalize CT to [0, 1]\n",
        "        ct_slice = (ct_slice - np.min(ct_slice)) / (np.max(ct_slice) - np.min(ct_slice) + 1e-5)\n",
        "\n",
        "        # Convert to torch tensors and add channel dim\n",
        "        ct_tensor = torch.tensor(ct_slice, dtype=torch.float32).unsqueeze(0)\n",
        "        mask_tensor = torch.tensor(mask_slice, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "        if self.transform:\n",
        "            ct_tensor, mask_tensor = self.transform(ct_tensor, mask_tensor)\n",
        "\n",
        "        return ct_tensor, mask_tensor"
      ],
      "metadata": {
        "id": "ZoGkyWq93X-H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Unet"
      ],
      "metadata": {
        "id": "h2iHKHfcF3yP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UpSample(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.up(x)\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.conv = DoubleConv(in_channels, out_channels)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        p = self.pool(x)\n",
        "        return x, self.dropout(p)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.up = UpSample(in_channels, out_channels)\n",
        "        self.conv = DoubleConv(out_channels * 2, out_channels)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        x = self.up(x)\n",
        "        x = torch.cat([x, skip], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.e1 = EncoderBlock(in_channels, 64, dropout=dropout)\n",
        "        self.e2 = EncoderBlock(64, 128, dropout=dropout)\n",
        "        self.e3 = EncoderBlock(128, 256, dropout=dropout)\n",
        "        self.e4 = EncoderBlock(256, 512, dropout=dropout)\n",
        "\n",
        "        self.b = DoubleConv(512, 1024)\n",
        "        self.dropout_bottleneck = nn.Dropout(p=dropout)\n",
        "\n",
        "        self.d1 = DecoderBlock(1024, 512, dropout=dropout)\n",
        "        self.d2 = DecoderBlock(512, 256, dropout=dropout)\n",
        "        self.d3 = DecoderBlock(256, 128, dropout=dropout)\n",
        "        self.d4 = DecoderBlock(128, 64, dropout=dropout)\n",
        "\n",
        "        self.outputs = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        s1, p1 = self.e1(x)\n",
        "        s2, p2 = self.e2(p1)\n",
        "        s3, p3 = self.e3(p2)\n",
        "        s4, p4 = self.e4(p3)\n",
        "\n",
        "        b = self.b(p4)\n",
        "        b = self.dropout_bottleneck(b)\n",
        "\n",
        "        d1 = self.d1(b, s4)\n",
        "        d2 = self.d2(d1, s3)\n",
        "        d3 = self.d3(d2, s2)\n",
        "        d4 = self.d4(d3, s1)\n",
        "\n",
        "        return self.outputs(d4)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # double_conv = DoubleConv(256, 256)\n",
        "#     # print(double_conv)\n",
        "#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "#     input_image = torch.randn((1, 1, 512, 512), dtype=torch.float32)\n",
        "#     model = UNet(1, 1).to(device)\n",
        "#     input_image = input_image.to(device)\n",
        "#     out = model(input_image)\n",
        "#     print(out.shape)\n",
        "#     print(device)\n",
        "#     print(torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "PamAo5NoFsRv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Loss Function"
      ],
      "metadata": {
        "id": "IFrRJqgG7wxo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "944_4uJbmPPx"
      },
      "outputs": [],
      "source": [
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, smooth=1e-6, epsilon=1e-8):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, preds, targets):\n",
        "        preds = torch.sigmoid(preds)\n",
        "        preds = preds.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        intersection = (preds * targets).sum()\n",
        "        dice_score = (2. * intersection + self.smooth) / (preds.sum() + targets.sum() + self.smooth + self.epsilon)\n",
        "        return 1 - dice_score\n",
        "\n",
        "class DiceBCELoss(nn.Module):\n",
        "    def __init__(self, smooth=1e-6, epsilon=1e-8):\n",
        "        super(DiceBCELoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "        self.epsilon = epsilon\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def forward(self, preds, targets):\n",
        "        preds = preds.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        intersection = (torch.sigmoid(preds) * targets).sum()\n",
        "        dice_loss = 1 - (2. * intersection + self.smooth) / (torch.sigmoid(preds).sum() + targets.sum() + self.smooth + self.epsilon)\n",
        "        bce_loss = self.bce(preds, targets)\n",
        "        return bce_loss + dice_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Test"
      ],
      "metadata": {
        "id": "CZZ3Gu-DD88X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UnetTest:\n",
        "    def __init__(self, test_result_path: str,metrics_csv, device: torch.device):\n",
        "        self.test_result_path = test_result_path\n",
        "        self.device = device\n",
        "        self.metrics_csv = metrics_csv\n",
        "\n",
        "        os.makedirs(self.test_result_path, exist_ok=True)\n",
        "\n",
        "        # Initialize CSV file with headers\n",
        "        if not os.path.exists(self.metrics_csv):\n",
        "            with open(self.metrics_csv, mode='w', newline='') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([\"SampleID\", \"Jaccard\", \"F1\", \"Recall\", \"Precision\", \"Accuracy\", \"Time\"])\n",
        "\n",
        "        print(f\"Test results will be saved to: {self.test_result_path}\")\n",
        "        print(f\"Using device: {self.device} (CUDA available: {torch.cuda.is_available()})\")\n",
        "\n",
        "    def calculate_metrics(self, y_true: torch.Tensor, y_pred: torch.Tensor) -> List[float]:\n",
        "        y_true = y_true.detach().cpu().numpy().astype(bool).reshape(-1)\n",
        "        y_pred = y_pred.detach().cpu().numpy().astype(bool).reshape(-1)\n",
        "\n",
        "        return [\n",
        "            jaccard_score(y_true, y_pred, zero_division=1),\n",
        "            f1_score(y_true, y_pred, zero_division=1),\n",
        "            recall_score(y_true, y_pred, zero_division=1),\n",
        "            precision_score(y_true, y_pred, zero_division=1),\n",
        "            accuracy_score(y_true, y_pred),\n",
        "        ]\n",
        "\n",
        "    def save_result(self, image: torch.Tensor, org_mask: torch.Tensor, predicted_mask: torch.Tensor, sample_id: int) -> None:\n",
        "        predicted_mask = (predicted_mask.detach().cpu().numpy().squeeze() > 0.5).astype(np.uint8) * 255\n",
        "        org_mask = (org_mask.detach().cpu().numpy().squeeze() > 0.5).astype(np.uint8) * 255\n",
        "        image = (image.detach().cpu().numpy().squeeze() * 255).astype(np.uint8)\n",
        "\n",
        "        h, w = image.shape\n",
        "        line = np.ones((h, 10), dtype=np.uint8) * 128\n",
        "        cat_images = np.concatenate([image, line, org_mask, line, predicted_mask], axis=1)\n",
        "\n",
        "        file_name = os.path.join(self.test_result_path, f\"sample_{sample_id}.png\")\n",
        "        success = cv2.imwrite(file_name, cat_images)\n",
        "\n",
        "        if success:\n",
        "            print(f\"âœ… Saved: {file_name}\")\n",
        "        else:\n",
        "            print(f\"âŒ Failed to save image: {file_name}\")\n",
        "\n",
        "    def append_metrics_to_csv(self, sample_id: int, metrics: List[float], elapsed_time: float) -> None:\n",
        "        with open(self.metrics_csv, mode='a', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([sample_id] + [f\"{m:.4f}\" for m in metrics] + [f\"{elapsed_time:.4f}\"])\n",
        "\n",
        "    def test(self, model: torch.nn.Module, test_loader: torch.utils.data.DataLoader) -> None:\n",
        "        model.eval()\n",
        "        metrics_score = np.zeros(5)\n",
        "        time_taken = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for pid, (x, y) in enumerate(test_loader):\n",
        "                if (pid >=3):\n",
        "                  break\n",
        "                x = x.to(self.device, dtype=torch.float32)\n",
        "                y = y.to(self.device, dtype=torch.float32)\n",
        "\n",
        "                start_time = time.time()\n",
        "                y_pred = torch.sigmoid(model(x))\n",
        "                elapsed_time = time.time() - start_time\n",
        "                time_taken.append(elapsed_time)\n",
        "\n",
        "                batch_metrics = self.calculate_metrics(y, y_pred)\n",
        "                metrics_score += np.array(batch_metrics)\n",
        "\n",
        "                for idx in range(x.size(0)):\n",
        "                    sample_id = pid * x.size(0) + idx\n",
        "                    self.save_result(x[idx], y[idx], y_pred[idx], sample_id)\n",
        "                    self.append_metrics_to_csv(sample_id, batch_metrics, elapsed_time)\n",
        "\n",
        "        num_batches = len(test_loader)\n",
        "        avg_metrics = metrics_score / num_batches\n",
        "\n",
        "        print(f\"\\nðŸ§ª Total Batches in Test Set: {num_batches}\")\n",
        "        print(f\"ðŸ“Š Jaccard: {avg_metrics[0]:.4f} | F1: {avg_metrics[1]:.4f} | Recall: {avg_metrics[2]:.4f} | \"\n",
        "              f\"Precision: {avg_metrics[3]:.4f} | Accuracy: {avg_metrics[4]:.4f}\")\n",
        "        print(f\"âš¡ FPS: {1 / np.mean(time_taken):.2f}\")\n"
      ],
      "metadata": {
        "id": "T4VsKIzmFGLP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Training"
      ],
      "metadata": {
        "id": "Fa8L5nD2EVP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, min_delta=0.001):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = float('inf')\n",
        "        self.counter = 0\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "\n",
        "        if self.counter >= self.patience:\n",
        "            print(f\"â›” Early stopping triggered after {self.patience} epochs without improvement!\")\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "\n",
        "class UnetTrain:\n",
        "    def __init__(self,\n",
        "                 model_file: str,\n",
        "                 loss_result_path: str,\n",
        "                 lr: float,\n",
        "                 num_epochs: int,\n",
        "                 device: torch.device):\n",
        "\n",
        "        self.model_file = model_file\n",
        "        self.loss_result_path = loss_result_path\n",
        "        self.lr = lr\n",
        "        self.num_epochs = num_epochs\n",
        "        self.device = device\n",
        "\n",
        "        # For reproducibility\n",
        "        self.seeding(42)\n",
        "\n",
        "        print(f\"ðŸ”§ Training initialized: lr={self.lr}, epochs={self.num_epochs}\")\n",
        "        print(f\"ðŸ“ Model will be saved to: {self.model_file}\")\n",
        "        print(f\"ðŸ“ Loss log will be saved to: {self.loss_result_path}\")\n",
        "        print(f\"ðŸ’» Device in use: {self.device} (CUDA available: {torch.cuda.is_available()})\")\n",
        "\n",
        "    def seeding(self, seed):\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    def epoch_time(self, start_time, end_time):\n",
        "        elapsed_time = end_time - start_time\n",
        "        elapsed_mins = int(elapsed_time / 60)\n",
        "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "        return elapsed_mins, elapsed_secs\n",
        "\n",
        "    def train(self, model, loader, optimizer, loss_fn, device):\n",
        "        scaler = amp.GradScaler('cuda')\n",
        "        epoch_loss = 0.0\n",
        "        model.train()\n",
        "\n",
        "        for batch_idx, (x, y) in enumerate(loader):\n",
        "            if (batch_idx>=10):\n",
        "              break\n",
        "            x = x.to(device, dtype=torch.float32)\n",
        "            y = y.to(device, dtype=torch.float32)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with amp.autocast('cuda'):\n",
        "                y_pred = model(x)\n",
        "                loss = loss_fn(y_pred, y)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        return epoch_loss / len(loader)\n",
        "\n",
        "    def evaluate(self, model, loader, loss_fn, device):\n",
        "        epoch_loss = 0.0\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (x, y) in enumerate(loader):\n",
        "                if (batch_idx>=10):\n",
        "                  break\n",
        "                x = x.to(device, dtype=torch.float32)\n",
        "                y = y.to(device, dtype=torch.float32)\n",
        "                y_pred = model(x)\n",
        "                loss = loss_fn(y_pred, y)\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "        return epoch_loss / len(loader)\n",
        "\n",
        "    def execute(self, train_loader, valid_loader):\n",
        "        model = UNet(in_channels=1, out_channels=1, dropout=0.3).to(self.device)\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=self.lr, weight_decay=1e-5)\n",
        "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "        loss_fn = DiceBCELoss()\n",
        "\n",
        "        early_stopping = EarlyStopping(patience=10, min_delta=0.001)\n",
        "        best_valid_loss = float(\"inf\")\n",
        "        results = {\"train_loss\": [], \"valid_loss\": []}\n",
        "\n",
        "        for epoch in tqdm(range(self.num_epochs)):\n",
        "            start_time = time.time()\n",
        "\n",
        "            train_loss = self.train(model, train_loader, optimizer, loss_fn, self.device)\n",
        "            valid_loss = self.evaluate(model, valid_loader, loss_fn, self.device)\n",
        "\n",
        "            if valid_loss < best_valid_loss:\n",
        "                print(f\"âœ… Valid loss improved from {best_valid_loss:.4f} to {valid_loss:.4f}. Saving checkpoint.\")\n",
        "                best_valid_loss = valid_loss\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'loss': best_valid_loss,\n",
        "                }, self.model_file)\n",
        "\n",
        "            end_time = time.time()\n",
        "            epoch_mins, epoch_secs = self.epoch_time(start_time, end_time)\n",
        "\n",
        "            results[\"train_loss\"].append(train_loss)\n",
        "            results[\"valid_loss\"].append(valid_loss)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}: Time: {epoch_mins}m {epoch_secs}s | Train Loss: {train_loss:.3f} | Val Loss: {valid_loss:.3f}\")\n",
        "\n",
        "            if early_stopping(valid_loss):\n",
        "                print(\"ðŸ›‘ Stopping early due to no improvement.\")\n",
        "                break\n",
        "\n",
        "        # Save train/val losses to CSV\n",
        "        with open(self.loss_result_path, \"w\", newline=\"\") as file:\n",
        "            writer = csv.writer(file)\n",
        "            for key, value in results.items():\n",
        "                writer.writerow([key] + value)\n",
        "\n"
      ],
      "metadata": {
        "id": "281KQS_iEIDX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Set working directory\n",
        "    target_dir = \"/content/drive/MyDrive/PhDwork/Segmentation\"\n",
        "    os.chdir(target_dir)\n",
        "    print(\"ðŸ“ Current Directory:\", os.getcwd())\n",
        "\n",
        "    # Training configuration\n",
        "    batch_size = 8\n",
        "    num_epochs = 100\n",
        "    lr = 1e-4\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    transformation = \"OriginalCT\"\n",
        "\n",
        "    # Define paths\n",
        "    output_dir = os.path.join(\".\",\"results\",f\"Results_{transformation}\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    loss_result_file = os.path.join(output_dir, \"train_and_valid_loss_results.csv\")\n",
        "    model_file = os.path.join(output_dir, \"model.pth\")\n",
        "    test_metrics_file = os.path.join(output_dir, \"test_metrics.csv\")\n",
        "    test_result_path = os.path.join(output_dir, \"test_outputs\")\n",
        "    os.makedirs(test_result_path, exist_ok=True)\n",
        "\n",
        "    DATASET_DIR = f\"./datasets/Datasets_{transformation}\"\n",
        "    train_path = os.path.join(DATASET_DIR, \"train_dataset.hdf5\")\n",
        "    valid_path = os.path.join(DATASET_DIR, \"valid_dataset.hdf5\")\n",
        "    test_path = os.path.join(DATASET_DIR, \"test_dataset.hdf5\")\n",
        "\n",
        "    print(\"ðŸ“¦ Loading datasets...\")\n",
        "    train_dataset = HDF5SegmentationDataset(train_path)\n",
        "    valid_dataset = HDF5SegmentationDataset(valid_path)\n",
        "    test_dataset = HDF5SegmentationDataset(test_path)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
        "    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "    print(f\"âœ… Dataset sizes â€” Train: {len(train_dataset)}, Valid: {len(valid_dataset)}, Test: {len(test_dataset)}\")\n",
        "\n",
        "    # Initialize and run training\n",
        "    trainer = UnetTrain(\n",
        "        model_file=model_file,\n",
        "        loss_result_path=loss_result_file,\n",
        "        lr=lr,\n",
        "        num_epochs=num_epochs,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    trainer.execute(train_loader, valid_loader)\n",
        "\n",
        "    # Load best model for testing\n",
        "    model = UNet(in_channels=1, out_channels=1).to(device)\n",
        "    checkpoint = torch.load(model_file, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Test and save metrics\n",
        "    tester = UnetTest(test_result_path,test_metrics_file,device)\n",
        "    tester.test(model, test_loader)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mp.set_start_method('spawn')  # Required for some multiprocessing backends\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwcTHPshqu0O",
        "outputId": "ef47ed59-eae0-4d4e-e02e-d512ed07ecb3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“ Current Directory: /content/drive/MyDrive/PhDwork/Segmentation\n",
            "ðŸ“¦ Loading datasets...\n",
            "âœ… Dataset sizes â€” Train: 5858, Valid: 5141, Test: 4653\n",
            "ðŸ”§ Training initialized: lr=0.0001, epochs=100\n",
            "ðŸ“ Model will be saved to: ./results/Results_OriginalCT/model.pth\n",
            "ðŸ“ Loss log will be saved to: ./results/Results_OriginalCT/train_and_valid_loss_results.csv\n",
            "ðŸ’» Device in use: cuda (CUDA available: True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Valid loss improved from inf to 0.0271. Saving checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 1/100 [00:21<35:18, 21.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Time: 0m 21s | Train Loss: 0.023 | Val Loss: 0.027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|â–         | 2/100 [00:43<35:43, 21.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Time: 0m 22s | Train Loss: 0.021 | Val Loss: 0.028\n",
            "âœ… Valid loss improved from 0.0271 to 0.0244. Saving checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|â–Ž         | 3/100 [00:57<29:30, 18.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Time: 0m 13s | Train Loss: 0.020 | Val Loss: 0.024\n",
            "âœ… Valid loss improved from 0.0244 to 0.0228. Saving checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|â–         | 4/100 [01:20<31:54, 19.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Time: 0m 22s | Train Loss: 0.020 | Val Loss: 0.023\n",
            "âœ… Valid loss improved from 0.0228 to 0.0219. Saving checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|â–Œ         | 5/100 [01:42<32:56, 20.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Time: 0m 22s | Train Loss: 0.019 | Val Loss: 0.022\n",
            "âœ… Valid loss improved from 0.0219 to 0.0217. Saving checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|â–Œ         | 6/100 [02:03<32:52, 20.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Time: 0m 21s | Train Loss: 0.019 | Val Loss: 0.022\n",
            "âœ… Valid loss improved from 0.0217 to 0.0212. Saving checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|â–‹         | 7/100 [02:26<33:18, 21.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Time: 0m 22s | Train Loss: 0.019 | Val Loss: 0.021\n",
            "âœ… Valid loss improved from 0.0212 to 0.0212. Saving checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|â–Š         | 8/100 [02:47<32:50, 21.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Time: 0m 21s | Train Loss: 0.019 | Val Loss: 0.021\n",
            "âœ… Valid loss improved from 0.0212 to 0.0212. Saving checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|â–‰         | 9/100 [03:08<32:20, 21.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Time: 0m 21s | Train Loss: 0.019 | Val Loss: 0.021\n",
            "âœ… Valid loss improved from 0.0212 to 0.0210. Saving checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|â–ˆ         | 10/100 [03:30<32:16, 21.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Time: 0m 21s | Train Loss: 0.018 | Val Loss: 0.021\n",
            "âœ… Valid loss improved from 0.0210 to 0.0208. Saving checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|â–ˆ         | 11/100 [03:45<28:53, 19.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: Time: 0m 14s | Train Loss: 0.018 | Val Loss: 0.021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|â–ˆâ–        | 12/100 [04:06<29:12, 19.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12: Time: 0m 20s | Train Loss: 0.018 | Val Loss: 0.021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|â–ˆâ–Ž        | 13/100 [04:22<27:18, 18.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13: Time: 0m 16s | Train Loss: 0.018 | Val Loss: 0.021\n",
            "âœ… Valid loss improved from 0.0208 to 0.0207. Saving checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|â–ˆâ–        | 14/100 [04:36<25:00, 17.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14: Time: 0m 14s | Train Loss: 0.018 | Val Loss: 0.021\n",
            "âœ… Valid loss improved from 0.0207 to 0.0206. Saving checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|â–ˆâ–Œ        | 15/100 [04:59<26:49, 18.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15: Time: 0m 22s | Train Loss: 0.018 | Val Loss: 0.021\n",
            "âœ… Valid loss improved from 0.0206 to 0.0205. Saving checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|â–ˆâ–Œ        | 16/100 [05:20<27:34, 19.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16: Time: 0m 21s | Train Loss: 0.018 | Val Loss: 0.020\n",
            "âœ… Valid loss improved from 0.0205 to 0.0204. Saving checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|â–ˆâ–‹        | 17/100 [05:42<27:57, 20.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17: Time: 0m 21s | Train Loss: 0.018 | Val Loss: 0.020\n",
            "âœ… Valid loss improved from 0.0204 to 0.0203. Saving checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|â–ˆâ–Š        | 18/100 [06:03<28:09, 20.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18: Time: 0m 21s | Train Loss: 0.018 | Val Loss: 0.020\n",
            "âœ… Valid loss improved from 0.0203 to 0.0202. Saving checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19%|â–ˆâ–‰        | 19/100 [06:24<28:03, 20.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19: Time: 0m 21s | Train Loss: 0.018 | Val Loss: 0.020\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|â–ˆâ–ˆ        | 20/100 [06:45<27:29, 20.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20: Time: 0m 20s | Train Loss: 0.018 | Val Loss: 0.020\n",
            "âœ… Valid loss improved from 0.0202 to 0.0200. Saving checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 21%|â–ˆâ–ˆ        | 21/100 [07:02<25:44, 19.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21: Time: 0m 17s | Train Loss: 0.018 | Val Loss: 0.020\n",
            "âœ… Valid loss improved from 0.0200 to 0.0200. Saving checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|â–ˆâ–ˆâ–       | 22/100 [07:17<23:34, 18.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22: Time: 0m 14s | Train Loss: 0.017 | Val Loss: 0.020\n",
            "âœ… Valid loss improved from 0.0200 to 0.0199. Saving checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|â–ˆâ–ˆâ–Ž       | 23/100 [07:31<21:54, 17.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23: Time: 0m 14s | Train Loss: 0.017 | Val Loss: 0.020\n",
            "âœ… Valid loss improved from 0.0199 to 0.0199. Saving checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|â–ˆâ–ˆâ–       | 24/100 [07:46<20:40, 16.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24: Time: 0m 14s | Train Loss: 0.017 | Val Loss: 0.020\n",
            "âœ… Valid loss improved from 0.0199 to 0.0198. Saving checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|â–ˆâ–ˆâ–       | 24/100 [08:07<25:44, 20.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25: Time: 0m 21s | Train Loss: 0.017 | Val Loss: 0.020\n",
            "â›” Early stopping triggered after 10 epochs without improvement!\n",
            "ðŸ›‘ Stopping early due to no improvement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test results will be saved to: ./results/Results_OriginalCT/test_outputs\n",
            "Using device: cuda (CUDA available: True)\n",
            "âœ… Saved: ./results/Results_OriginalCT/test_outputs/sample_0.png\n",
            "âœ… Saved: ./results/Results_OriginalCT/test_outputs/sample_1.png\n",
            "âœ… Saved: ./results/Results_OriginalCT/test_outputs/sample_2.png\n",
            "âœ… Saved: ./results/Results_OriginalCT/test_outputs/sample_3.png\n",
            "âœ… Saved: ./results/Results_OriginalCT/test_outputs/sample_4.png\n",
            "âœ… Saved: ./results/Results_OriginalCT/test_outputs/sample_5.png\n",
            "âœ… Saved: ./results/Results_OriginalCT/test_outputs/sample_6.png\n",
            "âœ… Saved: ./results/Results_OriginalCT/test_outputs/sample_7.png\n",
            "âœ… Saved: ./results/Results_OriginalCT/test_outputs/sample_8.png\n",
            "âœ… Saved: ./results/Results_OriginalCT/test_outputs/sample_9.png\n",
            "âœ… Saved: ./results/Results_OriginalCT/test_outputs/sample_10.png\n",
            "âœ… Saved: ./results/Results_OriginalCT/test_outputs/sample_11.png\n",
            "âœ… Saved: ./results/Results_OriginalCT/test_outputs/sample_12.png\n",
            "âœ… Saved: ./results/Results_OriginalCT/test_outputs/sample_13.png\n",
            "âœ… Saved: ./results/Results_OriginalCT/test_outputs/sample_14.png\n",
            "âœ… Saved: ./results/Results_OriginalCT/test_outputs/sample_15.png\n",
            "âœ… Saved: ./results/Results_OriginalCT/test_outputs/sample_16.png\n",
            "âœ… Saved: ./results/Results_OriginalCT/test_outputs/sample_17.png\n",
            "âœ… Saved: ./results/Results_OriginalCT/test_outputs/sample_18.png\n",
            "âœ… Saved: ./results/Results_OriginalCT/test_outputs/sample_19.png\n",
            "âœ… Saved: ./results/Results_OriginalCT/test_outputs/sample_20.png\n",
            "âœ… Saved: ./results/Results_OriginalCT/test_outputs/sample_21.png\n",
            "âœ… Saved: ./results/Results_OriginalCT/test_outputs/sample_22.png\n",
            "âœ… Saved: ./results/Results_OriginalCT/test_outputs/sample_23.png\n",
            "\n",
            "ðŸ§ª Total Batches in Test Set: 582\n",
            "ðŸ“Š Jaccard: 0.0000 | F1: 0.0000 | Recall: 0.0052 | Precision: 0.0000 | Accuracy: 0.0000\n",
            "âš¡ FPS: 213.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xKImglO4-twL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}