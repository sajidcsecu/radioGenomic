{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJ4/Y3dYBcMBBfLb6xKuz3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sajidcsecu/radioGenomic/blob/main/NewUnetTrain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVKhpcY_S6HG"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import  DataLoader\n",
        "import torch\n",
        "from NewUnetDataPreparation import PatientDataset2DUNet\n",
        "from NewUnet import UNet\n",
        "from NewUnetLoss import DiceBCELoss\n",
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, min_delta=0.001):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): Number of epochs to wait before stopping if no improvement.\n",
        "            min_delta (float): Minimum change in the monitored metric to qualify as an improvement.\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = float('inf')\n",
        "        self.counter = 0\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0  # Reset patience counter if loss improves\n",
        "        else:\n",
        "            self.counter += 1  # Increase counter if no improvement\n",
        "\n",
        "        if self.counter >= self.patience:\n",
        "            print(f\"â›” Early stopping triggered after {self.patience} epochs without improvement!\")\n",
        "            return True  # Stop training\n",
        "        return False  # Continue training\n",
        "\n",
        "class UnetTrain:\n",
        "    \"\"\" Seeding the randomness. \"\"\"\n",
        "    def seeding(self,seed):\n",
        "        random.seed(seed)\n",
        "        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    \"\"\" Calculate the time taken \"\"\"\n",
        "    def epoch_time(self, start_time, end_time):\n",
        "        elapsed_time = end_time - start_time\n",
        "        elapsed_mins = int(elapsed_time / 60)\n",
        "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "        return elapsed_mins, elapsed_secs\n",
        "\n",
        "    \"\"\" Training \"\"\"\n",
        "    def train(self, model, loader, optimizer, loss_fn, device):\n",
        "        epoch_loss = 0.0\n",
        "        model.train()\n",
        "        for x, y in loader:\n",
        "            x = x.to(device,dtype=torch.float32)\n",
        "            y = y.to(device,dtype=torch.float32)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(x)\n",
        "            loss = loss_fn(y_pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        epoch_loss = epoch_loss / len(loader)\n",
        "        return epoch_loss\n",
        "\n",
        "    \"\"\" Testing \"\"\"\n",
        "    def evaluate(self,model, loader, loss_fn, device):\n",
        "        epoch_loss = 0.0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for x, y in loader:\n",
        "                x = x.to(device, dtype=torch.float32)\n",
        "                y = y.to(device, dtype=torch.float32)\n",
        "                y_pred = model(x)\n",
        "                loss = loss_fn(y_pred, y)\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            epoch_loss = epoch_loss / len(loader)\n",
        "        return epoch_loss\n",
        "\n",
        "    \"\"\" Loading the paths of data  \"\"\"\n",
        "    def load_paths(self, path, split=0.2):\n",
        "        images = sorted(glob(os.path.join(path, \"images/*\")))\n",
        "        masks = sorted(glob(os.path.join(path, \"masks/*\")))\n",
        "        total_size = len(images)\n",
        "        print(\"Total Images : \", total_size)\n",
        "        valid_size = int(split * total_size)\n",
        "        test_size = int(split * total_size)\n",
        "\n",
        "\n",
        "        train_x, valid_x = train_test_split(images, test_size=valid_size, random_state=42)\n",
        "        train_y, valid_y = train_test_split(masks, test_size=valid_size, random_state=42)\n",
        "\n",
        "        train_x, test_x = train_test_split(train_x, test_size=test_size, random_state=42)\n",
        "        train_y, test_y = train_test_split(train_y, test_size=test_size, random_state=42)\n",
        "        print(len(train_x), len(valid_x), len(test_x))\n",
        "        return (train_x, train_y), (valid_x, valid_y), (test_x, test_y)\n",
        "\n",
        "    def test(self,model, loader, loss_fn, device):\n",
        "        pass\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ut = UnetTrain()\n",
        "\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    ut.seeding(42)\n",
        "    \"\"\" Hyperparameters \"\"\"\n",
        "    batch_size = 8\n",
        "    num_epochs = 10\n",
        "    lr = 1e-4\n",
        "\n",
        "\n",
        "    \"\"\" Paths \"\"\"\n",
        "    # path = \"F:\\\\Idiot Developer\\\\radioGenomic\\\\Segementation\"\n",
        "    path = \".\\Segementation\"\n",
        "\n",
        "\n",
        "    checkpoint_path = os.path.join(path, \"files\", \"checkpoint.pth\")\n",
        "    print(\"Check Point Path : \",checkpoint_path )\n",
        "    # print(data_path)\n",
        "    # \"\"\" Dataset and loader \"\"\"\n",
        "    # # image = torch.load(\"F:\\\\Idiot Developer\\\\radioGenomic\\\\files\\\\segmentation\\\\image.pt\")\n",
        "    # # mask = torch.load(\"F:\\\\Idiot Developer\\\\radioGenomic\\\\files\\\\segmentation\\\\mask.pt\")\n",
        "    \"\"\" Training from images in disk using slice dataset\"\"\"\n",
        "    # data_path = os.path.join(path, \"data\", \"full data\")\n",
        "    # (train_x, train_y), (valid_x, valid_y), (test_x, test_y) = ut.load_paths(data_path)\n",
        "    # train_dataset = SliceDataset(train_x, train_y)\n",
        "    # valid_dataset = SliceDataset(valid_x, valid_y)\n",
        "    #\n",
        "    # # train_dataset, valid_dataset = torch.utils.data.random_split(patient_dataset, [0.8, 0.2])\n",
        "    \"\"\" Training from dicoms in disk using patient dataset \"\"\"\n",
        "\n",
        "    # Load dataset\n",
        "    metadata_lung1 = pd.read_csv('.\\metadata\\metadata_lung1.csv', sep=',', index_col=False)\n",
        "    patient_list_lung1 = metadata_lung1[\"Subject ID\"].unique().tolist()\n",
        "    index_of_error_patient = [patient_list_lung1.index(i) for i in ['LUNG1-128']]\n",
        "    patient_list_lung1 = np.delete(patient_list_lung1, index_of_error_patient)\n",
        "    patient_list_lung1 =  patient_list_lung1[:30]\n",
        "    print(patient_list_lung1)\n",
        "    train_patient, valid_patient = train_test_split(patient_list_lung1, test_size=0.1, random_state=42)\n",
        "    train_patient, test_patient = train_test_split(train_patient, test_size=0.1, random_state=42)\n",
        "    print(\"Number of Total Patients : \", len(patient_list_lung1))\n",
        "    print(\"Number of Patients for Training : \", len(train_patient))\n",
        "    print(\"Number of Patients for Validation : \", len(valid_patient))\n",
        "    print(\"Number of Patients for Testing : \", len(test_patient))\n",
        "    # transform = transforms.Compose([\n",
        "    #     transforms.ToTensor(),\n",
        "    # ])\n",
        "\n",
        "    # # Load dataset\n",
        "    # print(\"Training Loading...\")\n",
        "    # train_dataset = PatientDatasetAllInOneTensor(train_patient, metadata_lung1, train=True)\n",
        "    # print(\"Valid Loading...\")\n",
        "    # valid_dataset = PatientDatasetAllInOneTensor(valid_patient, metadata_lung1, train=False)\n",
        "    # print(\"Testing Loading...\")\n",
        "    # test_dataset = PatientDatasetAllInOneTensor(test_patient, metadata_lung1, train=False)\n",
        "    # Load dataset\n",
        "    print(\"Training Loading...\")\n",
        "    train_dataset = PatientDataset2DUNet(train_patient, metadata_lung1, train=True)\n",
        "    print(\"Valid Loading...\")\n",
        "    valid_dataset = PatientDataset2DUNet(valid_patient, metadata_lung1, train=False)\n",
        "    print(\"Testing Loading...\")\n",
        "    test_dataset = PatientDataset2DUNet(test_patient, metadata_lung1, train=False)\n",
        "    #\n",
        "    # # train_dataset, valid_dataset = torch.utils.data.random_split(patient_dataset, [0.8, 0.2])\n",
        "    # #\n",
        "    # #\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=os.cpu_count()\n",
        "    )\n",
        "    #\n",
        "    valid_loader = DataLoader(\n",
        "        dataset=valid_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=os.cpu_count()\n",
        "    )\n",
        "\n",
        "    print(f\"Total images in Training Dataset: {len(train_dataset)}\")\n",
        "    print(f\"Total images in Valid Dataset: {len(valid_dataset)}\")\n",
        "    print(f\"Total images in Testing Dataset: {len(test_dataset)}\")\n",
        "    # Make device agnostic code\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = UNet(1,1)\n",
        "    model = model.to(device)\n",
        "    # #\n",
        "    \"\"\" Loss Function and Optimizer \"\"\"\n",
        "    # Adam is great, but AdamW (Adam with weight decay) improves generalization by preventing overfitting.\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "    # If loss plateaus, the model automatically reduces the learning rate.\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
        "    loss_fn = DiceBCELoss()\n",
        "    # #\n",
        "\n",
        "    # \"\"\" Training the model \"\"\"\n",
        "    # Initialize Early Stopping\n",
        "    early_stopping = EarlyStopping(patience=10, min_delta=0.001)\n",
        "    best_valid_loss = float(\"inf\")\n",
        "    #\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss = ut.train(model, train_loader, optimizer, loss_fn, device)\n",
        "        valid_loss = ut.evaluate(model, valid_loader, loss_fn, device)\n",
        "    #\n",
        "        \"\"\" Saving the model \"\"\"\n",
        "        if valid_loss < best_valid_loss:\n",
        "            data_str = f\"Valid loss improved from {best_valid_loss:2.4f} to {valid_loss:2.4f}. Saving checkpoint: {checkpoint_path}\"\n",
        "            print(data_str)\n",
        "\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "        end_time = time.time()\n",
        "        epoch_mins, epoch_secs = ut.epoch_time(start_time, end_time)\n",
        "\n",
        "        data_str = f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\\n'\n",
        "        data_str += f'\\tTrain Loss: {train_loss:.3f}\\n'\n",
        "        data_str += f'\\t Val. Loss: {valid_loss:.3f}\\n'\n",
        "        print(data_str)\n",
        "        # **Early Stopping Check**\n",
        "        if early_stopping(valid_loss):\n",
        "            print(\"ðŸ›‘ Stopping training early due to no improvement.\")\n",
        "            break"
      ]
    }
  ]
}