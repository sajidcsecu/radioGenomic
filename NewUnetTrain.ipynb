{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNDSDZxe5NKrRolkfJHsBs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sajidcsecu/radioGenomic/blob/main/NewUnetTrain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVKhpcY_S6HG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from itertools import chain\n",
        "import torch\n",
        "from NewUnetDataPreparation import PatientDataset\n",
        "from NewUnet import UNet\n",
        "from NewUnetLoss import DiceBCELoss\n",
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "\n",
        "\n",
        "class UnetTrain:\n",
        "    \"\"\" Seeding the randomness. \"\"\"\n",
        "    def seeding(self,seed):\n",
        "        random.seed(seed)\n",
        "        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    \"\"\" Calculate the time taken \"\"\"\n",
        "    def epoch_time(self, start_time, end_time):\n",
        "        elapsed_time = end_time - start_time\n",
        "        elapsed_mins = int(elapsed_time / 60)\n",
        "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "        return elapsed_mins, elapsed_secs\n",
        "\n",
        "    \"\"\" Training \"\"\"\n",
        "    def train(self, model, loader, optimizer, loss_fn, device):\n",
        "        epoch_loss = 0.0\n",
        "        model.train()\n",
        "        for x, y in loader:\n",
        "            x = x.to(device,dtype=torch.float32)\n",
        "            y = y.to(device,dtype=torch.float32)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(x)\n",
        "            loss = loss_fn(y_pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        epoch_loss = epoch_loss / len(loader)\n",
        "        return epoch_loss\n",
        "\n",
        "    \"\"\" Testing \"\"\"\n",
        "    def evaluate(self,model, loader, loss_fn, device):\n",
        "        epoch_loss = 0.0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for x, y in loader:\n",
        "                x = x.to(device, dtype=torch.float32)\n",
        "                y = y.to(device, dtype=torch.float32)\n",
        "                y_pred = model(x)\n",
        "                loss = loss_fn(y_pred, y)\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            epoch_loss = epoch_loss / len(loader)\n",
        "        return epoch_loss\n",
        "\n",
        "    \"\"\" Loading the paths of data  \"\"\"\n",
        "    def load_paths(self, path, split=0.2):\n",
        "        images = sorted(glob(os.path.join(path, \"images/*\")))[:10]\n",
        "        masks = sorted(glob(os.path.join(path, \"masks/*\")))[:10]\n",
        "        total_size = len(images)\n",
        "        print(\"Total Images : \", total_size)\n",
        "        valid_size = int(split * total_size)\n",
        "        test_size = int(split * total_size)\n",
        "\n",
        "\n",
        "        train_x, valid_x = train_test_split(images, test_size=valid_size, random_state=42)\n",
        "        train_y, valid_y = train_test_split(masks, test_size=valid_size, random_state=42)\n",
        "\n",
        "        train_x, test_x = train_test_split(train_x, test_size=test_size, random_state=42)\n",
        "        train_y, test_y = train_test_split(train_y, test_size=test_size, random_state=42)\n",
        "        print(len(train_x), len(valid_x), len(test_x))\n",
        "        return (train_x, train_y), (valid_x, valid_y), (test_x, test_y)\n",
        "\n",
        "    def test(self,model, loader, loss_fn, device):\n",
        "        pass\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ut = UnetTrain()\n",
        "\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    ut.seeding(42)\n",
        "    \"\"\" Hyperparameters \"\"\"\n",
        "    batch_size = 2\n",
        "    num_epochs = 50\n",
        "    lr = 1e-4\n",
        "\n",
        "\n",
        "    \"\"\" Paths \"\"\"\n",
        "    path = \"F:\\\\Idiot Developer\\\\radioGenomic\\\\Segementation\"\n",
        "    data_path = os.path.join(path,\"data\",\"full data\")\n",
        "    checkpoint_path = os.path.join(path, \"files\", \"checkpoint.pth\")\n",
        "\n",
        "\n",
        "    print(data_path)\n",
        "\n",
        "\n",
        "    # \"\"\" Dataset and loader \"\"\"\n",
        "    # # image = torch.load(\"F:\\\\Idiot Developer\\\\radioGenomic\\\\files\\\\segmentation\\\\image.pt\")\n",
        "    # # mask = torch.load(\"F:\\\\Idiot Developer\\\\radioGenomic\\\\files\\\\segmentation\\\\mask.pt\")\n",
        "    (train_x, train_y), (valid_x, valid_y), (test_x, test_y) = ut.load_paths(data_path)\n",
        "    print(train_x)\n",
        "    print(train_y)\n",
        "    # # image = cv2.imread(train_x[0], cv2.IMREAD_GRAYSCALE)\n",
        "    # # mask = cv2.imread(train_y[0], cv2.IMREAD_GRAYSCALE)\n",
        "    # # print(image.shape)\n",
        "    # # print(mask.shape)\n",
        "    # # plt.figure()\n",
        "    # # plt.subplot(1,2,1)\n",
        "    # # plt.imshow(image,cmap=\"gray\")\n",
        "    # # plt.subplot(1,2,2)\n",
        "    # # plt.imshow(mask,cmap=\"gray\")\n",
        "    # # plt.show()\n",
        "    # # patient_dataset = PatientDataset(image, mask)\n",
        "    train_dataset = PatientDataset(train_x,train_y)\n",
        "    valid_dataset = PatientDataset(valid_x, valid_y)\n",
        "    #\n",
        "    # # train_dataset, valid_dataset = torch.utils.data.random_split(patient_dataset, [0.8, 0.2])\n",
        "    # #\n",
        "    # #\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=os.cpu_count()\n",
        "    )\n",
        "    #\n",
        "    valid_loader = DataLoader(\n",
        "        dataset=valid_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=os.cpu_count()\n",
        "    )\n",
        "    # Make device agnostic code\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = UNet(1,1)\n",
        "    model = model.to(device)\n",
        "    # #\n",
        "    \"\"\" Loss Function and Optimizer \"\"\"\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n",
        "    loss_fn = DiceBCELoss()\n",
        "    # #\n",
        "    # \"\"\" Training the model \"\"\"\n",
        "    best_valid_loss = float(\"inf\")\n",
        "    #\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss = ut.train(model, train_loader, optimizer, loss_fn, device)\n",
        "        valid_loss = ut.evaluate(model, valid_loader, loss_fn, device)\n",
        "    #\n",
        "        \"\"\" Saving the model \"\"\"\n",
        "        if valid_loss < best_valid_loss:\n",
        "            data_str = f\"Valid loss improved from {best_valid_loss:2.4f} to {valid_loss:2.4f}. Saving checkpoint: {checkpoint_path}\"\n",
        "            print(data_str)\n",
        "\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "        end_time = time.time()\n",
        "        epoch_mins, epoch_secs = ut.epoch_time(start_time, end_time)\n",
        "\n",
        "        data_str = f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\\n'\n",
        "        data_str += f'\\tTrain Loss: {train_loss:.3f}\\n'\n",
        "        data_str += f'\\t Val. Loss: {valid_loss:.3f}\\n'\n",
        "        print(data_str)"
      ]
    }
  ]
}