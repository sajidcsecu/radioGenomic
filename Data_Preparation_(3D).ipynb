{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPWJTsgy24/3ET8F8EjllZU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sajidcsecu/radioGenomic/blob/main/Data_Preparation_(3D).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (1) Install Required Libraries"
      ],
      "metadata": {
        "id": "Eq97t6sueacq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SimpleITK\n",
        "!pip install pydicom===2.4.3\n",
        "!pip install pydicom-seg\n",
        "!pip install numpy==1.23.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "S9EC6i1nfcRB",
        "outputId": "6baac3d4-0bd7-4167-f0a6-c63cb71200ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.11/dist-packages (2.4.1)\n",
            "Requirement already satisfied: pydicom===2.4.3 in /usr/local/lib/python3.11/dist-packages (2.4.3)\n",
            "Requirement already satisfied: pydicom-seg in /usr/local/lib/python3.11/dist-packages (0.4.1)\n",
            "Requirement already satisfied: SimpleITK>1.2.4 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (2.4.1)\n",
            "Requirement already satisfied: jsonschema<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (3.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (1.23.5)\n",
            "Requirement already satisfied: pydicom>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (2.4.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (25.3.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (0.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (75.2.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (1.17.0)\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (2) Import Required Libraries"
      ],
      "metadata": {
        "id": "pwZI2It_JJ-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pydicom\n",
        "import pydicom_seg\n",
        "import SimpleITK as sitk\n",
        "import h5py\n",
        "import logging\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "metadata": {
        "id": "D1njhVn-edBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (3) Mount Google Drive"
      ],
      "metadata": {
        "id": "ikPOT8l5eXax"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYQhOOWMdfu5",
        "outputId": "332a6741-a69d-498c-c28d-cfef43f672d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            " Dataset Directory: /content/drive/MyDrive/PhDwork/datasets\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define dataset directory\n",
        "DATASET_DIR = \"/content/drive/MyDrive/PhDwork/datasets\"\n",
        "os.makedirs(DATASET_DIR, exist_ok=True)\n",
        "print(f\" Dataset Directory: {DATASET_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (4) Define Dataset Class with Optimized Mask Storage"
      ],
      "metadata": {
        "id": "VV3wNxCyeogU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PreparePatientDataset(Dataset):\n",
        "    def __init__(self, patient_ids, metadata_df, train=True):\n",
        "        self.patient_ids = patient_ids\n",
        "        self.metadata_df = metadata_df\n",
        "        self.train = train\n",
        "        self.volume_data = self.load_volumes()\n",
        "\n",
        "\n",
        "    def get_path(self, subject, modality):\n",
        "        filtered = subject[subject['Modality'] == modality]\n",
        "        if filtered.empty:\n",
        "            return None\n",
        "        path = os.path.normpath(filtered['File Location'].iloc[0]).replace(\"\\\\\", \"/\").strip()\n",
        "        return path if os.path.exists(path) else None\n",
        "\n",
        "\n",
        "    def read_ct_volume(self, path):\n",
        "        reader = sitk.ImageSeriesReader()\n",
        "        reader.SetImageIO(\"GDCMImageIO\")\n",
        "        dicom_names = reader.GetGDCMSeriesFileNames(path)\n",
        "        reader.SetFileNames(dicom_names)\n",
        "        image = reader.Execute()\n",
        "        return sitk.GetArrayFromImage(image)\n",
        "\n",
        "    def read_seg_volume(self, path,seg_type=\"GTV-1\"):\n",
        "        seg_file = os.path.join(path, '1-1.dcm')\n",
        "        segmentation = pydicom.dcmread(seg_file)\n",
        "        seg_df = pd.DataFrame({\n",
        "            f: [s[f].value for s in segmentation.SegmentSequence]\n",
        "            for f in ['SegmentNumber', 'SegmentDescription']\n",
        "        })\n",
        "        match = seg_df.loc[seg_df['SegmentDescription'] == seg_type]\n",
        "        if match.empty:\n",
        "            raise ValueError(f\"No segment of type {seg_type} found.\")\n",
        "        seg_number = match['SegmentNumber'].iloc[0]\n",
        "        mask = pydicom_seg.SegmentReader().read(segmentation).segment_image(seg_number)\n",
        "        return sitk.GetArrayFromImage(mask)\n",
        "\n",
        "    def load_volumes(self):\n",
        "        all_data = []\n",
        "        for pid in self.patient_ids:\n",
        "            try:\n",
        "                subject = self.metadata_df[self.metadata_df['Subject ID'] == pid]\n",
        "                ct_path = self.get_path(subject, \"CT\")\n",
        "                seg_path = self.get_path(subject, \"SEG\")\n",
        "                if ct_path is None or seg_path is None:\n",
        "                    logging.warning(f\"‚ö†Ô∏è Skipping {pid} due to missing CT or SEG path\")\n",
        "                    continue\n",
        "                ct_volume = self.read_ct_volume(ct_path)\n",
        "                seg_volume = self.read_seg_volume(seg_path)\n",
        "\n",
        "                min_slices = min(ct_volume.shape[0], seg_volume.shape[0])\n",
        "                ct_volume, seg_volume = ct_volume[:min_slices], seg_volume[:min_slices]\n",
        "                if self.train == True:\n",
        "                    tumor_slice_index = np.where(np.any(seg_volume, axis=(1, 2)))[0]\n",
        "                    ct_volume = ct_volume[tumor_slice_index]\n",
        "                    seg_volume = seg_volume[tumor_slice_index]\n",
        "\n",
        "                if self.train and np.sum(seg_volume) == 0:\n",
        "                    logging.info(f\"üö´ Skipping {pid} (no tumor)\")\n",
        "                    continue\n",
        "\n",
        "                # ct_volume = self.normalize_volume(ct_volume)\n",
        "                all_data.append((ct_volume.astype(np.float32), seg_volume.astype(np.uint8), pid))\n",
        "                logging.info(f\"‚úÖ Loaded {pid} | Volume shape: {ct_volume.shape}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"‚ö†Ô∏è Failed to load {pid}: {e}\")\n",
        "\n",
        "        return all_data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.volume_data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.volume_data)\n",
        "\n",
        "    def save_hdf5(self, output_path):\n",
        "        with h5py.File(output_path, \"w\") as h5f:\n",
        "            for i, (ct, mask, pid) in enumerate(self.volume_data):\n",
        "                h5f.create_dataset(f\"{pid}/ct\", data=ct, compression=\"gzip\")\n",
        "                h5f.create_dataset(f\"{pid}/mask\", data=mask, compression=\"gzip\")\n",
        "                h5f[f\"{pid}\"].attrs[\"shape\"] = ct.shape\n",
        "                logging.info(f\"üíæ Saved patient {pid} | Shape: {ct.shape}\")\n"
      ],
      "metadata": {
        "id": "OSsfQa0EeqhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (5) Merged Multiple Files"
      ],
      "metadata": {
        "id": "FKGNy_fgVVBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_hdf5_files(input_files, output_file):\n",
        "    with h5py.File(output_file, \"w\") as f_out:\n",
        "        for file in input_files:\n",
        "            with h5py.File(file, \"r\") as f:\n",
        "                for pid in f.keys():\n",
        "                    grp = f_out.create_group(pid)\n",
        "                    f_out[pid].create_dataset(\"ct\", data=f[pid][\"ct\"][:], compression=\"gzip\")\n",
        "                    f_out[pid].create_dataset(\"mask\", data=f[pid][\"mask\"][:], compression=\"gzip\")\n",
        "                    f_out[pid].attrs[\"shape\"] = f[pid].attrs[\"shape\"]\n",
        "                    logging.info(f\"üß¨ Merged patient {pid} | Shape: {f[pid]['ct'].shape}\")\n",
        "\n",
        "    logging.info(f\"\\n‚úÖ Merged HDF5 saved: {output_file}\")"
      ],
      "metadata": {
        "id": "W06Sheg7VUGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (6) PROCESS & SAVE TRAIN DATASET in HDF5 format"
      ],
      "metadata": {
        "id": "OJ575YgJFV7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Define the target directory\n",
        "    target_dir = \"/content/drive/MyDrive/PhDwork\"\n",
        "    # Change to the target directory\n",
        "    os.chdir(target_dir)\n",
        "    # Verify the change\n",
        "    print(\"Current Directory:\", os.getcwd())\n",
        "\n",
        "    # Metadata\n",
        "    metadata_lung1 = pd.read_csv(\"/content/drive/MyDrive/PhDwork/metadata/metadata_lung1.csv\")\n",
        "    patient_list_lung1 = metadata_lung1[\"Subject ID\"].unique().tolist()\n",
        "\n",
        "    train_patient, valid_patient = train_test_split(patient_list_lung1, test_size=0.1, random_state=42)\n",
        "    train_patient, test_patient = train_test_split(train_patient, test_size=0.1, random_state=42)\n",
        "    print(f\"Patients: Train={len(train_patient)}, Valid={len(valid_patient)}, Test={len(test_patient)}\")\n",
        "\n",
        "    # Define DATASET_DIR\n",
        "    DATASET_DIR = os.path.join(target_dir, \"datasets/allPatients\")  # Create a datasets subfolder\n",
        "    os.makedirs(DATASET_DIR, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "\n",
        "    # Create Train Dataset\n",
        "    print(\"Loading Training Data...\")\n",
        "    start_time_train = datetime.now()\n",
        "    train_dataset = PreparePatientDataset(train_patient, metadata_lung1, train=True)  # removed device argument\n",
        "    train_path = os.path.join(DATASET_DIR, \"train_dataset.hdf5\")\n",
        "    train_dataset.save_hdf5(train_path)\n",
        "    end_time_train = datetime.now()\n",
        "    duration_train = end_time_train - start_time_train\n",
        "    print(f\"Training dataset creation time: {duration_train}\")\n",
        "\n",
        "    # Create Valid Dataset\n",
        "    print(\"Loading valid Data...\")\n",
        "    start_time_valid = datetime.now()\n",
        "    valid_dataset = PreparePatientDataset(valid_patient, metadata_lung1, train=False) #removed device argument\n",
        "    valid_path = os.path.join(DATASET_DIR, \"valid_dataset.hdf5\")\n",
        "    valid_dataset.save_hdf5(valid_path)\n",
        "    end_time_valid = datetime.now()\n",
        "    duration_valid = end_time_valid - start_time_valid\n",
        "    print(f\"Validation dataset creation time: {duration_valid}\")\n",
        "\n",
        "    # Create Test Dataset\n",
        "    print(\"Loading test Data...\")\n",
        "    start_time_test = datetime.now()\n",
        "    test_dataset = PreparePatientDataset(test_patient, metadata_lung1, train=False) #removed device argument\n",
        "    test_path = os.path.join(DATASET_DIR, \"test_dataset.hdf5\")\n",
        "    test_dataset.save_hdf5(test_path)\n",
        "    end_time_test = datetime.now()\n",
        "    duration_test = end_time_test - start_time_test\n",
        "    print(f\"Test dataset creation time: {duration_test}\")"
      ],
      "metadata": {
        "id": "Mt_owGxsFU-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45b89462-384a-4863-c20a-0768791258eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Directory: /content/drive/MyDrive/PhDwork\n",
            "Patients: Train=1, Valid=1, Test=1\n",
            "Loading Training Data...\n",
            "‚úÖ HDF5 file saved at /content/drive/MyDrive/PhDwork/datasets/allPatients/train_dataset.hdf5\n",
            "Training dataset creation time: 0:05:17.565594\n",
            "Loading valid Data...\n",
            "‚úÖ HDF5 file saved at /content/drive/MyDrive/PhDwork/datasets/allPatients/valid_dataset.hdf5\n",
            "Validation dataset creation time: 0:08:05.516331\n",
            "Loading test Data...\n",
            "‚úÖ HDF5 file saved at /content/drive/MyDrive/PhDwork/datasets/allPatients/test_dataset.hdf5\n",
            "Test dataset creation time: 0:05:30.574955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ua_HRykvedQ6"
      }
    }
  ]
}