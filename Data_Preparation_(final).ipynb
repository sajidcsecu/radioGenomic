{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPVxiNYtYQmoIvtfpWYg5ZE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sajidcsecu/radioGenomic/blob/main/Data_Preparation_(final).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (1) Install Required Libraries"
      ],
      "metadata": {
        "id": "Eq97t6sueacq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SimpleITK\n",
        "!pip install pydicom===2.4.3\n",
        "!pip install pydicom-seg\n",
        "!pip install numpy==1.23.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "S9EC6i1nfcRB",
        "outputId": "6baac3d4-0bd7-4167-f0a6-c63cb71200ac"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.11/dist-packages (2.4.1)\n",
            "Requirement already satisfied: pydicom===2.4.3 in /usr/local/lib/python3.11/dist-packages (2.4.3)\n",
            "Requirement already satisfied: pydicom-seg in /usr/local/lib/python3.11/dist-packages (0.4.1)\n",
            "Requirement already satisfied: SimpleITK>1.2.4 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (2.4.1)\n",
            "Requirement already satisfied: jsonschema<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (3.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (1.23.5)\n",
            "Requirement already satisfied: pydicom>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (2.4.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (25.3.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (0.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (75.2.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (1.17.0)\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (2) Import Required Libraries"
      ],
      "metadata": {
        "id": "pwZI2It_JJ-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pydicom\n",
        "import pydicom_seg\n",
        "import SimpleITK as sitk\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import h5py\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "from multiprocessing import Pool"
      ],
      "metadata": {
        "id": "D1njhVn-edBG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (3) Mount Google Drive"
      ],
      "metadata": {
        "id": "ikPOT8l5eXax"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYQhOOWMdfu5",
        "outputId": "332a6741-a69d-498c-c28d-cfef43f672d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            " Dataset Directory: /content/drive/MyDrive/PhDwork/datasets\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define dataset directory\n",
        "DATASET_DIR = \"/content/drive/MyDrive/PhDwork/datasets\"\n",
        "os.makedirs(DATASET_DIR, exist_ok=True)\n",
        "print(f\" Dataset Directory: {DATASET_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (4) Define Dataset Class with Optimized Mask Storage"
      ],
      "metadata": {
        "id": "VV3wNxCyeogU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PreparePatientDataset(Dataset):\n",
        "    def __init__(self, patients, metadata, train=True):\n",
        "        self.patients = patients\n",
        "        self.metadata = metadata\n",
        "        self.train = train\n",
        "        self.slices = self._extract_slices()\n",
        "\n",
        "    def get_path(self, subject, modality):\n",
        "        subject_filtered = subject[subject['Modality'] == modality]\n",
        "        if subject_filtered.empty:\n",
        "            return None\n",
        "\n",
        "        path = os.path.normpath(subject_filtered['File Location'].iloc[0])\n",
        "        path = path.replace(\"\\\\\", \"/\")\n",
        "        path = path.strip()\n",
        "\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"❌ WARNING: Path does not exist: {path}\")\n",
        "\n",
        "        return path if os.path.exists(path) else None\n",
        "\n",
        "    def get_num_slices(self, path, modality):\n",
        "        path = os.path.normpath(path).replace(\"\\\\\", \"/\")\n",
        "\n",
        "        if modality == \"CT\":\n",
        "            if not os.path.exists(path):\n",
        "                print(f\"❌ WARNING: CT folder not found: {path}\")\n",
        "                return 0\n",
        "            if not os.path.isdir(path):\n",
        "                print(f\"❌ ERROR: Expected a folder but found a file: {path}\")\n",
        "                return 0\n",
        "            return len([f for f in os.listdir(path) if f.endswith('.dcm')])\n",
        "\n",
        "        elif modality == \"SEG\":\n",
        "            seg_file = os.path.join(path, '1-1.dcm')\n",
        "            if not os.path.exists(seg_file):\n",
        "                print(f\"❌ WARNING: SEG file not found: {seg_file}\")\n",
        "                return 0\n",
        "            try:\n",
        "                segmentation = pydicom.dcmread(seg_file)\n",
        "                mask_data = pydicom_seg.SegmentReader().read(segmentation)\n",
        "                return mask_data.segment_image(1).GetDepth()\n",
        "            except Exception as e:\n",
        "                print(f\"❌ ERROR reading segmentation {seg_file}: {e}\")\n",
        "                return 0\n",
        "\n",
        "    def has_tumor(self, seg_path):\n",
        "        \"\"\"\n",
        "        Checks if a segmentation mask contains tumor regions.\n",
        "        Args:\n",
        "            seg_path (str): Path to the segmentation DICOM file.\n",
        "        Returns:\n",
        "            np.ndarray: Boolean array where True indicates a non-empty slice.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            mask = self.read_seg_array(seg_path, slice_idx=None, seg_type=\"GTV-1\")  # Load full mask\n",
        "            return np.any(mask, axis=(1, 2))  # Check per-slice if any pixel is nonzero (tumor present)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ ERROR in has_tumor(): {e}\")\n",
        "            return np.zeros(1, dtype=bool)  # Default to no tumor if error occurs\n",
        "\n",
        "    def _extract_slices(self):\n",
        "        slices = []\n",
        "        patient_paths = {p: self.metadata[self.metadata['Subject ID'] == p] for p in self.patients}\n",
        "\n",
        "        for patient, subject in patient_paths.items():\n",
        "            img_path = self.get_path(subject, \"CT\")\n",
        "            msk_path = self.get_path(subject, \"SEG\")\n",
        "\n",
        "            if img_path and msk_path:\n",
        "                min_slices = min(self.get_num_slices(img_path, \"CT\"), self.get_num_slices(msk_path, \"SEG\"))\n",
        "                slice_indices = np.arange(min_slices) if not self.train else np.where(self.has_tumor(msk_path))[0]\n",
        "                slices.extend(zip([img_path] * len(slice_indices), [msk_path] * len(slice_indices), slice_indices))\n",
        "\n",
        "        return slices\n",
        "\n",
        "    def read_ct_array(self, path, slice_idx):\n",
        "        reader = sitk.ImageSeriesReader()\n",
        "        reader.SetImageIO(\"GDCMImageIO\")\n",
        "        path = os.path.normpath(path).replace(\"\\\\\", \"/\")\n",
        "        dicom_names = reader.GetGDCMSeriesFileNames(path)\n",
        "        reader.SetFileNames(dicom_names)\n",
        "        image = reader.Execute()\n",
        "        return sitk.GetArrayFromImage(image)[slice_idx].astype(np.float32)\n",
        "\n",
        "    def read_seg_array(self, path, slice_idx, seg_type=\"GTV-1\"):\n",
        "        path = os.path.normpath(path).replace(\"\\\\\", \"/\")\n",
        "        try:\n",
        "            segmentation = pydicom.dcmread(os.path.join(path, '1-1.dcm'))\n",
        "            seg_df = pd.DataFrame({f: [s[f].value for s in segmentation.SegmentSequence] for f in ['SegmentNumber', 'SegmentDescription']})\n",
        "            seg_number = seg_df.loc[seg_df['SegmentDescription'] == seg_type, 'SegmentNumber'].iloc[0]\n",
        "            mask = pydicom_seg.SegmentReader().read(segmentation).segment_image(seg_number)\n",
        "            return sitk.GetArrayFromImage(mask)[slice_idx].astype(np.uint8)  # Use uint8 to save space\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading segmentation from {path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path, slice_idx = self.slices[idx]\n",
        "        img = self.read_ct_array(img_path, slice_idx)\n",
        "        msk = self.read_seg_array(mask_path, slice_idx, \"GTV-1\")\n",
        "\n",
        "        if img is None or msk is None:\n",
        "            img = torch.zeros((512, 512), dtype=torch.float16)\n",
        "            msk = torch.zeros((512, 512), dtype=torch.uint8)\n",
        "            return img.unsqueeze(0), msk.unsqueeze(0)\n",
        "\n",
        "        img = (img - img.min()) / max(img.max(), 1e-6)\n",
        "        img = torch.tensor(img, dtype=torch.float32 ).unsqueeze(0) # add channel dim\n",
        "        msk = torch.tensor(msk, dtype=torch.uint8).unsqueeze(0) # add channel dim\n",
        "\n",
        "        return img, msk\n",
        "    def __len__(self):\n",
        "            return len(self.slices)\n",
        "\n",
        "    @staticmethod\n",
        "    def process_slice(slice_data):\n",
        "        img, msk = slice_data\n",
        "        img_np = img.squeeze(0).numpy()\n",
        "        msk_np = msk.squeeze(0).numpy().astype(np.uint8)\n",
        "        return img_np, msk_np\n",
        "\n",
        "    def save_hdf5(self, output_file):\n",
        "        slice_data = [self[i] for i in range(len(self))]\n",
        "\n",
        "        with Pool() as pool:\n",
        "            results = pool.map(PreparePatientDataset.process_slice, slice_data) #changed to class.process_slice\n",
        "\n",
        "        images, masks = zip(*results)\n",
        "        images = np.stack(images)\n",
        "        masks = np.stack(masks)\n",
        "\n",
        "        with h5py.File(output_file, \"w\") as f:\n",
        "            f.create_dataset(\"images\", data=images, compression=\"gzip\", compression_opts=9)\n",
        "            f.create_dataset(\"masks\", data=masks, compression=\"gzip\", compression_opts=9)\n",
        "\n",
        "        print(f\"✅ HDF5 file saved at {output_file}\")"
      ],
      "metadata": {
        "id": "OSsfQa0EeqhM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (5) PROCESS & SAVE TRAIN DATASET in HDF5 format"
      ],
      "metadata": {
        "id": "OJ575YgJFV7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Define the target directory\n",
        "    target_dir = \"/content/drive/MyDrive/PhDwork\"\n",
        "    # Change to the target directory\n",
        "    os.chdir(target_dir)\n",
        "    # Verify the change\n",
        "    print(\"Current Directory:\", os.getcwd())\n",
        "\n",
        "    # Metadata\n",
        "    metadata_lung1 = pd.read_csv(\"/content/drive/MyDrive/PhDwork/metadata/metadata_lung1.csv\")\n",
        "    patient_list_lung1 = metadata_lung1[\"Subject ID\"].unique().tolist()\n",
        "\n",
        "    train_patient, valid_patient = train_test_split(patient_list_lung1, test_size=0.1, random_state=42)\n",
        "    train_patient, test_patient = train_test_split(train_patient, test_size=0.1, random_state=42)\n",
        "    print(f\"Patients: Train={len(train_patient)}, Valid={len(valid_patient)}, Test={len(test_patient)}\")\n",
        "\n",
        "    # Define DATASET_DIR\n",
        "    DATASET_DIR = os.path.join(target_dir, \"datasets/allPatients\")  # Create a datasets subfolder\n",
        "    os.makedirs(DATASET_DIR, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "\n",
        "    # Create Train Dataset\n",
        "    print(\"Loading Training Data...\")\n",
        "    start_time_train = datetime.now()\n",
        "    train_dataset = PreparePatientDataset(train_patient, metadata_lung1, train=True)  # removed device argument\n",
        "    train_path = os.path.join(DATASET_DIR, \"train_dataset.hdf5\")\n",
        "    train_dataset.save_hdf5(train_path)\n",
        "    end_time_train = datetime.now()\n",
        "    duration_train = end_time_train - start_time_train\n",
        "    print(f\"Training dataset creation time: {duration_train}\")\n",
        "\n",
        "    # Create Valid Dataset\n",
        "    print(\"Loading valid Data...\")\n",
        "    start_time_valid = datetime.now()\n",
        "    valid_dataset = PreparePatientDataset(valid_patient, metadata_lung1, train=False) #removed device argument\n",
        "    valid_path = os.path.join(DATASET_DIR, \"valid_dataset.hdf5\")\n",
        "    valid_dataset.save_hdf5(valid_path)\n",
        "    end_time_valid = datetime.now()\n",
        "    duration_valid = end_time_valid - start_time_valid\n",
        "    print(f\"Validation dataset creation time: {duration_valid}\")\n",
        "\n",
        "    # Create Test Dataset\n",
        "    print(\"Loading test Data...\")\n",
        "    start_time_test = datetime.now()\n",
        "    test_dataset = PreparePatientDataset(test_patient, metadata_lung1, train=False) #removed device argument\n",
        "    test_path = os.path.join(DATASET_DIR, \"test_dataset.hdf5\")\n",
        "    test_dataset.save_hdf5(test_path)\n",
        "    end_time_test = datetime.now()\n",
        "    duration_test = end_time_test - start_time_test\n",
        "    print(f\"Test dataset creation time: {duration_test}\")"
      ],
      "metadata": {
        "id": "Mt_owGxsFU-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45b89462-384a-4863-c20a-0768791258eb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Directory: /content/drive/MyDrive/PhDwork\n",
            "Patients: Train=1, Valid=1, Test=1\n",
            "Loading Training Data...\n",
            "✅ HDF5 file saved at /content/drive/MyDrive/PhDwork/datasets/allPatients/train_dataset.hdf5\n",
            "Training dataset creation time: 0:05:17.565594\n",
            "Loading valid Data...\n",
            "✅ HDF5 file saved at /content/drive/MyDrive/PhDwork/datasets/allPatients/valid_dataset.hdf5\n",
            "Validation dataset creation time: 0:08:05.516331\n",
            "Loading test Data...\n",
            "✅ HDF5 file saved at /content/drive/MyDrive/PhDwork/datasets/allPatients/test_dataset.hdf5\n",
            "Test dataset creation time: 0:05:30.574955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (5) Load and Save Train Dataset"
      ],
      "metadata": {
        "id": "bGpwmn6Ee7-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset_hdf5(filename):\n",
        "    \"\"\"\n",
        "    Load images and masks from an HDF5 file.\n",
        "\n",
        "    Args:\n",
        "        filename (str): Path to the HDF5 file.\n",
        "\n",
        "    Returns:\n",
        "        images (torch.Tensor): Tensor of images.\n",
        "        masks (torch.Tensor): Tensor of masks.\n",
        "    \"\"\"\n",
        "    load_path = os.path.join(DATASET_DIR, filename)\n",
        "\n",
        "    with h5py.File(load_path, 'r') as f:\n",
        "        images = f[\"images\"][:]  # Load images\n",
        "        masks = f[\"masks\"][:]    # Load masks\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    images = torch.tensor(images, dtype=torch.float16)\n",
        "    masks = torch.tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "    print(f\"✅ Loaded {filename} from {load_path} - Shape: {images.shape}, {masks.shape}\")\n",
        "\n",
        "    return images, masks\n",
        "\n",
        "images, masks = load_dataset_hdf5(\"train_dataset.hdf5\")\n",
        "\n",
        "# Check the shape\n",
        "print(\"Images shape:\", images.shape)  # (N, 1, H, W)\n",
        "print(\"Masks shape:\", masks.shape)    # (N, 1, H, W)"
      ],
      "metadata": {
        "id": "L4LJZ9FRwUFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (6) Display Sample Slices"
      ],
      "metadata": {
        "id": "DRpQFTMhfAmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_sample_shape(dataset):\n",
        "    idx = np.random.randint(0, len(dataset))\n",
        "    img, mask = dataset[idx]\n",
        "\n",
        "    print(f\"Image Shape: {img.shape}\")  # Should be (1, H, W)\n",
        "    print(f\"Mask Shape: {mask.shape}\")  # Should be (1, H, W)\n",
        "\n",
        "display_sample_shape(train_dataset)\n"
      ],
      "metadata": {
        "id": "C1uQ7-4PfCRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ua_HRykvedQ6"
      }
    }
  ]
}