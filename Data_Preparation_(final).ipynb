{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPng2jktHIJrNUhwLJIA61a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sajidcsecu/radioGenomic/blob/main/Data_Preparation_(final).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (1) Mount Google Drive"
      ],
      "metadata": {
        "id": "ikPOT8l5eXax"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYQhOOWMdfu5",
        "outputId": "45b1e49b-c31c-4fba-c38b-1e6519eaf157"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            " Dataset Directory: /content/drive/MyDrive/PhDwork/datasets\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define dataset directory\n",
        "DATASET_DIR = \"/content/drive/MyDrive/PhDwork/datasets\"\n",
        "os.makedirs(DATASET_DIR, exist_ok=True)\n",
        "print(f\" Dataset Directory: {DATASET_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (2) Import Required Libraries"
      ],
      "metadata": {
        "id": "Eq97t6sueacq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SimpleITK\n",
        "!pip install pydicom===2.4.3\n",
        "!pip install pydicom-seg\n",
        "!pip install numpy==1.23.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "S9EC6i1nfcRB",
        "outputId": "a0461787-0479-40fe-9d02-d5a337d541e9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SimpleITK\n",
            "  Downloading SimpleITK-2.4.1-cp311-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
            "Downloading SimpleITK-2.4.1-cp311-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: SimpleITK\n",
            "Successfully installed SimpleITK-2.4.1\n",
            "Collecting pydicom===2.4.3\n",
            "  Downloading pydicom-2.4.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Downloading pydicom-2.4.3-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-2.4.3\n",
            "Collecting pydicom-seg\n",
            "  Downloading pydicom_seg-0.4.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: SimpleITK>1.2.4 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (2.4.1)\n",
            "Collecting jsonschema<4.0.0,>=3.2.0 (from pydicom-seg)\n",
            "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting numpy<2.0.0,>=1.18.0 (from pydicom-seg)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydicom>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from pydicom-seg) (2.4.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (25.3.0)\n",
            "Collecting pyrsistent>=0.14.0 (from jsonschema<4.0.0,>=3.2.0->pydicom-seg)\n",
            "  Downloading pyrsistent-0.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (75.1.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom-seg) (1.17.0)\n",
            "Downloading pydicom_seg-0.4.1-py3-none-any.whl (27 kB)\n",
            "Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyrsistent-0.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (120 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyrsistent, numpy, jsonschema, pydicom-seg\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 4.23.0\n",
            "    Uninstalling jsonschema-4.23.0:\n",
            "      Successfully uninstalled jsonschema-4.23.0\n",
            "Successfully installed jsonschema-3.2.0 numpy-1.26.4 pydicom-seg-0.4.1 pyrsistent-0.20.0\n",
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.2.0 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 1.41.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.21.1 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "c193a6e0ea5f4125a7165809f79d5bc3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "import pydicom\n",
        "import pydicom_seg\n",
        "import SimpleITK as sitk\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "D1njhVn-edBG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (2) Define Dataset Class with Optimized Mask Storage"
      ],
      "metadata": {
        "id": "VV3wNxCyeogU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PreparePatientDataset:\n",
        "    def __init__(self, patients, metadata, train=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patients (list): List of patient IDs.\n",
        "            metadata (DataFrame): Metadata containing patient information.\n",
        "            train (bool): If True, filters out empty slices.\n",
        "        \"\"\"\n",
        "        self.patients = patients\n",
        "        self.metadata = metadata\n",
        "        self.train = train\n",
        "        self.slices = self._extract_slices()  # Store (img_path, mask_path, slice_idx) tuples\n",
        "\n",
        "    def get_path(self, subject, modality):\n",
        "        subject_filtered = subject[subject['Modality'] == modality]\n",
        "        if subject_filtered.empty:\n",
        "            return None\n",
        "\n",
        "        path = os.path.normpath(subject_filtered['File Location'].iloc[0])\n",
        "        path = path.replace(\"\\\\\", \"/\")  # ✅ Convert Windows backslashes to Unix forward slashes\n",
        "        path = path.strip()  # ✅ Remove any accidental whitespace or newlines\n",
        "\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"❌ WARNING: Path does not exist: {path}\")\n",
        "\n",
        "        return path if os.path.exists(path) else None\n",
        "\n",
        "    def get_num_slices(self, path, modality):\n",
        "        path = os.path.normpath(path).replace(\"\\\\\", \"/\")\n",
        "\n",
        "        if modality == \"CT\":\n",
        "            if not os.path.exists(path):\n",
        "                print(f\"❌ WARNING: CT folder not found: {path}\")\n",
        "                return 0\n",
        "            if not os.path.isdir(path):  # ✅ Check if it's actually a directory\n",
        "                print(f\"❌ ERROR: Expected a folder but found a file: {path}\")\n",
        "                return 0\n",
        "            return len([f for f in os.listdir(path) if f.endswith('.dcm')])\n",
        "\n",
        "        elif modality == \"SEG\":\n",
        "            seg_file = os.path.join(path, '1-1.dcm')\n",
        "            if not os.path.exists(seg_file):\n",
        "                print(f\"❌ WARNING: SEG file not found: {seg_file}\")\n",
        "                return 0\n",
        "            try:\n",
        "                segmentation = pydicom.dcmread(seg_file)\n",
        "                mask_data = pydicom_seg.SegmentReader().read(segmentation)\n",
        "                return mask_data.segment_image(1).GetDepth()  # Number of slices\n",
        "            except Exception as e:\n",
        "                print(f\"❌ ERROR reading segmentation {seg_file}: {e}\")\n",
        "                return 0\n",
        "\n",
        "    def has_tumor(self, seg_path):\n",
        "        \"\"\"\n",
        "        Checks if a segmentation mask contains tumor regions.\n",
        "        Args:\n",
        "            seg_path (str): Path to the segmentation DICOM file.\n",
        "        Returns:\n",
        "            np.ndarray: Boolean array where True indicates a non-empty slice.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            mask = self.read_seg_array(seg_path, slice_idx=None, seg_type=\"GTV-1\")  # Load full mask\n",
        "            return np.any(mask, axis=(1, 2))  # Check per-slice if any pixel is nonzero (tumor present)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ ERROR in has_tumor(): {e}\")\n",
        "            return np.zeros(1, dtype=bool)  # Default to no tumor if error occurs\n",
        "\n",
        "    def _extract_slices(self):\n",
        "        slices = []\n",
        "        patient_paths = {p: self.metadata[self.metadata['Subject ID'] == p] for p in self.patients}\n",
        "\n",
        "        for patient, subject in patient_paths.items():\n",
        "            img_path = self.get_path(subject, \"CT\")\n",
        "            msk_path = self.get_path(subject, \"SEG\")\n",
        "\n",
        "            if img_path and msk_path:\n",
        "                min_slices = min(self.get_num_slices(img_path, \"CT\"), self.get_num_slices(msk_path, \"SEG\"))\n",
        "                slice_indices = np.arange(min_slices) if not self.train else np.where(self.has_tumor(msk_path))[0]\n",
        "                slices.extend(zip([img_path] * len(slice_indices), [msk_path] * len(slice_indices), slice_indices))\n",
        "\n",
        "        return slices\n",
        "\n",
        "    def read_ct_array(self, path, slice_idx):\n",
        "        reader = sitk.ImageSeriesReader()\n",
        "        reader.SetImageIO(\"GDCMImageIO\")\n",
        "        path = os.path.normpath(path).replace(\"\\\\\", \"/\")\n",
        "        dicom_names = reader.GetGDCMSeriesFileNames(path)\n",
        "        reader.SetFileNames(dicom_names)\n",
        "        image = reader.Execute()\n",
        "        return sitk.GetArrayFromImage(image)[slice_idx].astype(np.float32)\n",
        "\n",
        "    def read_seg_array(self, path, slice_idx, seg_type=\"GTV-1\"):\n",
        "        path = os.path.normpath(path).replace(\"\\\\\", \"/\")\n",
        "        try:\n",
        "            segmentation = pydicom.dcmread(os.path.join(path, '1-1.dcm'))\n",
        "            seg_df = pd.DataFrame({f: [s[f].value for s in segmentation.SegmentSequence] for f in ['SegmentNumber', 'SegmentDescription']})\n",
        "            seg_number = seg_df.loc[seg_df['SegmentDescription'] == seg_type, 'SegmentNumber'].iloc[0]\n",
        "            mask = pydicom_seg.SegmentReader().read(segmentation).segment_image(seg_number)\n",
        "            return sitk.GetArrayFromImage(mask)[slice_idx].astype(np.uint8)  # Use uint8 to save space\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading segmentation from {path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path, slice_idx = self.slices[idx]\n",
        "\n",
        "        # Read one slice at a time\n",
        "        img = self.read_ct_array(img_path, slice_idx)\n",
        "        msk = self.read_seg_array(mask_path, slice_idx, \"GTV-1\")\n",
        "\n",
        "        if img is None or msk is None:\n",
        "            return np.zeros((512, 512), dtype=np.float16), np.zeros((512, 512), dtype=np.uint8)\n",
        "\n",
        "        # Normalize image\n",
        "        img = (img - img.min()) / max(img.max(), 1e-6)  # Avoid divide-by-zero\n",
        "\n",
        "         # Convert to tensors\n",
        "        img = torch.tensor(img, dtype=torch.float16).unsqueeze(0)  # Convert to float16\n",
        "        msk = torch.tensor(msk, dtype=torch.uint8).unsqueeze(0)  # Use uint8 for masks\n",
        "\n",
        "        return img, msk\n",
        "\n",
        "\n",
        "    def save_hdf5(self, output_file):\n",
        "        \"\"\"\n",
        "        Saves dataset as HDF5.\n",
        "        \"\"\"\n",
        "        images, masks = [], []\n",
        "\n",
        "        for i in range(len(self)):\n",
        "            img, msk = self[i]  # Uses __getitem__()\n",
        "            images.append(img)\n",
        "            masks.append(msk)\n",
        "\n",
        "        images, masks = np.stack(images), np.stack(masks)\n",
        "\n",
        "        with h5py.File(output_file, \"w\") as f:\n",
        "            f.create_dataset(\"images\", data=images, compression=\"gzip\", compression_opts=9)\n",
        "            f.create_dataset(\"masks\", data=masks, compression=\"gzip\", compression_opts=9)\n",
        "\n",
        "        print(f\"✅ HDF5 file saved at {output_file}\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.slices)"
      ],
      "metadata": {
        "id": "OSsfQa0EeqhM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (4) PROCESS & SAVE TRAIN DATASET (.npz)"
      ],
      "metadata": {
        "id": "OJ575YgJFV7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "  # Define the target directory\n",
        "  target_dir = \"/content/drive/MyDrive/PhDwork\"\n",
        "  # Change to the target directory\n",
        "  os.chdir(target_dir)\n",
        "  # Verify the change\n",
        "  print(\"Current Directory:\", os.getcwd())\n",
        "  # Metadata\n",
        "  metadata_lung1 = pd.read_csv(\"/content/drive/MyDrive/PhDwork/metadata/metadata_lung1.csv\")\n",
        "  patient_list_lung1 = metadata_lung1[\"Subject ID\"].unique().tolist()[:3]\n",
        "\n",
        "  train_patient, valid_patient = train_test_split(patient_list_lung1, test_size=0.1, random_state=42)\n",
        "  train_patient, test_patient = train_test_split(train_patient, test_size=0.1, random_state=42)\n",
        "  print(f\"Patients: Train={len(train_patient)}, Valid={len(valid_patient)}, Test={len(test_patient)}\")\n",
        "\n",
        "  # Create Train Dataset\n",
        "  print(\"Loading Training Data...\")\n",
        "  train_dataset = PreparePatientDataset(train_patient, metadata_lung1, train=True)\n",
        "  train_path = os.path.join(DATASET_DIR, \"train_dataset.hdf5\")\n",
        "  train_dataset.save_hdf5(train_path)\n",
        "\n",
        "   # Create Valid Dataset\n",
        "  print(\"Loading valid Data...\")\n",
        "  valid_dataset = PreparePatientDataset(valid_patient, metadata_lung1, train=False)\n",
        "  valid_path = os.path.join(DATASET_DIR, \"valid_dataset.hdf5\")\n",
        "  valid_dataset.save_hdf5(valid_path)\n",
        "\n",
        "  # Create Test Dataset\n",
        "  print(\"Loading test Data...\")\n",
        "  test_dataset = PreparePatientDataset(test_patient, metadata_lung1, train=False)\n",
        "  test_path = os.path.join(DATASET_DIR, \"test_dataset.hdf5\")\n",
        "  test_dataset.save_hdf5(test_path)\n"
      ],
      "metadata": {
        "id": "Mt_owGxsFU-9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (5) Load and Save Train Dataset"
      ],
      "metadata": {
        "id": "bGpwmn6Ee7-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset_hdf5(filename):\n",
        "    \"\"\"\n",
        "    Load images and masks from an HDF5 file.\n",
        "\n",
        "    Args:\n",
        "        filename (str): Path to the HDF5 file.\n",
        "\n",
        "    Returns:\n",
        "        images (torch.Tensor): Tensor of images.\n",
        "        masks (torch.Tensor): Tensor of masks.\n",
        "    \"\"\"\n",
        "    load_path = os.path.join(DATASET_DIR, filename)\n",
        "\n",
        "    with h5py.File(load_path, 'r') as f:\n",
        "        images = f[\"images\"][:]  # Load images\n",
        "        masks = f[\"masks\"][:]    # Load masks\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    images = torch.tensor(images, dtype=torch.float16).unsqueeze(1)\n",
        "    masks = torch.tensor(masks, dtype=torch.uint8).unsqueeze(1)\n",
        "\n",
        "    print(f\"✅ Loaded {filename} from {load_path} - Shape: {images.shape}, {masks.shape}\")\n",
        "\n",
        "    return images, masks\n",
        "\n",
        "images, masks = load_dataset_hdf5(\"train_dataset.hdf5\")\n",
        "\n",
        "# Check the shape\n",
        "print(\"Images shape:\", images.shape)  # (N, 1, H, W)\n",
        "print(\"Masks shape:\", masks.shape)    # (N, 1, H, W)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4LJZ9FRwUFY",
        "outputId": "1337a006-44f7-4566-e66b-efb0b4e51242"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded train_dataset.hdf5 from /content/drive/MyDrive/PhDwork/datasets/train_dataset.hdf5 - Shape: torch.Size([100, 1, 1, 512, 512]), torch.Size([100, 1, 1, 512, 512])\n",
            "Images shape: torch.Size([100, 1, 1, 512, 512])\n",
            "Masks shape: torch.Size([100, 1, 1, 512, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (6) Display Sample Slices"
      ],
      "metadata": {
        "id": "DRpQFTMhfAmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_sample_shape(dataset):\n",
        "    idx = np.random.randint(0, len(dataset))\n",
        "    img, mask = dataset[idx]\n",
        "\n",
        "    print(f\"Image Shape: {img.shape}\")  # Should be (1, H, W)\n",
        "    print(f\"Mask Shape: {mask.shape}\")  # Should be (1, H, W)\n",
        "\n",
        "display_sample_shape(train_dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1uQ7-4PfCRx",
        "outputId": "cd26ebbf-96d8-4f4d-92f5-e42426943927"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Shape: torch.Size([1, 512, 512])\n",
            "Mask Shape: torch.Size([1, 512, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ua_HRykvedQ6"
      }
    }
  ]
}