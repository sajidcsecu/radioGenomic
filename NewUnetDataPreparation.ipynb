{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaNdUaPDjEzMDgcH9rWqKY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sajidcsecu/radioGenomic/blob/main/NewUnetDataPreparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Yaep646tk-L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from NewRadioImage import RadioImage\n",
        "import SimpleITK as sitk\n",
        "import pydicom\n",
        "import pydicom_seg\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from itertools import chain\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from albumentations import HorizontalFlip, VerticalFlip, Rotate\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "\n",
        "# class PatientDataset(Dataset):\n",
        "#     def __init__(self, image, mask):\n",
        "#         self.image = image\n",
        "#         self.mask = mask\n",
        "#         self.n_samples = len(self.image)\n",
        "#\n",
        "#     def __getitem__(self, index):\n",
        "#         img = torch.unsqueeze(self.image[index,:,:], 0)\n",
        "#         msk = torch.unsqueeze(self.mask[index,:,:], 0)\n",
        "#         return img,msk\n",
        "#\n",
        "#     def __len__(self):\n",
        "#         return self.n_samples\n",
        "\n",
        "class PatientDataset2DUNet(Dataset):\n",
        "    def __init__(self, patients, metadata, train=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patients (list): List of patient IDs.\n",
        "            metadata (DataFrame): Metadata containing patient information.\n",
        "            train (bool): If True, filters out empty slices.\n",
        "        \"\"\"\n",
        "        self.patients = patients\n",
        "        self.metadata = metadata\n",
        "        self.train = train\n",
        "        self.slices = self._extract_slices()  # Store (img_path, mask_path, slice_idx) tuples\n",
        "\n",
        "    def get_path(self, subject, modality):\n",
        "        subject_filtered = subject[subject['Modality'] == modality]\n",
        "        return subject_filtered['File Location'].iloc[0] if not subject_filtered.empty else None\n",
        "\n",
        "    def _extract_slices(self):\n",
        "        slices = []\n",
        "        for patient in self.patients:\n",
        "            print(f\"Processing Patient: {patient}\")\n",
        "            subject = self.metadata[self.metadata['Subject ID'] == patient]\n",
        "\n",
        "            img_path = self.get_path(subject, \"CT\")\n",
        "            msk_path = self.get_path(subject, \"SEG\")\n",
        "\n",
        "            if img_path and msk_path:\n",
        "                img = self.read_ct_array(img_path)\n",
        "                msk = self.read_seg_array(msk_path, \"GTV-1\")\n",
        "\n",
        "                if img is not None and msk is not None:\n",
        "                    image = sitk.GetArrayFromImage(img).astype(np.float32)\n",
        "                    mask = sitk.GetArrayFromImage(msk).astype(np.float32)\n",
        "\n",
        "                    min_slices = min(image.shape[0], mask.shape[0])\n",
        "                    image, mask = image[:min_slices], mask[:min_slices]\n",
        "\n",
        "                    # Vectorized filtering of empty slices\n",
        "                    slice_indices = np.arange(min_slices) if not self.train else np.where(np.any(mask, axis=(1, 2)))[0]\n",
        "\n",
        "                    # Store (image_path, mask_path, slice_index)\n",
        "                    slices.extend(zip([img_path] * len(slice_indices), [msk_path] * len(slice_indices), slice_indices))\n",
        "\n",
        "        return slices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.slices)\n",
        "\n",
        "    def read_ct_array(self, path):\n",
        "        reader = sitk.ImageSeriesReader()\n",
        "        reader.SetImageIO(\"GDCMImageIO\")\n",
        "        reader.SetFileNames(reader.GetGDCMSeriesFileNames(path))\n",
        "        return reader.Execute()\n",
        "\n",
        "    def read_seg_array(self, path, seg_type=\"GTV-1\"):\n",
        "        try:\n",
        "            segmentation = pydicom.dcmread(os.path.join(path, '1-1.dcm'))\n",
        "            seg_df = pd.DataFrame({f: [s[f].value for s in segmentation.SegmentSequence] for f in ['SegmentNumber', 'SegmentDescription']})\n",
        "            seg_number = seg_df.loc[seg_df['SegmentDescription'] == seg_type, 'SegmentNumber'].iloc[0]\n",
        "            return pydicom_seg.SegmentReader().read(segmentation).segment_image(seg_number)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading segmentation from {path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path, slice_idx = self.slices[idx]\n",
        "\n",
        "        # Load the full volume but extract only one slice\n",
        "        img = self.read_ct_array(img_path)\n",
        "        msk = self.read_seg_array(mask_path, \"GTV-1\")\n",
        "\n",
        "        if img is None or msk is None:\n",
        "            return None\n",
        "\n",
        "        image = sitk.GetArrayFromImage(img).astype(np.float32)\n",
        "        mask = sitk.GetArrayFromImage(msk).astype(np.float32)\n",
        "\n",
        "        # Extract relevant slice vectorized\n",
        "        image, mask = image[slice_idx], mask[slice_idx]  # Shape: (H, W)\n",
        "\n",
        "        # Vectorized normalization\n",
        "        image = (image - image.min()) / max(image.max(), 1e-6)  # Avoid divide-by-zero\n",
        "\n",
        "        # Convert to PyTorch tensor and add channel dimension (C=1)\n",
        "        image, mask = map(lambda x: torch.from_numpy(x).unsqueeze(0), (image, mask))  # Shape: [1, H, W]\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "class PatientDatasetAllInOneTensor(Dataset):\n",
        "    def __init__(self, patients, metadata, train=True):\n",
        "        \"\"\"\n",
        "                Args:\n",
        "                    patients (list): List of patient IDs.\n",
        "                    metadata (DataFrame): Metadata containing patient information.\n",
        "                    train (bool): If True, filters out empty slices.\n",
        "                \"\"\"\n",
        "        self.patients = patients\n",
        "        self.metadata = metadata\n",
        "        self.train = train\n",
        "        self.images, self.masks = self._extract_data()\n",
        "\n",
        "    def _extract_data(self):\n",
        "        ri = RadioImage()\n",
        "        all_images = []\n",
        "        all_masks = []\n",
        "\n",
        "        for patient in self.patients:\n",
        "            print(f\"Processing Patient: {patient}\")\n",
        "            subject = self.metadata[self.metadata['Subject ID'] == patient]\n",
        "\n",
        "            img = ri.read_dicom_ct(subject)\n",
        "            msk = ri.read_dicom_seg(subject, 'GTV-1')\n",
        "\n",
        "            image = sitk.GetArrayFromImage(img).astype(np.float16)\n",
        "            mask = sitk.GetArrayFromImage(msk).astype(np.float16)\n",
        "\n",
        "            print(f\"Image Shape: {image.shape}, Mask Shape: {mask.shape}\")\n",
        "\n",
        "            # Ensure number of slices are the same\n",
        "            min_slices = min(image.shape[0], mask.shape[0])\n",
        "            image, mask = image[:min_slices], mask[:min_slices]\n",
        "\n",
        "            # Keep only non-empty slices for training\n",
        "            if self.train:\n",
        "                non_empty_slices = np.any(mask, axis=(1, 2))  # Detect slices with any segmentation\n",
        "                image, mask = image[non_empty_slices], mask[non_empty_slices]\n",
        "\n",
        "            # Normalize image (Min-Max Scaling)\n",
        "            image -= image.min()\n",
        "            if image.max() > 0:\n",
        "                image /= image.max()\n",
        "\n",
        "            all_images.append(image)\n",
        "            all_masks.append(mask)\n",
        "\n",
        "        # Concatenate all images & masks to maintain DataLoader consistency\n",
        "        all_images = np.concatenate(all_images, axis=0)  # [Total Slices, H, W]\n",
        "        all_masks = np.concatenate(all_masks, axis=0)  # [Total Slices, H, W]\n",
        "\n",
        "        return all_images, all_masks\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = torch.from_numpy(self.images[idx]).unsqueeze(0)  # Add channel dim (C=1)\n",
        "        mask = torch.from_numpy(self.masks[idx]).unsqueeze(0)  # Add channel dim (C=1)\n",
        "        return image, mask\n",
        "\n",
        "class SliceDataset(Dataset):\n",
        "    def __init__(self,images_path,masks_path):\n",
        "        self.images_path = images_path\n",
        "        self.masks_path = masks_path\n",
        "        self.n_samples = len(images_path)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        \"\"\" Reading Image\"\"\"\n",
        "        image = cv2.imread(self.images_path[index], cv2.IMREAD_GRAYSCALE)\n",
        "        image = image/image.max()\n",
        "        image =image.astype(np.float32)\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "        image = torch.from_numpy(image)\n",
        "\n",
        "        \"\"\"Reading Mask\"\"\"\n",
        "        mask = cv2.imread(self.masks_path[index], cv2.IMREAD_GRAYSCALE)\n",
        "        mask = mask / mask.max()\n",
        "        mask = mask.astype(np.float32)\n",
        "        mask = np.expand_dims(mask,axis=0)\n",
        "        mask = torch.from_numpy(mask)\n",
        "\n",
        "        return image,mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DataPreprocess:\n",
        "    def __init__(self):\n",
        "        metadata_lung1 = pd.read_csv('metadata/metadata_lung1.csv', sep=',', index_col=False)\n",
        "        patient_list_lung1 = metadata_lung1[\"Subject ID\"].unique().tolist()\n",
        "        index_of_error_patient = [patient_list_lung1.index(i) for i in ['LUNG1-128'] ]\n",
        "        patient_list_lung1 = np.delete(patient_list_lung1, index_of_error_patient)\n",
        "        patient_list_lung1 = [p + \"/None\" + \"/GTV-1\" for p in patient_list_lung1]\n",
        "\n",
        "        metadata_md = pd.read_csv('metadata/metadata_md.csv', sep=',', index_col=False)\n",
        "        patient_list_md = metadata_md[\"Subject ID\"].unique().tolist()\n",
        "        index_of_error_patient = [patient_list_md.index(i) for i in ['interobs09']]\n",
        "        patient_list_md = np.delete(patient_list_md, index_of_error_patient)\n",
        "\n",
        "        patient_list_md1 = [p + \"/None\" + \"/GTV-1vis-1\" for p in patient_list_md]\n",
        "        patient_list_md2 = [p + \"/None\" + \"/GTV-1vis-2\" for p in patient_list_md]\n",
        "        patient_list_md3 = [p + \"/None\" + \"/GTV-1vis-3\" for p in patient_list_md]\n",
        "        patient_list_md4 = [p + \"/None\" + \"/GTV-1vis-4\" for p in patient_list_md]\n",
        "        patient_list_md5 = [p + \"/None\" + \"/GTV-1vis-5\" for p in patient_list_md]\n",
        "        metadata_test = pd.read_csv('metadata/metadata_test_retest.csv', sep=',', index_col=False)\n",
        "        patient_list_test_retest = metadata_test[\"Subject ID\"].unique().tolist()\n",
        "        index_of_error_patient = [patient_list_test_retest.index(i) for i in\n",
        "                                  ['RIDER-2283289298', 'RIDER-5195703382', 'RIDER-8509201188']]\n",
        "        patient_list_test_retest = np.delete(patient_list_test_retest, index_of_error_patient)\n",
        "        patient_list_test = [p + \"/TEST\" + \"/GTVp_test_man\" for p in patient_list_test_retest]\n",
        "        patient_list_retest = [p + \"/RETEST\" + \"/GTVp_retest_man\" for p in patient_list_test_retest]\n",
        "        self.patient_list = list(chain(patient_list_lung1,patient_list_test,patient_list_retest,patient_list_md1,\n",
        "                                       patient_list_md2,patient_list_md3,patient_list_md4,patient_list_md5))\n",
        "    def get_data(self):\n",
        "        return self.patient_list\n",
        "\n",
        "    def load_array(self, pid, sub_type, seg_type):\n",
        "        ri = RadioImage()\n",
        "\n",
        "        print(\"Patient ID : \", pid)\n",
        "        print(\"Subject Type : \", sub_type)\n",
        "        print(\"Segment Type : \", seg_type)\n",
        "        if \"LUNG1\" in pid:\n",
        "            metadata = pd.read_csv('metadata/metadata_lung1.csv', sep=',', index_col=False)\n",
        "            subject = metadata[metadata['Subject ID'] == pid]\n",
        "\n",
        "        elif \"interobs\" in pid:\n",
        "            metadata = pd.read_csv('metadata/metadata_md.csv', sep=',', index_col=False)\n",
        "            subject = metadata[metadata['Subject ID'] == pid]\n",
        "\n",
        "        else:\n",
        "            metadata = pd.read_csv('metadata/metadata_test_retest.csv', sep=',', index_col=False)\n",
        "            subject = metadata[metadata['Subject ID'] == pid]\n",
        "\n",
        "        if sub_type == \"None\":\n",
        "            img = ri.read_dicom_ct(subject)\n",
        "            msk = ri.read_dicom_seg(subject, seg_type)\n",
        "        else:\n",
        "            img = ri.read_dicom_test_retest_ct(subject, sub_type)\n",
        "            msk = ri.read_dicom_test_retest_seg(subject, sub_type, seg_type)\n",
        "\n",
        "        image = sitk.GetArrayFromImage(img)\n",
        "        mask = sitk.GetArrayFromImage(msk)\n",
        "        print(\"Shape of image : \", image.shape)\n",
        "        print(\"Shape of mask : \", mask.shape)\n",
        "        # Ensure the number of slices are the same\n",
        "        num_slices_img, num_slices_mask = image.shape[0], mask.shape[0]\n",
        "\n",
        "        if num_slices_img != num_slices_mask:\n",
        "            print(\"Warning: The number of slices in the image and mask are not the same!\")\n",
        "\n",
        "        # Determine the minimum number of slices to avoid indexing errors\n",
        "        min_slices = min(num_slices_img, num_slices_mask)\n",
        "\n",
        "        # Adjust the image and mask to have the same number of slices\n",
        "        image = image[:min_slices]\n",
        "        mask = mask[:min_slices]\n",
        "\n",
        "        # Find non-empty slices efficiently using np.any()\n",
        "        non_empty_slices = np.any(mask, axis=(1, 2))  # Check if any pixel is non-zero along height & width\n",
        "        image = image[non_empty_slices]\n",
        "        mask = mask[non_empty_slices]\n",
        "        # print(\"Shape of image : \", image.shape)\n",
        "        # print(\"Shape of mask : \", mask.shape)\n",
        "        image = image.astype(np.float64)\n",
        "        mask = mask.astype(np.float64)\n",
        "        image -= image.min()\n",
        "        image = 255*image / image.max()\n",
        "        mask = 255 * mask\n",
        "        # image = torch.from_numpy(image)\n",
        "        # mask = torch.from_numpy(mask)\n",
        "        # print(\"Shape of Image : \", image.shape)\n",
        "        # print(\"Shape of Mask : \", mask.shape)\n",
        "        # ri.display(img, msk, seg_type)\n",
        "        return image, mask\n",
        "\n",
        "    \"\"\"Saving into tensor\"\"\"\n",
        "    def entire_data_to_tensor(self, p_list):\n",
        "        \"\"\" Reading image and Mask \"\"\"\n",
        "        pid, sub_type, seg_type = p_list[0].split(\"/\")\n",
        "        image, mask = self.load_array(pid, sub_type, seg_type)\n",
        "        print(image.shape)\n",
        "        print(mask.shape)\n",
        "        print(\"After\")\n",
        "        p_list = np.delete(p_list, [0])\n",
        "        for patient in p_list:\n",
        "            print(patient)\n",
        "            pid, sub_type, seg_type =patient.split(\"/\")\n",
        "            temp_image, temp_mask = self.load_array(pid, sub_type, seg_type)\n",
        "            # print(temp_image.shape)\n",
        "            # print(temp_mask.shape)\n",
        "            image = torch.cat([image, temp_image], axis=0)\n",
        "            mask = torch.cat([mask, temp_mask], axis=0)\n",
        "        print(image.shape)\n",
        "        print(mask.shape)\n",
        "        return image, mask\n",
        "\n",
        "    \"\"\"Saving into Disk\"\"\"\n",
        "    def entire_data_to_disk(self,p_list):\n",
        "        path = \"F:\\\\Idiot Developer\\\\radioGenomic\\\\Segementation\\\\data\\\\full data\"\n",
        "        for patient in p_list:\n",
        "            pid, sub_type, seg_type = patient.split(\"/\")\n",
        "            image,mask = self.load_array(pid, sub_type, seg_type)\n",
        "            for idx in tqdm(range(len(image))):\n",
        "                if \"RIDER\" in pid :\n",
        "                    file_name = f\"{pid}_{sub_type}_{idx}.png\"\n",
        "                elif \"interobs\" in pid:\n",
        "                    _, _, seg_num = seg_type.split(\"-\")\n",
        "                    file_name = f\"{pid}_md_{seg_num}_{idx}.png\"\n",
        "                else:\n",
        "                    file_name = f\"{pid}_{idx}.png\"\n",
        "                image_path = os.path.join(path,\"images\",file_name)\n",
        "                mask_path = os.path.join(path,\"masks\",file_name)\n",
        "                # print(file_name)\n",
        "                # print(image_path)\n",
        "                # print(mask_path)\n",
        "                # print(image[idx])\n",
        "                # print(mask[idx])\n",
        "                cv2.imwrite(image_path, image[idx])\n",
        "                cv2.imwrite(mask_path, mask[idx])\n",
        "            # print(image.shape)\n",
        "            # print(mask.shape)\n",
        "            # break\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # datapreprocess = DataPreprocess()\n",
        "    # print(\"Final List\")\n",
        "    # patient_list = datapreprocess.get_data()\n",
        "    # print(len(patient_list))\n",
        "    # print(patient_list)\n",
        "    metadata_lung1 = pd.read_csv('metadata/metadata_lung1.csv', sep=',', index_col=False)\n",
        "    patient_list_lung1 = metadata_lung1[\"Subject ID\"].unique().tolist()\n",
        "    index_of_error_patient = [patient_list_lung1.index(i) for i in ['LUNG1-128']]\n",
        "    patient_list_lung1 = np.delete(patient_list_lung1, index_of_error_patient)\n",
        "    print(patient_list_lung1)\n",
        "    train_patient, valid_patient = train_test_split(patient_list_lung1,test_size = 0.1, random_state =42)\n",
        "    train_patient, test_patient = train_test_split(train_patient, test_size=0.1, random_state=42)\n",
        "    print(\"Number of Total Patients : \", len(patient_list_lung1))\n",
        "    print(\"Number of Patients for Training : \",len(train_patient))\n",
        "    print(\"Number of Patients for Validation : \",len(valid_patient))\n",
        "    print(\"Number of Patients for Testing : \",len(test_patient))\n",
        "    # transform = transforms.Compose([\n",
        "    #     transforms.ToTensor(),\n",
        "    # ])\n",
        "    batch_size = 2\n",
        "    # # Load dataset\n",
        "    # print(\"Training Loading...\")\n",
        "    # train_dataset = PatientDatasetAllInOneTensor(train_patient, metadata_lung1, train=True)\n",
        "    # print(\"Valid Loading...\")\n",
        "    # valid_dataset = PatientDatasetAllInOneTensor(valid_patient, metadata_lung1, train=False)\n",
        "    # print(\"Testing Loading...\")\n",
        "    # test_dataset = PatientDatasetAllInOneTensor(test_patient, metadata_lung1, train=False)\n",
        "    # Load dataset\n",
        "    print(\"Training Loading...\")\n",
        "    train_dataset = PatientDataset2DUNet(train_patient, metadata_lung1, train=True)\n",
        "    print(\"Valid Loading...\")\n",
        "    valid_dataset = PatientDataset2DUNet(valid_patient, metadata_lung1, train=False)\n",
        "    print(\"Testing Loading...\")\n",
        "    test_dataset = PatientDataset2DUNet(test_patient, metadata_lung1, train=False)\n",
        "    #\n",
        "    # # train_dataset, valid_dataset = torch.utils.data.random_split(patient_dataset, [0.8, 0.2])\n",
        "    # #\n",
        "    # #\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=os.cpu_count()\n",
        "    )\n",
        "    #\n",
        "    valid_loader = DataLoader(\n",
        "        dataset=valid_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=os.cpu_count()\n",
        "    )\n",
        "\n",
        "    #\n",
        "    test_loader = DataLoader(\n",
        "        dataset=test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=os.cpu_count()\n",
        "    )\n",
        "    # Fetch one batch\n",
        "    for images, masks in train_loader:\n",
        "        print(f\"Batch Image Shape: {images.shape}\")  # Expected: [8, 1, H, W]\n",
        "        print(f\"Batch Mask Shape: {masks.shape}\")  # Expected: [8, 1, H, W]\n",
        "        break\n",
        "    # for image ,mask in test_loader:\n",
        "    #     print(\"Shape of image : \", image.shape)\n",
        "    #     print(\"Shape of mask : \", mask.shape)\n",
        "    #\n",
        "    # for image ,mask in valid_loader:\n",
        "    #     print(\"Shape of image : \", image.shape)\n",
        "    #     print(\"Shape of mask : \", mask.shape)\n",
        "    print(f\"Total images in Training Dataset: {len(train_dataset)}\")\n",
        "    print(f\"Total images in Valid Dataset: {len(valid_dataset)}\")\n",
        "    print(f\"Total images in Testing Dataset: {len(test_dataset)}\")\n",
        "\n",
        "\n",
        "    # # datapreprocess.load_array(\"LUNG1-172/None/GTV-1\")\n",
        "    # datapreprocess.entire_data_to_disk(patient_list)\n",
        "    # np.random.seed(42)\n",
        "    # p_list = np.random.choice(patient_list,60)\n",
        "    # print(len(p_list))\n",
        "    # image,mask = datapreprocess.entire_data_to_tensor(p_list)\n",
        "    # torch.save(image, \"F:\\\\Idiot Developer\\\\radioGenomic\\\\files\\\\segmentation\\\\image.pt\")\n",
        "    # torch.save(mask, \"F:\\\\Idiot Developer\\\\radioGenomic\\\\files\\\\segmentation\\\\mask.pt\")\n",
        "    # image = torch.load(\"F:\\\\Idiot Developer\\\\radioGenomic\\\\files\\\\segmentation\\\\image.pt\")\n",
        "    # mask = torch.load(\"F:\\\\Idiot Developer\\\\radioGenomic\\\\files\\\\segmentation\\\\mask.pt\")\n",
        "    # print(image.shape)\n",
        "    # print(mask.shape)\n",
        "    # print(len(image))\n",
        "    # patient_dataset = PatientDataset(image,mask)\n",
        "    # train_dataset, test_dataset = torch.utils.data.random_split(patient_dataset, [0.8, 0.2])\n",
        "    # print(train_dataset[0][0].shape)\n",
        "    # print(train_dataset[0][1].shape)\n",
        "    # print( len(train_dataset))\n",
        "    # print(len(test_dataset))\n",
        "    # print(train_dataset[0][0])\n",
        "    # print(test_dataset[0][0])\n",
        "    # plt.imshow(train_dataset[0][0],cmap=\"gray\")\n",
        "    # plt.show()\n",
        "    # plt.imshow(train_dataset[0][1],cmap=\"gray\")\n",
        "    # plt.show()\n",
        "    # image, mask = dp. __getitem__(500)\n",
        "\n",
        "    # print(type(dp))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NenZLBCXucXJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}